{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzV9wsJ5pGhf"
   },
   "source": [
    "# <img src=\"https://img.icons8.com/bubbles/50/000000/mind-map.png\" style=\"height:50px;display:inline\"> ECE 046211 - Technion - Deep Learning\n",
    "---\n",
    "\n",
    "## HW2 - Multilayer NNs and Convolutional NNs\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq2c8X93pGhh"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/clouds/96/000000/keyboard.png\" style=\"height:50px;display:inline\"> Keyboard Shortcuts\n",
    "---\n",
    "* Run current cell: **Ctrl + Enter**\n",
    "* Run current cell and move to the next: **Shift + Enter**\n",
    "* Show lines in a code cell: **Esc + L**\n",
    "* View function documentation: **Shift + Tab** inside the parenthesis or `help(name_of_module)`\n",
    "* New cell below: **Esc + B**\n",
    "* Delete cell: **Esc + D, D** (two D's)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZZybn3NpGhh"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/information.png\" style=\"height:50px;display:inline\"> Students Information\n",
    "---\n",
    "* Fill in\n",
    "\n",
    "| Name              | Campus Email                    | ID        |\n",
    "|-------------------|---------------------------------|-----------|\n",
    "| Nir Elfasi        | nirelfasi@campus.technion.ac.il | 206094567 |\n",
    "| Itamar Nierenberg | itamarnie@campus.technion.ac.il | 205939010 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDK5zqhdpGhi"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/upload-to-cloud.png\" style=\"height:50px;display:inline\"> Submission Guidelines\n",
    "---\n",
    "* Maximal garde: 100.\n",
    "* Submission only in **pairs**. \n",
    "    * Please make sure you have registered your group in Moodle (there is a group creation component on the Moodle where you need to create your group and assign members).\n",
    "* **No handwritten submissions.** You can choose whether to answer in a Markdown cell in this notebook or attach a PDF with your answers.\n",
    "* <a style='color:red'> SAVE THE NOTEBOOKS WITH THE OUTPUT, CODE CELLS THAT WERE NOT RUN WILL NOT GET ANY POINTS! </a>\n",
    "* What you have to submit:\n",
    "    * If you have answered the questions in the notebook, you should submit this file only, with the name: `ece046211_hw2_id1_id2.ipynb`.\n",
    "    * If you answered the questions in a different file you should submit a `.zip` file with the name `ece046211_hw2_id1_id2.zip` with content:\n",
    "        * `ece046211_hw2_id1_id2.ipynb` - the code tasks\n",
    "        * `ece046211_hw2_id1_id2.pdf` - answers to questions.\n",
    "    * No other file-types (`.py`, `.docx`...) will be accepted.\n",
    "* Submission on the course website (Moodle).\n",
    "* **Latex in Colab** - in some cases, Latex equations may no be rendered. To avoid this, make sure to not use *bullets* in your answers (\"* some text here with Latex equations\" -> \"some text here with Latex equations\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmSj_UufpGhi"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/online.png\" style=\"height:50px;display:inline\"> Working Online and Locally\n",
    "---\n",
    "* You can choose your working environment:\n",
    "    1. `Jupyter Notebook`, **locally** with <a href=\"https://www.anaconda.com/distribution/\">Anaconda</a> or **online** on <a href=\"https://colab.research.google.com/\">Google Colab</a>\n",
    "        * Colab also supports running code on GPU, so if you don't have one, Colab is the way to go. To enable GPU on Colab, in the menu: `Runtime`$\\rightarrow$ `Change Runtime Type` $\\rightarrow$`GPU`.\n",
    "    2. Python IDE such as <a href=\"https://www.jetbrains.com/pycharm/\">PyCharm</a> or <a href=\"https://code.visualstudio.com/\">Visual Studio Code</a>.\n",
    "        * Both allow editing and running Jupyter Notebooks.\n",
    "\n",
    "* Please refer to `Setting Up the Working Environment.pdf` on the Moodle or our GitHub (https://github.com/taldatech/ee046211-deep-learning) to help you get everything installed.\n",
    "* If you need any technical assistance, please go to our Piazza forum (`hw2` folder) and describe your problem (preferably with images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlp1Fp4ppGhj"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "---\n",
    "\n",
    "* [Part 1 - Theory](#-Part-1---Theory)\n",
    "    * [Q1 - Generalization in A Teacher-Student Setup](#-Question-1--Generalization-in-A-Teacher-Student-Setup)\n",
    "    * [Q2 - Backpropagation By Hand](#-Question-2---Backpropagation-By-Hand)\n",
    "    * [Q3 - Deep Double Descent](#-Question-3---Deep-Double-Descent)\n",
    "    * [Q4 - Initialization](#-Question-4---Initialization)\n",
    "    * [Q5 - MLP and Invaraince](#-Question-5---MLP-and-Invaraince)\n",
    "    * [Q6 - VGG Architecture](#-Question-6--VGG-Architecture)\n",
    "* [Part 2 - Code Assignments](#-Part-2---Code-Assignments)\n",
    "    * [Task 1 - The Importance of Activation and Initialization](#-Task-1---The-Importance-of-Activation-and-Initialization)\n",
    "    * [Task 2 - MLP-based Deep Classifer](#-Task-2---MLP-based-Deep-Classifer)\n",
    "    * [Task 3 - Design a CNN](#-Task-3---Design-a-CNN)\n",
    "* [Credits](#-Credits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKtSiQX_pGhj"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/cute-clipart/64/000000/ball-point-pen.png\" style=\"height:50px;display:inline\"> Part 1 - Theory\n",
    "---\n",
    "* You can choose whether to answser these straight in the notebook (Markdown + Latex) or use another editor (Word, LyX, Latex, Overleaf...) and submit an additional PDF file, **but no handwritten submissions**.\n",
    "* You can attach additional figures (drawings, graphs,...) in a separate PDF file, just make sure to refer to them in your answers.\n",
    "\n",
    "* $\\large\\LaTeX$ <a href=\"https://kapeli.com/cheat_sheets/LaTeX_Math_Symbols.docset/Contents/Resources/Documents/index\">Cheat-Sheet</a> (to write equations)\n",
    "    * <a href=\"http://tug.ctan.org/info/latex-refsheet/LaTeX_RefSheet.pdf\">Another Cheat-Sheet</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsqSFZG1pGhj"
   },
   "source": [
    "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 1 -Generalization in A Teacher-Student Setup\n",
    "---\n",
    "\n",
    "Recall from lecture 4 the Risk $\\mathcal{R}(w)$: $$ \\mathcal{R}(w) \\triangleq \\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) } \\left[ ||w^Tx^{(0)} - w_t^Tx^{(0)}||^2 \\right] $$\n",
    "\n",
    "Prove:\n",
    "\n",
    "$$ \\mathcal{R}(w) = ||w-w_t||^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 1 - Answer\n",
    "\n",
    "We'll prove this claim by using the properties of a Gaussian Vector.\n",
    "$$ w^Tx^{(0)} - w_t^Tx^{(0)} = (w^T - w_t^T)x^{(0)} $$\n",
    "Now we'll define $ z = w - w_t $ and notate:\n",
    "\n",
    "$$\n",
    "x^{(0)} = \\begin{bmatrix}\n",
    "           x_{1} \\\\\n",
    "           x_{2} \\\\\n",
    "           \\vdots \\\\\n",
    "           x_{n}\n",
    "         \\end{bmatrix}\n",
    "z = \\begin{bmatrix}\n",
    "           z_{1} \\\\\n",
    "           z_{2} \\\\\n",
    "           \\vdots \\\\\n",
    "           z_{n}\n",
    "         \\end{bmatrix}\n",
    "\\Rightarrow (w^T - w_t^T)x^{(0)} = z^Tx^{(0)} = z_{1}x_{1} + z_{2}x_{2} + ... + z_{n}x_{n}\n",
    "$$\n",
    "\n",
    "This means that $z^Tx^{(0)}$ is a linear combination of the components of a Gaussian vector and by definition of the Gaussian vector, $z^Tx^{(0)} $ is itself a Gaussian random variable.\n",
    "After this conclusion we can safely say that $ \\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) } \\left[ ||z^Tx^{(0)}||^2 \\right] $ Is the second momentum of the random Gaussian variable $ z^Tx^{(0)} $\n",
    "\n",
    "$$ \\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) } \\left[ ||z^Tx^{(0)}||^2 \\right]  = \\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) } \\left[ (z_{1}x_{1} + z_{2}x_{2} + ... + z_{n}x_{n})^2 \\right]$$\n",
    "\n",
    "Since the Covariance matrix is $ I $ for a Gaussian vector that means that all $ x_{i} $ are statistically independent plus all the expectations of $ x_{i} $ are zero, then all the expectations which contain the multiplication $ x_{i} \\cdot x_{j} \\; s.t \\;  i \\neq j $ will result in zero which leave us with the result:\n",
    "$$ \\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) } \\left[ (z_{1}x_{1} + z_{2}x_{2} + ... + z_{n}x_{n})^2 \\right] = \\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) } \\left[ (z_{1}x_{1})^2 + (z_{2}x_{2})^2 + ... + (z_{n}x_{n})^2 \\right] = \\[ \\sum_{i=1}^{n} \\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) } \\left[ (z_{i}x_{i})^2 \\right] \\] = \\[ \\sum_{i=1}^{n} z_{i}^2 \\mathbb{E}_{x^{(0)} \\sim \\mathcal{N}(0, I) } \\left[ x_{i}^2 \\right] \\] = \\[ \\sum_{i=1}^{n} z_{i}^2 \\] = ||z||^2 = ||w - w_t||^2$$\n",
    "\n",
    "$$ \\blacksquare $$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 2 - Backpropagation By Hand\n",
    "---\n",
    "Consider the following network:\n",
    "<img src=\"https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/backprop_by_hand_ex1.png\" style=\"height:300px\">\n",
    "\n",
    "We will work with one sample for this example, but it can be extended to mini-batches.\n",
    "\n",
    "* Input: $x = \\begin{bmatrix} 1 \\\\ 4 \\\\ 5 \\end{bmatrix} \\in \\mathbb{R}^3$\n",
    "* Output (target): $ t = \\begin{bmatrix} 0.1 \\\\ 0.05 \\end{bmatrix} \\in \\mathbb{R}^2 $\n",
    "* Number of Hidden Layers: 1\n",
    "* Activation: Sigmoid for both hidden and output layers\n",
    "* Loss Functions: MSE\n",
    "\n",
    "We initialize the weights and biases to random values as follows:\n",
    "<img src=\"https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/backprop_by_hand_ex2.png\" style=\"height:300px\">\n",
    "\n",
    "1. Perform one forward pass and calculate the MSE.\n",
    "2. Perform backpropagation (one backward pass, i.e., calculate the gradients).\n",
    "3. With a learning rate of $\\alpha = 0.01$, what are the new values of the weights after performing the forward pass and backward pass (assume we use SGD)?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 2 - Answer\n",
    "We'll start by defining parameter matrix for each layer:\n",
    "$$ x = \\begin{pmatrix} 1 \\\\ 4 \\\\ 5 \\end{pmatrix} $$\n",
    "$$ W_1 = \\begin{pmatrix} 0.1 & 0.3 & 0.5 \\\\ 0.2 & 0.4 & 0.6 \\end{pmatrix} \\;\\; b_1 = \\begin{pmatrix} 0.5 \\\\ 0.5 \\end{pmatrix} $$\n",
    "$$ h = \\sigma(W_1x + b_1) $$\n",
    "$$ W_2 = \\begin{pmatrix} 0.7 & 0.9 \\\\ 0.8 & 0.1 \\end{pmatrix} \\;\\; b_2 = \\begin{pmatrix} 0.5 \\\\ 0.5 \\end{pmatrix} $$\n",
    "$$ o = \\sigma(W_2h + b_2) $$\n",
    "$$ MSE = ||o-t||^2 $$\n",
    "\n",
    "1. We'll do the forward pass, While using code segments for pure calculation, we will calculate the relevant derivatives as we go:\n",
    "\n",
    "$$ z_1 = W_1x + b_1 = \\begin{pmatrix} 0.1 & 0.3 & 0.5 \\\\ 0.2 & 0.4 & 0.6 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 4 \\\\ 5 \\end{pmatrix} + \\begin{pmatrix} 0.5 \\\\ 0.5 \\end{pmatrix} = \\begin{pmatrix} 0.1 + 1.2 + 2.5 + 0.5 \\\\ 0.2 + 1.6 + 3 + 0.5 \\end{pmatrix} = \\begin{pmatrix} 4.3 \\\\ 5.3 \\end{pmatrix}$$\n",
    "$$ \\frac{\\partial z_1}{\\partial W_1} $$\n",
    "$$ \\frac{\\partial z_1}{\\partial b_1} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} $$\n",
    "$$ z_2 = \\sigma(z_1) = h = \\begin{pmatrix} \\sigma(4.3) \\\\ \\sigma(5.3) \\end{pmatrix} = \\begin{pmatrix} 0.98661308 \\\\ 0.9950332 \\end{pmatrix} $$\n",
    "$$ \\frac{\\partial z_2}{\\partial z_1} = \\begin{pmatrix} \\sigma(z_{11})(1-\\sigma(z_{11}) & 0 \\\\ 0 & \\sigma(z_{12})(1-\\sigma(z_{12}) \\end{pmatrix} = \\begin{pmatrix} 0.01320771 & 0 \\\\ 0 & 0.00494213 \\end{pmatrix}$$\n",
    "$$ z_3 = W_2z_2 + b_2 =W_2h + b_2 = \\begin{pmatrix} 0.7 & 0.9 \\\\ 0.8 & 0.1 \\end{pmatrix}\\begin{pmatrix} \\sigma(4.3) \\\\ \\sigma(5.3) \\end{pmatrix} + \\begin{pmatrix} 0.5 \\\\ 0.5 \\end{pmatrix} = \\begin{pmatrix} 0.7\\cdot\\sigma(4.3) + 0.9\\cdot\\sigma(5.3) + 0.5 \\\\ 0.8\\cdot\\sigma(4.3) + 0.1\\cdot\\sigma(5.3) + 0.5 \\end{pmatrix} = \\begin{pmatrix} 2.08615904 \\\\ 1.38879379 \\end{pmatrix} $$\n",
    "$$ \\frac{\\partial z_3}{\\partial z_2} = W_2 = \\begin{pmatrix} 0.7 & 0.9 \\\\ 0.8 & 0.1 \\end{pmatrix} $$\n",
    "$$ \\frac{\\partial z_3}{\\partial W_2}$$\n",
    "$$ \\frac{\\partial z_3}{\\partial b_2} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} $$\n",
    "$$ z_4 = o = \\sigma(z_3) = \\begin{pmatrix} \\sigma(0.7\\cdot\\sigma(4.3) + 0.9\\cdot\\sigma(5.3) + 0.5) \\\\ \\sigma(0.8\\cdot\\sigma(4.3) + 0.1\\cdot\\sigma(5.3) + 0.5) \\end{pmatrix} = \\begin{pmatrix} 0.88955061 \\\\ 0.80039961 \\end{pmatrix} $$\n",
    "$$ \\frac{\\partial z_4}{\\partial z_3} = \\begin{pmatrix} \\sigma(z_{31})(1-\\sigma(z_{31}) & 0 \\\\ 0 & \\sigma(z_{32})(1-\\sigma(z_{32}) \\end{pmatrix} = \\begin{pmatrix} 0.09825032 & 0 \\\\ 0 & 0.15976008 \\end{pmatrix}$$\n",
    "$$ z_5 = ||z_4 -t||^2 =||o-t||^2 = (\\sigma(0.7\\cdot\\sigma(4.3) + 0.9\\cdot\\sigma(5.3) + 0.5) - 0.1)^2 + (\\sigma(0.8\\cdot\\sigma(4.3) + 0.1\\cdot\\sigma(5.3) + 0.5) - 0.05)^2 = 1.186489743807772$$\n",
    "$$ \\frac{\\partial z_5}{\\partial z_4} = 2(z_4-t)  = 2\\begin{pmatrix} 0.88955061 - 0.1 & 0.80039961 - 0.05 \\end{pmatrix} = \\begin{pmatrix} 1.57910122 & 1.50079922 \\end{pmatrix}$$\n",
    "\n",
    "*To Summarize this part, $ MSE = z_5 = 1.186489743807772 $*\n",
    "\n",
    "2. Now we will perform the backpropagation part:\n",
    "\n",
    "$$ \\frac{\\partial z_5}{\\partial W_{2_{11}}} = \\frac{\\partial z_5}{\\partial z_{4_{1}}} \\frac{\\partial z_{4_{1}}}{\\partial z_{3_{1}}} \\frac {\\partial z_{3_{1}}}{\\partial W_{2_{11}}} = 1.57910122 \\cdot 0.09825032 \\cdot 0.98661308 = 0.153070257$$\n",
    "$$ \\frac{\\partial z_5}{\\partial W_{2_{12}}} = \\frac{\\partial z_5}{\\partial z_{4_{1}}} \\frac{\\partial z_{4_{1}}}{\\partial z_{3_{1}}} \\frac {\\partial z_{3_{1}}}{\\partial W_{2_{12}}} = 1.57910122 \\cdot 0.09825032 \\cdot 0.9950332 = 0.1543766151$$\n",
    "$$ \\frac{\\partial z_5}{\\partial W_{2_{21}}} = \\frac{\\partial z_5}{\\partial z_{4_{2}}} \\frac{\\partial z_{4_{2}}}{\\partial z_{3_{2}}} \\frac {\\partial z_{3_{2}}}{\\partial W_{2_{21}}} = 1.50079922 \\cdot 0.15976008 \\cdot 0.98661308 = 0.236558051$$\n",
    "$$ \\frac{\\partial z_5}{\\partial W_{2_{22}}} = \\frac{\\partial z_5}{\\partial z_{4_{2}}} \\frac{\\partial z_{4_{2}}}{\\partial z_{3_{2}}} \\frac {\\partial z_{3_{2}}}{\\partial W_{2_{22}}} = 1.50079922 \\cdot 0.15976008 \\cdot 0.9950332 = 0.2385769247$$\n",
    "\n",
    "$$\\frac{\\partial z_5}{\\partial W_2} = \\begin{pmatrix} 0.153070257 & 0.1543766151 \\\\ 0.236558051 & 0.2385769247 \\end{pmatrix} $$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial z_5}{\\partial W_{1_{11}}} = \\frac{\\partial z_5}{\\partial z_4}\\frac{\\partial z_4}{\\partial z_3}\\frac{\\partial z_3}{\\partial z_2}\\frac{\\partial z_2}{\\partial z_1}\\frac{\\partial z_{1}}{\\partial W_{1_{11}}} = \\begin{pmatrix} 1.57910122 & 1.50079922 \\end{pmatrix} \\begin{pmatrix} 0.09825032 & 0 \\\\ 0 & 0.15976008 \\end{pmatrix} \\begin{pmatrix} 0.7 & 0.9 \\\\ 0.8 & 0.1 \\end{pmatrix}\\begin{pmatrix} 0.01320771 & 0 \\\\ 0 & 0.00494213 \\end{pmatrix} * \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 0.00396782$$\n",
    "$$\\frac{\\partial z_5}{\\partial W_{1_{12}}} = \\frac{\\partial z_5}{\\partial z_4}\\frac{\\partial z_4}{\\partial z_3}\\frac{\\partial z_3}{\\partial z_2}\\frac{\\partial z_2}{\\partial z_1}\\frac{\\partial z_{1}}{\\partial W_{1_{12}}} = \\begin{pmatrix} 1.57910122 & 1.50079922 \\end{pmatrix} \\begin{pmatrix} 0.09825032 & 0 \\\\ 0 & 0.15976008 \\end{pmatrix} \\begin{pmatrix} 0.7 & 0.9 \\\\ 0.8 & 0.1 \\end{pmatrix}\\begin{pmatrix} 0.01320771 & 0 \\\\ 0 & 0.00494213 \\end{pmatrix} * \\begin{pmatrix} 4 \\\\ 0 \\end{pmatrix} = 0.01587129$$\n",
    "$$\\frac{\\partial z_5}{\\partial W_{1_{13}}} = \\frac{\\partial z_5}{\\partial z_4}\\frac{\\partial z_4}{\\partial z_3}\\frac{\\partial z_3}{\\partial z_2}\\frac{\\partial z_2}{\\partial z_1}\\frac{\\partial z_{1}}{\\partial W_{1_{13}}} = \\begin{pmatrix} 1.57910122 & 1.50079922 \\end{pmatrix} \\begin{pmatrix} 0.09825032 & 0 \\\\ 0 & 0.15976008 \\end{pmatrix} \\begin{pmatrix} 0.7 & 0.9 \\\\ 0.8 & 0.1 \\end{pmatrix}\\begin{pmatrix} 0.01320771 & 0 \\\\ 0 & 0.00494213 \\end{pmatrix} * \\begin{pmatrix} 5 \\\\ 0 \\end{pmatrix} = 0.01983912$$\n",
    "$$\\frac{\\partial z_5}{\\partial W_{1_{21}}} = \\frac{\\partial z_5}{\\partial z_4}\\frac{\\partial z_4}{\\partial z_3}\\frac{\\partial z_3}{\\partial z_2}\\frac{\\partial z_2}{\\partial z_1}\\frac{\\partial z_{1}}{\\partial W_{1_{21}}} = \\begin{pmatrix} 1.57910122 & 1.50079922 \\end{pmatrix} \\begin{pmatrix} 0.09825032 & 0 \\\\ 0 & 0.15976008 \\end{pmatrix} \\begin{pmatrix} 0.7 & 0.9 \\\\ 0.8 & 0.1 \\end{pmatrix}\\begin{pmatrix} 0.01320771 & 0 \\\\ 0 & 0.00494213 \\end{pmatrix} * \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = 0.00080858$$\n",
    "$$\\frac{\\partial z_5}{\\partial W_{1_{22}}} = \\frac{\\partial z_5}{\\partial z_4}\\frac{\\partial z_4}{\\partial z_3}\\frac{\\partial z_3}{\\partial z_2}\\frac{\\partial z_2}{\\partial z_1}\\frac{\\partial z_{1}}{\\partial W_{1_{22}}} = \\begin{pmatrix} 1.57910122 & 1.50079922 \\end{pmatrix} \\begin{pmatrix} 0.09825032 & 0 \\\\ 0 & 0.15976008 \\end{pmatrix} \\begin{pmatrix} 0.7 & 0.9 \\\\ 0.8 & 0.1 \\end{pmatrix}\\begin{pmatrix} 0.01320771 & 0 \\\\ 0 & 0.00494213 \\end{pmatrix} * \\begin{pmatrix} 0 \\\\ 4 \\end{pmatrix} = 0.00323431$$\n",
    "$$\\frac{\\partial z_5}{\\partial W_{1_{23}}} = \\frac{\\partial z_5}{\\partial z_4}\\frac{\\partial z_4}{\\partial z_3}\\frac{\\partial z_3}{\\partial z_2}\\frac{\\partial z_2}{\\partial z_1}\\frac{\\partial z_{1}}{\\partial W_{1_{23}}} = \\begin{pmatrix} 1.57910122 & 1.50079922 \\end{pmatrix} \\begin{pmatrix} 0.09825032 & 0 \\\\ 0 & 0.15976008 \\end{pmatrix} \\begin{pmatrix} 0.7 & 0.9 \\\\ 0.8 & 0.1 \\end{pmatrix}\\begin{pmatrix} 0.01320771 & 0 \\\\ 0 & 0.00494213 \\end{pmatrix} * \\begin{pmatrix} 0 \\\\ 5 \\end{pmatrix} = 0.00404289$$\n",
    "\n",
    "$$\\frac{\\partial z_5}{\\partial W_1} = \\begin{pmatrix} 0.00396782 & 0.01587129 & 0.01983912 \\\\ 0.00080858 & 0.00323431 & 0.00404289 \\end{pmatrix}$$\n",
    "\n",
    "$$\\frac{\\partial z_5}{\\partial b_2} = \\frac{\\partial z_5}{\\partial z_4}\\frac{\\partial z_4}{\\partial z_3}\\frac{\\partial z_3}{\\partial b_2} = \\begin{pmatrix} 1.57910122 & 1.50079922 \\end{pmatrix}\\begin{pmatrix} 0.09825032 & 0 \\\\ 0 & 0.15976008 \\end{pmatrix}\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}  = \\begin{pmatrix} 0.1551472 & 0.2397678 \\end{pmatrix}$$\n",
    "\n",
    "$$\\frac{\\partial z_5}{\\partial b_1} = \\frac{\\partial z_5}{\\partial z_4}\\frac{\\partial z_4}{\\partial z_3}\\frac{\\partial z_3}{\\partial z_2}\\frac{\\partial z_2}{\\partial z_1}\\frac{\\partial z_1}{\\partial b_1} = \\begin{pmatrix} 1.57910122 & 1.50079922 \\end{pmatrix}\\begin{pmatrix} 0.09825032 & 0 \\\\ 0 & 0.15976008\\end{pmatrix} \\begin{pmatrix} 0.7 & 0.9 \\\\ 0.8 & 0.1 \\end{pmatrix} \\begin{pmatrix} 0.01320771 & 0 \\\\ 0 & 0.00494213 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}  = \\begin{pmatrix} 0.00396782 & 0.00080858 \\end{pmatrix}$$\n",
    "\n",
    "*To Summarize this part:*\n",
    "$$\\frac{\\partial z_5}{\\partial W_2} = \\begin{pmatrix} 0.153070257 & 0.1543766151 \\\\ 0.236558051 & 0.2385769247 \\end{pmatrix} \\frac{\\partial z_5}{\\partial W_1} = \\begin{pmatrix} 0.00396782 & 0.01587129 & 0.01983912 \\\\ 0.00080858 & 0.00323431 & 0.00404289 \\end{pmatrix}$$\n",
    "\n",
    "$$ \\frac{\\partial z_5}{\\partial b_2} = \\begin{pmatrix} 0.1551472 & 0.2397678 \\end{pmatrix} \\frac{\\partial z_5}{\\partial b_1} = \\begin{pmatrix} 0.00396782 & 0.00080858 \\end{pmatrix} $$\n",
    "\n",
    "3. Now we will perform the SGD step with $\\alpha = 0.01$:\n",
    "\n",
    "$$ W_{1_{new}} = W_1 - \\alpha \\cdot \\frac{\\partial z_5}{\\partial W_1} = \\begin{pmatrix} 0.1 & 0.3 & 0.5 \\\\ 0.2 & 0.4 & 0.6 \\end{pmatrix} - 0.01\\cdot \\begin{pmatrix} 0.00396782 & 0.01587129 & 0.01983912 \\\\ 0.00080858 & 0.00323431 & 0.00404289 \\end{pmatrix} = \\begin{pmatrix} 0.0999603218 & 0.2998412871 & 0.4998016088 \\\\ 0.1999919142 & 0.3999676569 & 0.5999595711 \\end{pmatrix}$$\n",
    "\n",
    "$$ W_{2_{new}} = W_2 - \\alpha \\cdot \\frac{\\partial z_5}{\\partial W_2} = \\begin{pmatrix} 0.7 & 0.9 \\\\ 0.8 & 0.1 \\end{pmatrix} - 0.01 \\cdot \\begin{pmatrix} 0.153070257 & 0.1543766151 \\\\ 0.236558051 & 0.2385769247 \\end{pmatrix} = \\begin{pmatrix} 0.6984692974 & 0.8984562338 \\\\ 0.7976344195 & 0.9761423075 \\end{pmatrix} $$\n",
    "\n",
    "$$ b_{1_{new}} = b_1 - \\alpha \\cdot \\frac{\\partial z_5}{\\partial b_1} ^ T = \\begin{pmatrix} 0.5 \\\\ 0.5 \\end{pmatrix} - 0.01 \\cdot \\begin{pmatrix} 0.00396782 \\\\ 0.00080858 \\end{pmatrix} = \\begin{pmatrix} 0.4999603218 \\\\ 0.4999919142 \\end{pmatrix} $$\n",
    "$$ b_{1_{new}} = b_1 - \\alpha \\cdot \\frac{\\partial z_5}{\\partial b_2} ^ T = \\begin{pmatrix} 0.5 \\\\ 0.5 \\end{pmatrix} - 0.01 \\cdot \\begin{pmatrix} 0.1551472 \\\\ 0.2397678 \\end{pmatrix} = \\begin{pmatrix} 0.498448528 \\\\ 0.497602322 \\end{pmatrix} $$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsqSFZG1pGhj"
   },
   "source": [
    "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 3 - Deep Double Descent\n",
    "---\n",
    "\n",
    "For the following plots:\n",
    "1. Where is the critical point (the point of transition between the \"Classical Regime\" and \"Modern Regime\") of the deep double descent?\n",
    "2. What type of double descent is shown (**look closely at the graph**)? Explain. There can be more than one correct answer.\n",
    "    \n",
    "\n",
    "a. <img src='https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/double_descent_transformer.PNG' style=\"height:300px\">\n",
    "\n",
    "b. <img src='https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/double_descent_resnet.PNG' style=\"height:400px\">\n",
    "\n",
    "c. <img src='https://raw.githubusercontent.com/taldatech/ee046211-deep-learning/main/assets/double_descent_intermediate.PNG' style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 3 - Answer\n",
    "\n",
    "##3.2 graph 3\n",
    "the type of double descent here is epoch wise double descent, across different width parameter.\n",
    "* a.\n",
    "![Graph A](https://raw.githubusercontent.com/Itamarnierenberg/046211-Deep-Learning/main/assets/double_descent_graph_a.png?token=GHSAT0AAAAAACCKNRNEGYZXQHRFT2S2XCAAZCXTGRQ)\n",
    "1. we have 2 different architectures compared. for the De-En model,the critical point is obtained when the transformer model size has approximately 200 embedding dimensions.The En-Fr has critical point when the transformer model size has approximately 300 embedding dimensions. as we can see the training loss is monotonic decreasing. not surprising.\n",
    "2. the type of double descent here is model wise double descent, across different architectures, and different transformer model size.\n",
    "* b.\n",
    "![Graph B](https://raw.githubusercontent.com/Itamarnierenberg/046211-Deep-Learning/main/assets/double_descent_graph_b.png?token=GHSAT0AAAAAACCKNRNE6T2SOQG7TVJ57LH2ZCXTIDQ)\n",
    "1. in the 2nd graph, with 0% label noise, the critical point obtained at width parameter = 9, with 10% label noise, the critical point obtained at width parameter = 11, with 20% label noise, the critical point obtained at width parameter = 13. as we can see when the labels are noisy, the train error will increase.\n",
    "2. the type of double descent here is sample wise double descent, accross different width parameter, and with noisy labels.\n",
    "* c.\n",
    "![Graph C](https://raw.githubusercontent.com/Itamarnierenberg/046211-Deep-Learning/main/assets/double_descent_graph_c.png?token=GHSAT0AAAAAACCKNRNFGLE4HJVSPEXHEVIYZCXTJIQ)\n",
    "1. in the 3rd graph, in the slice where the width parameter = 64, the critical point obtained at epoch = 70, in the slice where width parameter = 12, there is no critical point, in the slice where width parameter = 3, there is no critical point. as we can see for a medium model size we have a region where the error is big, just as we have seen in graphs 1,2.\n",
    "2. the type of double descent here is epoch wise double descent, across different width parameter."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsqSFZG1pGhj"
   },
   "source": [
    "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 4 - Initialization\n",
    "---\n",
    "Recall that in lecture 5 we were discussing how to calculate the initialization variance, and reached the conclusion that $$ \\sigma_l =\\frac{1}{\\sqrt{\\sum_j \\mathbb{E} \\left[\\varphi^2(u_{l-1}[j])\\right]}} $$\n",
    "Show that for ReLU activation ($\\varphi(z) = max(0,z)$), the optimal variance satisfies: $$ \\sigma_l = \\sqrt{\\frac{2}{d_{l-1}}}$$\n",
    "\n",
    "1. Under the assumption that the distribution of $W$ is symmetric ($\\to$ the distribution of $u$ is symmetric).\n",
    "2. Using the central limit theorem for large width.\n",
    "\n",
    "Answer each section **separately** and assume the sections are independent.\n",
    "\n",
    "All the notations are the same as in the lecture slides."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 4 - Answer\n",
    "We'll calculate $\\sum_j \\mathbb{E} \\left[\\varphi^2(u_{l-1}[j])\\right] $ for the ReLU activation\n",
    "\n",
    "$$ \\sum_j \\mathbb{E} \\left[\\varphi^2(u_{l-1}[j])\\right] = \\sum_j \\mathbb{E} \\left[(\\max \\{0, u_{l-1}[j])^2\\right] =\n",
    " \\sum_j \\mathbb{E}[ \\mathbb{E} \\left[(\\max \\{0, u_{l-1}[j])^2 | u_{l-1}[j]\\right]] =\n",
    " \\sum_j (p(u_{l-1}[j] \\leq 0) \\cdot 0 + p(u_{l-1}[j] < 0) \\cdot \\mathbb{E} \\left[(u_{l-1}[j]^2)\\right] = $$\n",
    " $$ \\sum_j p(u_{l-1}[j] > 0) \\cdot \\mathbb{E} \\left[(u_{l-1}[j]^2)\\right] $$\n",
    "\n",
    "1. Now if we will assume that the distribution of $u$ is symmetric, and from what we've seen in the lecture that $ \\mathbb{E} \\left[(u_{l-1}[j])\\right] = 0 $ Which holds if $ \\mathbb{E} \\left[W\\right] = 0 $\n",
    "We can conclude that $ p(u_{l-1}[j] > 0) = 1/2 $\n",
    "Therefor under the assumption of symmetric distribution and that $\\mathbb{E} \\left[(u_{l-1}[j]^2)\\right] = 1$:\n",
    "$$ \\sum_j p(u_{l-1}[j] > 0) \\cdot \\mathbb{E} \\left[(u_{l-1}[j]^2)\\right] =  \\sum_j \\frac{1}{2} = \\frac{d_{l-1}}{2} \\Rightarrow$$\n",
    "$$\\Rightarrow \\sigma_l = \\sqrt{\\frac{2}{d_{l-1}}}$$\n",
    "\n",
    "2. Using the central limit theorem for large width we can determine that $u_{l-1}[j]$ is approximately distributed as a Gaussian Random Variable.\n",
    "Meaning that $u_{l-1}[j] \\sim \\mathcal{N} (0,1)$ and this Gaussian distribution is also symmetrical which leads us to the same result of 1.\n",
    "\n",
    "$$ \\sum_j p(u_{l-1}[j] > 0) \\cdot \\mathbb{E} \\left[(u_{l-1}[j]^2)\\right] =  \\sum_j \\frac{1}{2} = \\frac{d_{l-1}}{2} \\Rightarrow$$\n",
    "$$\\Rightarrow \\sigma_l = \\sqrt{\\frac{2}{d_{l-1}}}$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsqSFZG1pGhj"
   },
   "source": [
    "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 5 - MLP and Invaraince\n",
    "---\n",
    "\n",
    "You have to design an MLP with the following input: DNA sequences of length $d$. The DNA is a sequence of bases, where each base can be one of 4 options: $(C, T, G, A)$. Thus, the input can be described as the following matrix: $$ X \\in \\mathcal{R}^{4 \\times d}, $$ where $X[j,i]$ denotes the measured value of base concentration of the $j^{th}$ base at location $i$. \n",
    "\n",
    "The network should output a **binary** classification $y \\in \\{-1, 1\\}$ for a specific property we wish to find. The network will be trained on samples $\\{X^{(n)}, y^{(n)} \\}_{n=1}^{N}$, with a **logistic loss function**.\n",
    "\n",
    "First, we will examine a network with 1 hidden layerof size $4 \\times d$ and a **LeakyReLU** activation $\\phi$: $$ f_w(X) = \\sum_{r=1}^{4}\\sum_{k=1}^d W_2[r,k]\\phi\\left(\\sum_{j=1}^{4}\\sum_{i=1}^d W_1[r, k,j, i]X[j, i] \\right),$$ where $w=\\{W_1, W_2\\}$ are the layers of the weight **tensors**. After training is done, the classification will be done with $\\text{sign}(f(X))$.\n",
    "\n",
    "1. Which invariances exist in the network's parameters?\n",
    "2. Now, we notice the fact that: the *direction* in which the DNA is scanned is arbitrary. Thus, if for two inputs $X, \\tilde{X}$: $$ \\forall i,j: \\: X[j,i] = \\tilde{X}[j, d-i+1], $$ then the two inputs are **equivalent** in their meaning. What constraints should we put on the network's parameters to improve the network's classification performance? Explain why using an **invariant hidden layer** is not optimal.\n",
    "3. After that, we now recall that the DNA bases come in pairs, and thus if for two inputs $X, \\tilde{X}$: $$ \\forall i,j : \\: X[j,i] = \\tilde{X}[(4-j)\\text{mod}4 + 1,i] = \\tilde{X}[5-j,i], $$ then the two inputs are **equivalent** in their meaning. What constraints should we put on the network's parameters to improve the network's classification performance?\n",
    "4. We now notice that the measurement process in noisy, each sample $X^{(n)}$ is in arbitrary scale, and thus if for two $X, \\tilde{X}$: $$ \\forall i,j: \\: X[j,i] = c\\tilde{X}[j,i], $$ for some constant $c>0$, then the two inputs are **equivalent** in their meaning.\n",
    "    * (a) For the given network, that **is already trained**, what is the effect of the scale $c$ on the classification result?\n",
    "    * (b) Can the arbitrary scale hurt the training process? Hint: think what happens to the gradient of each sample.\n",
    "    * (c) How can use this information to improve the classifier performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 5 - Answer\n",
    "\n",
    "**Section 1**\n",
    "--\n",
    "\n",
    "The network has rescaled symmetry because we have LeakyReLU activation function.  Translation invariance doesn't\n",
    "exist in binary classification because the activation at the end is logistic function which is not normalized thus, has no\n",
    "translation invariance as happens in softmax\n",
    "\n",
    "\n",
    "**Section 2**\n",
    "--\n",
    "$$ f_w(X) = \\sum_{r=1}^{4}\\sum_{k=1}^d W_2[r,k]\\phi\\left(\\sum_{j=1}^{4}\\sum_{i=1}^d W_1[r, k,j, i]X[j, i] \\right)=f_w(\\tilde{X})= \\sum_{r=1}^{4}\\sum_{k=1}^d W_2[r,k]\\phi\\left(\\sum_{j=1}^{4}\\sum_{i=1}^d W_1[r, k,j, i]\\tilde{X}[j, i] \\right)$$\n",
    "\n",
    "$$=_{index-shifting:p=d-i+1}\\sum_{r=1}^{4}\\sum_{k=1}^d W_2[r,k]\\phi\\left(\\sum_{j=1}^{4}\\sum_{p=1}^d W_1[r, k,j,d-p+1]\\tilde{X}[j, d-p+1] \\right)$$\n",
    "$$=_{p→i}\\sum_{r=1}^{4}\\sum_{k=1}^d W_2[r,k]\\phi\\left(\\sum_{j=1}^{4}\\sum_{i=1}^d W_1[r, k,j,d-i+1]X[j, i] \\right)$$\n",
    "\n",
    "\n",
    "assuming $X[j, i]$ orthogonal, lets compare coefficients and we will get:\n",
    "\n",
    "# $$W_1[r, k,j,i]= W_1[r, k,j,d-i+1] $$\n",
    "\n",
    "Using an invariant hidden layer is not optimal because it fails to capture the reverse symmetry present in the DNA sequences X.\n",
    "it happens because if the hidden layer is invariant, it means that it treats ${X}$ and $\\tilde{X}$ as equivalent inputs and produces the same output and therefore after that layer we wont be able to tell if the input were ${X}$ or $\\tilde{X}$ and it might matter.\n",
    "\n",
    "\n",
    "**Section 3**\n",
    "--\n",
    "$$ f_w(X) = \\sum_{r=1}^{4}\\sum_{k=1}^d W_2[r,k]\\phi\\left(\\sum_{j=1}^{4}\\sum_{i=1}^d W_1[r, k,j, i]X[j, i] \\right)=f_w(\\tilde{X})= \\sum_{r=1}^{4}\\sum_{k=1}^d W_2[r,k]\\phi\\left(\\sum_{j=1}^{4}\\sum_{i=1}^d W_1[r, k,j, i]\\tilde{X}[j, i] \\right)$$\n",
    "\n",
    "$$=_{index-shifting:p=5-j}\\sum_{r=1}^{4}\\sum_{k=1}^d W_2[r,k]\\phi\\left(\\sum_{p=1}^{4}\\sum_{i=1}^d W_1[r, k,5-p,i]\\tilde{X}[5-p, i] \\right)$$\n",
    "$$=_{p→j}\\sum_{r=1}^{4}\\sum_{k=1}^d W_2[r,k]\\phi\\left(\\sum_{j=1}^{4}\\sum_{i=1}^d W_1[r, k,5-j,i]X[j, i] \\right)$$\n",
    "\n",
    "\n",
    "assuming $X[j, i]$ orthogonal, lets compare coefficients and we will get:\n",
    "\n",
    "# $$W_1[r, k,j,i]= W_1[r, k,5-j,i] $$\n",
    "\n",
    "**Section 4**\n",
    "--\n",
    "(a). For the given network, that **is already trained**, there is an effect of the scale $c$ on the classification result.\n",
    "for example if X classified as 1 and $\\tilde{X}$ classified as -1 scaling $\\tilde{X}$ by the factor $c$ may result in a wrong classification. no good.\n",
    "(b). the arbitrary scale will not hurt the training process. the reason is the when we calculate gradients with backpropogation the scale factor c cancels.\n",
    "(c). since scaling by c will not ruin our training process we can use it as transformer to get more robust model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsqSFZG1pGhj"
   },
   "source": [
    "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 6 -VGG Architecture\n",
    "---\n",
    "\n",
    "1.The VGG-11 CNN architecture consists of 11 convolution (CONV)/fully-connected (FC) layers (every CONV layer has the same padding and stride, every MAXPOOL layer is 2×2 and has padding of 0 and stride 2). Fill in the table. You need to **consider the bias**.\n",
    "\n",
    "\n",
    " * CONV$M$-$N$: a convolutional layer of size $M \\times M \\times N$, where $M$ is the kernel size and $N$ is the number of filters. $stride=1, padding=1$.\n",
    " * POOL2: $2 \\times 2$ Max Pooling with $stride=2$\n",
    "     * In case the input of the layer is odd, you should round down. For example, if the output of the layer should be $3.5 \\times 3.5 \\times 3$, you should round to $3 \\times 3 \\times 3$ (i.e., ignore the last column of the input image when performing MaxPooling).\n",
    " * FC-N: a fully connected layer with $N$ neurons.\n",
    "\n",
    "\n",
    "    | Layer  | Output Dimension | Number of Parameters (Weights) |\n",
    "    |---|------------------|--------------------------------|\n",
    "    | INPUT  | 224x224x3        | 0                              |\n",
    "    |  CONV3-64 | 224x224x64       | 3x3x3x64+64=1792               |\n",
    "    | ReLU | 224x224x64       | 0                              |\n",
    "    | POOL2| 112x112x64       | 0                              |\n",
    "    |CONV3-128 | 112x112x128      | 3x3x128x64+128=73856           |\n",
    "    |ReLU | 112x112x128      | 0                              |\n",
    "    | POOL2| 56x56x128        | 0                              |\n",
    "    |CONV3-256 | 56x56x256        | 3x3x128x256+256=295168         |\n",
    "    |ReLU | 56x56x256        | 0                              |\n",
    "    |CONV3-256 | 56x56x256        | 3x3x256x256+256=590080         |\n",
    "    |ReLU | 56x56x256        | 0                              |\n",
    "    | POOL2| 28x28x256        | 0                              |\n",
    "    |CONV3-512 | 28x28x512        | 3x3x256x512+512=1180160        |\n",
    "    |ReLU | 28x28x512        | 0                              |\n",
    "    |CONV3-512 | 28x28x512        | 3x3x512x512+512=2359808        |\n",
    "    |ReLU | 28x28x512        | 0                              |\n",
    "    | POOL2| 14x14x512        | 0                              |\n",
    "    |CONV3-512 | 14x14x512        | 3x3x512x512+512=2359808        |\n",
    "    |ReLU | 14x14x512        | 0                              |\n",
    "    |CONV3-512 | 14x14x512        | 3x3x512x512+512=2359808        |\n",
    "    |ReLU | 14x14x512        | 0                              |\n",
    "    | POOL2| 7x7x512          | 0                              |\n",
    "    | FC-4096| 1x4096             | 7x7x512x4096+4096=102764544    |\n",
    "    | FC-4096| 1x4096             | 4096x4096+4096=16781312        |\n",
    "    | FC-1000| 1x1000             | 4096x1000+1000=4097000         |\n",
    "    | SOFTMAX| 1x1000             | 0                              |\n",
    "\n",
    "2.What is the total number of parameters? (use a calculator for this one)\n",
    "\n",
    "The total number of parameters is 132,863,336\n",
    "\n",
    "3.What percentage of the weights are found in the fully-connected layers?\n",
    "\n",
    "The percentage of the weights are found in the fully-connected layers is 93.06%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7D-14iM7pGhm"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/officel/80/000000/code.png\" style=\"height:50px;display:inline\"> Part 2 - Code Assignments\n",
    "---\n",
    "* You must write your code in this notebook and save it with the output of all of the code cells.\n",
    "* Additional text can be added in Markdown cells.\n",
    "* You can use any other IDE you like (PyCharm, VSCode...) to write/debug your code, but for the submission you must copy it to this notebook, run the code and save the notebook with the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tips\n",
    "---\n",
    "1. Uniformly distributed tensors - `torch.Tensor(dim1, dim2, ...,dimN).uniform_(-1, 1)`\n",
    "2. Separation to **validation set** in PyTorch - <a href=\"https://gist.github.com/MattKleinsmith/5226a94bad5dd12ed0b871aed98cb123\">See example here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "ExecuteTime": {
     "end_time": "2023-05-24T19:46:49.731586Z",
     "start_time": "2023-05-24T19:46:49.702622Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7f9160b90408>"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports for the practice (you can add more if you need)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, ConcatDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from kornia import augmentation as k\n",
    "from kornia.augmentation import AugmentationSequential\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "# %matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 211\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 1 - The Importance of Activation and Initialization\n",
    "---\n",
    "In this task, we are going to use $x \\in \\mathcal{R}^{512}$ and simple neural network that outputs $f(x) \\in \\mathcal{R}^{512}$. The network will have 100 layers with 512 units in each layer.\n",
    "\n",
    "1. We initialize the weights from a unit normal distribution. Run the following code cell and explain what happens. Add a short piece of code that locates when it happens (hint: use `torch.isnan()`). **Print** the layer number.\n",
    "2. We can demonstrate that at a given layer, the matrix product of inputs $x$ and weight matrix $a$ that is initialized from a standard normal distribution will, on average, have a standard deviation very close to the square root of the number of input connections. For our example, with 512 dimensions, show that for 10,000 multiplications of $a$ and $x$, the empirical standard deviation is similar to the square root of the number of input connections. Use the unbiased version: $$ \\hat{std} = \\sqrt{\\frac{\\sum_{i=1}^{10000}\\frac{1}{N}\\sum_{j=1}^N y^2}{10000}}, $$ where $y=ax$ and $N$ is the number of input connections. **Print** the mean, std and the square root of the number of input connections.\n",
    "3. For the code from 1, normalize the weight initialization by the square root of the input connections. How does that change the outcome? **Print** the mean and std after the modification.\n",
    "4. Add a `tanh()` activation after each layer for the code from 1. **Print** the mean and std after the modification. Explain the result.\n",
    "5. Xavier initialization sets a layer’s weights to values chosen from a random uniform distribution that’s bounded between $$\\pm \\sqrt{\\frac{6}{n_i + n_{i+1}}}$$ where $n_i$ is the number of incoming network connections, or “fan-in,” to the layer, and $ n_{i+1}$ is the number of outgoing network connections from that layer, also known as the “fan-out”. Glorot and Bengio believed that Xavier weight initialization would maintain the variance of activations and back-propagated gradients all the way up or down the layers of a network and demonstrated that networks initialized with Xavier achieved substantially quicker convergence and higher accuracy. Implement **Xavier Uniform** as `xavier_init(fan_in, fan_out)`, a function that returns a tensor initialized according to **Xavier Uniform**. Use it on the simple network from 1 with `tanh` activation. **Print** the mean and std after the modification.\n",
    "6. If you try to replace the `tanh` activation with `relu` activation in section 5, you will see very different results. Xavier strives to acheive activation outputs of each layer to have a mean of 0 and a standard deviation around 1, on average. When using a ReLU activation, a single layer will, on average have standard deviation that’s very close to the square root of the number of input connections, **divided by the square root of two** ($\\sqrt{\\frac{512}{2}}$ in our example). **Kaiming He et. al.** proposed an initialization scheme that’s tailored for deep neural nets that use these kinds of asymmetric, non-linear activations. Implement **Kaiming Normal** as `kaiming_init(fan_in, fan_out)`, a function that returns a tensor initialized according to **Kaiming Normal** (use `fan_in` mode). Use it on the simple network from 1 with `relu` activation. **Print** the mean and std after the modification. What happens when you use Xavier with RelU activation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "ExecuteTime": {
     "end_time": "2023-05-24T10:59:44.168709Z",
     "start_time": "2023-05-24T10:59:43.464087Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All x values are NaN at iteration number = 29\n",
      "tensor(nan) tensor(nan)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "is_nan = False\n",
    "for i in range(100):\n",
    "    a = torch.randn(512, 512)\n",
    "    x = a @ x\n",
    "    if False in torch.isnan(x):\n",
    "        continue\n",
    "    elif not is_nan:\n",
    "        is_nan = True\n",
    "        print(f\"All x values are NaN at iteration number = {i}\")\n",
    "print(x.mean(), x.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Answer\n",
    "1. What happend is that the vector x was reduced to NaN values, this happend because $ a \\sim \\mathcal{N}[0,1] $ and $ x \\sim \\mathcal{N}[0,1] $, And each layer of the network is a linear layer that performs $ h = ax $ as we go deeper into the layers, x values continue to be increased by the a values until finally the numerical representation of x isn't possible via this computer so it is reduced to NaN values, and we can see that in layer 29 we first get that *ALL* of x's values are NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T11:34:23.942671Z",
     "start_time": "2023-05-24T11:33:47.483302Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:36<00:00, 275.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical STD = 22.618465423583984\n",
      "Calculated Mean = 0.006609560456126928\n",
      "Calculated STD = 22.596790313720703\n",
      "Square root of 512 is 22.627416997969522\n"
     ]
    }
   ],
   "source": [
    "def approx_std(mul=10000):\n",
    "    sum = 0\n",
    "    y_mean = list()\n",
    "    y_std = list()\n",
    "    for i in tqdm(range(mul)):\n",
    "        x = torch.randn(512)\n",
    "        a = torch.randn(512, 512)\n",
    "        y = a @ x\n",
    "        y_mean.append(y.mean())\n",
    "        y_std.append(y.std())\n",
    "        sum += torch.sum((1 / len(x)) * ((y)**2))\n",
    "    return np.sqrt(sum/mul), np.mean(y_mean), np.mean(y_std)\n",
    "\n",
    "\n",
    "N = 512\n",
    "emp_std, calc_mean, calc_std = approx_std()\n",
    "print(f'Empirical STD = {emp_std}')\n",
    "print(f'Calculated Mean = {calc_mean}')\n",
    "print(f'Calculated STD = {calc_std}')\n",
    "print(f'Square root of {N} is {np.sqrt(N)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0112) tensor(0.5600)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "for i in range(100):\n",
    "    a = torch.randn(512, 512) / np.sqrt(512)\n",
    "    x = a @ x\n",
    "print(x.mean(), x.std())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T11:34:25.764060Z",
     "start_time": "2023-05-24T11:34:25.423174Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. We can see that even this initialization removes the problem of massively increasing values through the feed forward step and no more NaN values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0024) tensor(0.0511)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "for i in range(100):\n",
    "    a = torch.randn(512, 512) / np.sqrt(512)\n",
    "    x = torch.tanh(a @ x)\n",
    "print(x.mean(), x.std())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T11:34:30.217763Z",
     "start_time": "2023-05-24T11:34:29.873409Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Here the tanh activation always forces the ouput of each layer to remain in the interval [-1,1] and by doing so eliminating the massively increasing values through the feed forward step"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0032) tensor(0.0715)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/itamarnierenberg/opt/anaconda3/envs/deep_learn/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "def xavier_init(fan_in, fan_out):\n",
    "    a = torch.FloatTensor(fan_in, fan_out)\n",
    "    nn.init.xavier_uniform(a)\n",
    "    return a\n",
    "\n",
    "x = torch.randn(512)\n",
    "for i in range(100):\n",
    "    a = xavier_init(512, 512)\n",
    "    x = torch.tanh(a @ x)\n",
    "print(x.mean(), x.std())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T11:34:32.435367Z",
     "start_time": "2023-05-24T11:34:32.313039Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6979) tensor(1.0576)\n"
     ]
    }
   ],
   "source": [
    "def kaiming_init(fan_in, fan_out):\n",
    "    a = torch.FloatTensor(fan_in, fan_out)\n",
    "    nn.init.kaiming_normal_(a, mode='fan_in', nonlinearity='relu')\n",
    "    return a\n",
    "\n",
    "\n",
    "x = torch.randn(512)\n",
    "for i in range(100):\n",
    "    a = kaiming_init(512, 512)\n",
    "    x = torch.relu(a @ x)\n",
    "print(x.mean(), x.std())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T11:34:34.957520Z",
     "start_time": "2023-05-24T11:34:34.615391Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3480e-16) tensor(3.4399e-16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/itamarnierenberg/opt/anaconda3/envs/deep_learn/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "for i in range(100):\n",
    "    a = xavier_init(512, 512)\n",
    "    x = torch.relu(a @ x)\n",
    "print(x.mean(), x.std())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-25T07:16:30.662875Z",
     "start_time": "2023-05-25T07:16:30.503337Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. When using xavier initilization with ReLU the weights are redudced to values very close to 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 2 - MLP-based Deep Classifer\n",
    "---\n",
    "In this task you are going to design and train your first neural network for classification.\n",
    "\n",
    "For this task, we will use the \"<a href=\"https://archive.ics.uci.edu/ml/datasets/MAGIC+Gamma+Telescope\">MAGIC Gamma Telescope Data Set\"</a>. Cherenkov gamma telescope observes high energy gamma rays, taking advantage of the radiation emitted by charged particles produced inside the electromagnetic showers initiated by the gammas, and developing in the atmosphere. This Cherenkov radiation (of visible to UV wavelengths) leaks through the atmosphere and gets recorded in the detector, allowing reconstruction of the shower parameters. The available information consists of pulses left by the incoming Cherenkov photons on the photomultiplier tubes, arranged in a plane, the camera. \n",
    "\n",
    "Depending on the energy of the primary gamma, a total of few hundreds to some 10000 Cherenkov photons get collected, in patterns (called the shower image), allowing to discriminate statistically those caused by primary gammas (**signal**) from the images of hadronic showers initiated by cosmic rays in the upper atmosphere (**background**).\n",
    "\n",
    "Our data has 10 features and 2 classes (signal and background).\n",
    "\n",
    "1. Load the MAGIC dataset sored in `magic04.data` and display the first 5 features (just run the cell).\n",
    "2. Separate the data to train, validation and test, reserve 10% of the data for validation and 20% for test.\n",
    "3. Perform pre-processing steps of your choice and convert the class label from `str` to `int` (for example, `y_train = np.array([0 if y_train[i] == 'g' else 1 for i in range(len(y_train))]).astype(np.int)`).\n",
    "4. Train a Logistic Regression model from `sklearn` as a baseline for our neural network (only for this section use both the train and validation sets for training the classifier). **Print the test accuracy**.\n",
    "5. Convert the `numpy` arrays to `torch` tensors with `TensorDataset` as done in the tutorial.\n",
    "6. Design a **MLP** to classify the data. Optimize the hyper-parameters of your model using the accuracy on the validation set, and when you are satisfied with the model train it on both the train and validation sets and evaluate it on the test set. **You need to reach at least 85% accuracy on the test set, and 87% for a full grade**.\n",
    "    * You have a free choice of architecture, optimizer, learning scheduler, initialization, regularization and activations.\n",
    "    * The loss criterion is binary cross entropy: `nn.BCEWithLogitsLoss()` (performs `sigmoid` for you) or `nn.BCELoss` (you need to apply `sigmoid` on the network output yourself).\n",
    "    * In a Markdown block, write down the chosen architectures and all the hyper-parameters.\n",
    "        * Make sure to describe any design choice that you used to improve the performance (e.g. if you used a certain regularization or layer, mention it and describe why you think it helped).\n",
    "    * **Plot** the loss curves (and any oter statistic you want) as a function of epochs/iterations. **Print** the final performance.\n",
    "    * **Print** the test accuracy.\n",
    "3. Change the initialization of the linear layers and re-train the model (with the same optimal hyper-parameters you found). You can pick an initialization of your choosing from : https://pytorch.org/docs/stable/nn.init.html . See example below how to use. **Print** the change in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T11:35:47.443850Z",
     "start_time": "2023-05-24T11:35:47.347432Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "    fLength    fWidth   fSize   fConc  fConc1     fAsym  fM3Long  fM3Trans  \\\n0   28.7967   16.0021  2.6449  0.3918  0.1982   27.7004  22.0110   -8.2027   \n1   31.6036   11.7235  2.5185  0.5303  0.3773   26.2722  23.8238   -9.9574   \n2  162.0520  136.0310  4.0612  0.0374  0.0187  116.7410 -64.8580  -45.2160   \n3   23.8172    9.5728  2.3385  0.6147  0.3922   27.2107  -6.4633   -7.1513   \n4   75.1362   30.9205  3.1611  0.3168  0.1832   -5.5277  28.5525   21.8393   \n\n    fAlpha     fDist class  \n0  40.0920   81.8828     g  \n1   6.3609  205.2610     g  \n2  76.9600  256.7880     g  \n3  10.4490  116.7370     g  \n4   4.6480  356.4620     g  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fLength</th>\n      <th>fWidth</th>\n      <th>fSize</th>\n      <th>fConc</th>\n      <th>fConc1</th>\n      <th>fAsym</th>\n      <th>fM3Long</th>\n      <th>fM3Trans</th>\n      <th>fAlpha</th>\n      <th>fDist</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>28.7967</td>\n      <td>16.0021</td>\n      <td>2.6449</td>\n      <td>0.3918</td>\n      <td>0.1982</td>\n      <td>27.7004</td>\n      <td>22.0110</td>\n      <td>-8.2027</td>\n      <td>40.0920</td>\n      <td>81.8828</td>\n      <td>g</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>31.6036</td>\n      <td>11.7235</td>\n      <td>2.5185</td>\n      <td>0.5303</td>\n      <td>0.3773</td>\n      <td>26.2722</td>\n      <td>23.8238</td>\n      <td>-9.9574</td>\n      <td>6.3609</td>\n      <td>205.2610</td>\n      <td>g</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>162.0520</td>\n      <td>136.0310</td>\n      <td>4.0612</td>\n      <td>0.0374</td>\n      <td>0.0187</td>\n      <td>116.7410</td>\n      <td>-64.8580</td>\n      <td>-45.2160</td>\n      <td>76.9600</td>\n      <td>256.7880</td>\n      <td>g</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>23.8172</td>\n      <td>9.5728</td>\n      <td>2.3385</td>\n      <td>0.6147</td>\n      <td>0.3922</td>\n      <td>27.2107</td>\n      <td>-6.4633</td>\n      <td>-7.1513</td>\n      <td>10.4490</td>\n      <td>116.7370</td>\n      <td>g</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>75.1362</td>\n      <td>30.9205</td>\n      <td>3.1611</td>\n      <td>0.3168</td>\n      <td>0.1832</td>\n      <td>-5.5277</td>\n      <td>28.5525</td>\n      <td>21.8393</td>\n      <td>4.6480</td>\n      <td>356.4620</td>\n      <td>g</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the data\n",
    "col_names = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym',  'fM3Long', 'fM3Trans', 'fAlpha', 'fDist', 'class']\n",
    "feature_names = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym',  'fM3Long', 'fM3Trans', 'fAlpha', 'fDist']\n",
    "data = pd.read_csv(\"./magic04.data\", names=col_names)\n",
    "X = data[feature_names]\n",
    "Y = data['class']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T11:35:49.372324Z",
     "start_time": "2023-05-24T11:35:49.261394Z"
    }
   },
   "outputs": [],
   "source": [
    "# separate to train, test\n",
    "lengths = [int(len(data) * 0.7), int(len(data) * 0.1), int(len(data) * 0.2)]\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(data, lengths)\n",
    "X_train = train_set.dataset.drop(['class'], axis=1)\n",
    "X_val = val_set.dataset.drop(['class'], axis=1)\n",
    "X_test = test_set.dataset.drop(['class'], axis=1)\n",
    "y_train = train_set.dataset['class']\n",
    "y_val = val_set.dataset['class']\n",
    "y_test = test_set.dataset['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T11:35:50.978001Z",
     "start_time": "2023-05-24T11:35:50.846863Z"
    }
   },
   "outputs": [],
   "source": [
    "# pre-processing and converting labels to integers\n",
    "y_train = np.array([0 if y_train[i] == 'g' else 1 for i in range(len(y_train))]).astype(np.int)\n",
    "y_val = np.array([0 if y_val[i] == 'g' else 1 for i in range(len(y_val))]).astype(np.int)\n",
    "y_test = np.array([0 if y_test[i] == 'g' else 1 for i in range(len(y_test))]).astype(np.int)\n",
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "X_test = np.array(X_test)\n",
    "y_train_np = np.concatenate((y_train, y_val))\n",
    "y_test_np = y_test\n",
    "X_train_prep = np.concatenate((X_train, X_val), axis=0)\n",
    "X_test_prep = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T11:35:53.900747Z",
     "start_time": "2023-05-24T11:35:53.714281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points 8022 out of 19020 total points.\n",
      "Logistic Regression Model accuracy = 0.7891167192429022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/itamarnierenberg/opt/anaconda3/envs/deep_learn/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# training a Logistic Regression baseline - complete the code with your variables\n",
    "logstic_model = LogisticRegression(solver='lbfgs')\n",
    "model = logstic_model.fit(X_train_prep, y_train_np)\n",
    "y_pred = model.predict(X_train_prep)\n",
    "print(\"Number of mislabeled points %d out of %d total points.\"% ((y_train_np != y_pred).sum(), X_train.shape[0]))\n",
    "print(\"Logistic Regression Model accuracy =\" , logstic_model.score(X_test_prep, y_test_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T11:35:56.071480Z",
     "start_time": "2023-05-24T11:35:56.064866Z"
    }
   },
   "outputs": [],
   "source": [
    "# create TensorDataset from numpy arrays\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float())\n",
    "val_ds = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).float())\n",
    "test_ds = TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T07:14:43.477193Z",
     "start_time": "2023-05-25T06:36:59.025923Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:25<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Learning Rate = 0.01 and Batch Size = 128 the validation accuracy is = 88.68033648790747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:00<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Learning Rate = 0.01 and Batch Size = 256 the validation accuracy is = 88.03364879074658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:46<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Learning Rate = 0.01 and Batch Size = 512 the validation accuracy is = 91.90325972660357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:25<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Learning Rate = 0.04 and Batch Size = 128 the validation accuracy is = 83.26498422712933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:01<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Learning Rate = 0.04 and Batch Size = 256 the validation accuracy is = 88.95899053627761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:44<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Learning Rate = 0.04 and Batch Size = 512 the validation accuracy is = 87.94426919032597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:24<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Learning Rate = 0.07 and Batch Size = 128 the validation accuracy is = 87.66561514195584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:01<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Learning Rate = 0.07 and Batch Size = 256 the validation accuracy is = 88.75920084121977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:47<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Learning Rate = 0.07 and Batch Size = 512 the validation accuracy is = 86.74553101997897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:26<00:00,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Learning Rate = 0.09 and Batch Size = 128 the validation accuracy is = 89.96845425867508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:01<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Learning Rate = 0.09 and Batch Size = 256 the validation accuracy is = 86.56677181913774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:47<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Learning Rate = 0.09 and Batch Size = 512 the validation accuracy is = 86.85068349106204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:28<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Learning Rate = 0.1 and Batch Size = 128 the validation accuracy is = 86.34595162986331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:00<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Learning Rate = 0.1 and Batch Size = 256 the validation accuracy is = 89.931650893796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:46<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Learning Rate = 0.1 and Batch Size = 512 the validation accuracy is = 90.98843322818087\n",
      "Selected Learning Rate = 0.01, Selected Batch Size = 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:17<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 87.95478443743427\n",
      "Using Personal Weight initializaion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:16<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with personal weight initialization: 89.73186119873817\n",
      "Change in Accuracy (With weight initialization) - (Withouts weight initialization) = 1.7770767613039027\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 576x360 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model, hyperparameters and training\n",
    "class MyFirstNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim=1, change_weight_initialization=False):\n",
    "        super(MyFirstNN, self).__init__()\n",
    "        self.hidden = nn.Sequential(nn.Linear(input_dim, 20),\n",
    "                                    nn.Linear(20, 16),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(16, 12),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(12, 8),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(8, 4),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(4, output_dim))\n",
    "        self.output_layer = nn.Sigmoid()\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if change_weight_initialization:\n",
    "            self.init_weights()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.output_layer(self.hidden(X))\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # pick initialzation: https://pytorch.org/docs/stable/nn.init.html\n",
    "                # examples\n",
    "                # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                # nn.init.constant(m.bias, 0)\n",
    "                # don't forget the bias term (m.bias)\n",
    "\n",
    "    def calc_accuracy(self, test_tensor, batch_size):\n",
    "        self.eval()\n",
    "        test_loader = DataLoader(dataset=test_tensor, batch_size=batch_size, shuffle=False)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for features, targets in test_loader:\n",
    "                features.to(self.device)\n",
    "                targets.to(self.device)\n",
    "                outputs = self(features)\n",
    "                idx = 0\n",
    "                for output in outputs:\n",
    "                    if float(output) >= 0.5 and int(targets[idx]) == 1:\n",
    "                        correct += 1\n",
    "                    elif int(output) < 0.5 and int(targets[idx]) == 0:\n",
    "                        correct += 1\n",
    "                    total += 1\n",
    "                    idx += 1\n",
    "                accuracy = 100 * (correct/total)\n",
    "        return accuracy\n",
    "\n",
    "def train_model(train_set, num_features=10, output_dim=1, batch_size=128, learning_rate=0.1, num_epochs=500, plot=False, init_weights=False):\n",
    "    model = MyFirstNN(num_features, output_dim, change_weight_initialization=init_weights)\n",
    "    model = model.to(model.device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    train_data_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    total_losses = []\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        epoch_losses = []\n",
    "        for features, targets in train_data_loader:\n",
    "            features = features.to(model.device)\n",
    "            targets = targets.to(model.device)\n",
    "            output = model(features)\n",
    "            loss = model.criterion(output.view(-1), targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_losses.append(loss.item())\n",
    "        # if epoch % 50 == 0:\n",
    "        #         print(f'epoch: {epoch} loss: {np.mean(epoch_losses)}')\n",
    "        total_losses.append(epoch_losses)\n",
    "    if plot:\n",
    "        plot_loss_function(total_losses)\n",
    "    return model\n",
    "\n",
    "def plot_loss_function(loss_list):\n",
    "    mean_loss = []\n",
    "    for loss in loss_list:\n",
    "        mean_loss.append(np.mean(loss))\n",
    "    mean_loss = np.array(mean_loss)\n",
    "    fig = plt.figure(figsize=(8, 5)) # create a figure, just like in matlab\n",
    "    ax = fig.add_subplot(1, 1 ,1) # create a subplot of certain size\n",
    "    ax.plot(mean_loss, label=\"Loss Per Epoch\")\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel(\"Mean Epoch Loss\")\n",
    "    ax.set_title(\"Epoch\")\n",
    "    ax.grid()\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "possible_learning_rates = [0.01, 0.04, 0.07, 0.09, 0.1]\n",
    "possible_batch_sizes = [128, 256, 512]\n",
    "results = []\n",
    "for lr in possible_learning_rates:\n",
    "    for b_size in possible_batch_sizes:\n",
    "        my_model = train_model(train_ds, num_features=10, output_dim=1, batch_size=b_size, learning_rate=lr, num_epochs=500)\n",
    "        accuracy = my_model.calc_accuracy(val_ds, b_size)\n",
    "        results.append((lr, b_size, accuracy))\n",
    "        print(f'For Learning Rate = {lr} and Batch Size = {b_size} the validation accuracy is = {accuracy}')\n",
    "\n",
    "max_acc = 0\n",
    "max_acc_idx = 0\n",
    "idx = 0\n",
    "for result in results:\n",
    "    if result[2] > max_acc:\n",
    "        max_acc = result[2]\n",
    "        max_acc_idx = idx\n",
    "    idx += 1\n",
    "print(f'Selected Learning Rate = {results[max_acc_idx][0]}, Selected Batch Size = {results[max_acc_idx][1]}')\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train_prep).float(), torch.from_numpy(y_train_np).float())\n",
    "my_model = train_model(ConcatDataset([train_ds, val_ds]),\n",
    "                       num_features=10,\n",
    "                       output_dim=1,\n",
    "                       batch_size=results[max_acc_idx][1],\n",
    "                       learning_rate=results[max_acc_idx][0],\n",
    "                       num_epochs=500,\n",
    "                       plot=True)\n",
    "test_error_no_weights = my_model.calc_accuracy(test_ds,results[max_acc_idx][1])\n",
    "print(f'Test Accuracy: {test_error_no_weights}')\n",
    "print(f'Using Personal Weight initializaion')\n",
    "my_model = train_model(ConcatDataset([train_ds, val_ds]),\n",
    "                       num_features=10,\n",
    "                       output_dim=1,\n",
    "                       batch_size=results[max_acc_idx][1],\n",
    "                       learning_rate=results[max_acc_idx][0],\n",
    "                       num_epochs=500,\n",
    "                       plot=False,\n",
    "                       init_weights=True)\n",
    "test_error_with_weights = my_model.calc_accuracy(test_ds,results[max_acc_idx][1])\n",
    "print(f'Test Accuracy with personal weight initialization: {test_error_with_weights}')\n",
    "print(f'Change in Accuracy (With weight initialization) - (Withouts weight initialization) = {test_error_with_weights - test_error_no_weights}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Architecture and Hyper Parameters\n",
    "\n",
    "Our design choice was quite conventional, we thought about first trying it the conventional way using Linear layer and RELU activation function as learned in class, and if the results will not be satisfying then we will change the architecture and put more thought in to it.\n",
    "Eventually the basic design of linear layers and RELU activation achieved the accuracy goal so we stuck with it. We choose to decrease the dimensions gradualy as we progress through the network in order to allow better weight training and avoid data loss.\n",
    "\n",
    "\n",
    "The architecture:\n",
    "\n",
    "| Layer  | Input Dimension | Output Dimension |\n",
    "    |---|------------------|--------------------------------|\n",
    "    | Linear  | 20        | 16                              |\n",
    "    |  ReLU | 16       | 16               |\n",
    "    | Linear | 16       | 12                              |\n",
    "    | ReLU| 12       | 12                              |\n",
    "    |Linear | 12      | 8           |\n",
    "    |ReLU | 8      | 8                              |\n",
    "    | Linear| 8        | 4                              |\n",
    "    |ReLU | 4        | 4         |\n",
    "    |Linear | 4        | 1                              |\n",
    "    |Sigmoid | 1        | 1         |\n",
    "\n",
    "\n",
    "The hyper parameters are:\n",
    "Learning Rate\n",
    "Batch Size\n",
    "Num of epochs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 3 - Design a CNN\n",
    "---\n",
    "In this task you are going to design a deep convolutional neural network to classify house number digits from the **The Street View House Numbers (SVHN)** Dataset. \n",
    "\n",
    "SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with minimal requirement on data preprocessing and formatting. It can be seen as similar in flavor to MNIST (e.g., the images are of small cropped digits), but incorporates an order of magnitude more labeled data (over 600,000 digit images) and comes from a significantly harder, unsolved, real world problem (recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images.\n",
    "\n",
    "* 10 classes, 1 for each digit. Digit '0' has label 0, '1' has label 1,...\n",
    "* 73257 digits for training, 26032 digits for testing, and 531131 additional, somewhat less difficult samples, to use as extra training data.\n",
    "\n",
    "<img src=\"http://ufldl.stanford.edu/housenumbers/32x32eg.png\" style=\"height:250px\">\n",
    "\n",
    "1. Load the SVHN dataset with PyTorch using `torchvision.datasets.SVHN(root, split='train', transform=None, target_transform=None, download=True)`, you can read more here: https://pytorch.org/vision/stable/generated/torchvision.datasets.SVHN.html#torchvision.datasets.SVHN. Display 5 images from the train set.\n",
    "2. Design a Convolutional Neural Network (CNN) to classify digits from the images.\n",
    "    * Describe the chosen architecture, how many layers? What activations did you choose? What are the filter sizes? Did you use fully-connected layers (if you did, explain their sizes)?\n",
    "    * What is the input dimension? What is the output dimension?\n",
    "    * Calculate the number of parameters (weights) in the network. What is the model size in Megabytes? (see the convolution tutorial). **Print** these numbers.\n",
    "3. Train the classifier (preferably on a GPU - use Colab for this part if you don't have a GPU).\n",
    "    * Describe the the hyper-parameters of the model (batch size, epochs, learning rate....). How did you tune your model? Did you use a validation set to tune the model?\n",
    "    * What is the final accuracy on the test set? **Print** it.\n",
    "        * You need to reach at least 86% accuracy in this section, and 90% for a full grade.\n",
    "    * **Plot** the loss curves (and any other statistic you calculate) as a function of epochs/iterations.\n",
    "4. For the trained classifier, what is the accuracy on the test set when each test image is added a small noise $a=(0.05, 0.01, 0.005)$: $$ \\text{image} + a \\times \\mathcal{N}(0, 1) $$. **Print** the result for each value of $a$.\n",
    "5. Retrain the classifier, but this time use data augementation of your choosing. Briefly explain what augmentation you chose and how it works. Did the test accuracy improve? **Print** the result.\n",
    "    * You can use transformations available in `torchvision.transforms` as shown in the tutorial.\n",
    "    * You are welcome to use <a href=\"https://kornia.github.io/\">`kornia`</a> for the augmentations (**2 points bonus**, maximal grade is still 100).\n",
    "    * **Plot** the loss curves (and any other statistic you want) as a function of epochs/iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Section 1 Answer**\n",
    "--\n",
    "1. Load the SVHN dataset and Display 5 images from the train set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./datasets/train_32x32.mat\n",
      "5 random pictures:\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 864x180 with 5 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load SVHN dataset\n",
    "train_set = torchvision.datasets.SVHN(\n",
    "    root='./datasets', split='train', transform=None,\n",
    "     target_transform=None, download=True)\n",
    "\n",
    "\n",
    "# Display 5 images from the train set without transformation\n",
    "fig, axes = plt.subplots(1, 5, figsize=(12, 2.5))\n",
    "indices = random.sample(range(len(train_set)), 5)\n",
    "print(f'5 random pictures:')\n",
    "for i, idx in enumerate(indices):\n",
    "    image, label = train_set[idx]\n",
    "    axes[i].imshow(image)\n",
    "    axes[i].set_title(label)\n",
    "    axes[i].set_xticks([])\n",
    "    axes[i].set_yticks([])\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T12:37:55.717174Z",
     "start_time": "2023-05-24T12:37:53.433258Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Section 1.5 Answer**\n",
    "--\n",
    "now we will define pre-processing steps on the images"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./datasets/train_32x32.mat\n",
      "Using downloaded and verified file: ./datasets/extra_32x32.mat\n",
      "Using downloaded and verified file: ./datasets/test_32x32.mat\n",
      "mean (for each channel) is (0.43597240888794314, 0.4420362291555156, 0.4709575299553918) , while std (for each channel) is (0.19725656929785426, 0.20023040743423295, 0.19626646929312347)\n",
      "Using downloaded and verified file: ./datasets/extra_32x32.mat\n",
      "73257\n"
     ]
    }
   ],
   "source": [
    "# getting std and mean for the transform getting it only from the **train_set**\n",
    "mean=(np.mean(train_set.data[:,0])/256,np.mean(train_set.data[:,1])/256,\n",
    "      np.mean(train_set.data[:,2])/256)\n",
    "std=(np.std(train_set.data[:,0])/256,np.std(train_set.data[:,1])/256,\n",
    "     np.std(train_set.data[:,2])/256)\n",
    "\n",
    "# define our transform\n",
    "# Normalize the all sets without augmentation\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # uint8  -> float\n",
    "    transforms.Normalize(mean,std),  ])\n",
    "\n",
    "# Load SVHN dataset\n",
    "train_set =torchvision.datasets.SVHN(\n",
    "    root='./datasets', split='train', transform=transform,\n",
    "     target_transform=None, download=True)\n",
    "val_set = torchvision.datasets.SVHN(\n",
    "    root='./datasets', split='extra', transform=transform,\n",
    "     target_transform=None, download=True)\n",
    "test_set = torchvision.datasets.SVHN(\n",
    "    root='./datasets', split='test', transform=transform,\n",
    "     target_transform=None, download=True)\n",
    "\n",
    "# now we have the entire data set split and Normalized\n",
    "print(f'mean (for each channel) is {mean} , while std (for each channel) is {std}')\n",
    "\n",
    "# Create a Validation set\n",
    "new_val_set = datasets.SVHN(\n",
    "    root=val_set.root,\n",
    "    split=val_set.split,\n",
    "    transform=val_set.transform,\n",
    "    target_transform=val_set.target_transform,\n",
    "    download=val_set.download\n",
    ")\n",
    "\n",
    "# Override the data of the new test set\n",
    "indices = range(len(train_set))\n",
    "new_val_set.data = new_val_set.data[indices]\n",
    "\n",
    "print(len(new_val_set))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T12:38:31.296465Z",
     "start_time": "2023-05-24T12:37:56.400717Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Section 2 Answer**\n",
    "--\n",
    " **Our architecture has:**\n",
    "\n",
    "  4 Convolutional layers,\n",
    "\n",
    "  2 Max pooling layers,\n",
    "\n",
    "  4 Batch normalization layers,\n",
    "\n",
    "  3 Fully-connected layers.\n",
    "\n",
    "**The activations function we chose is** ReLU.\n",
    "\n",
    "**the sizes of the filters we have are:**\n",
    "\n",
    "The **first** convolutional layer has 32 filters with a kernel size of 3x3.\n",
    "\n",
    "The **second** convolutional layer has 32 filters with a kernel size of 3x3.\n",
    "\n",
    "The **third** convolutional layer has 64 filters with a kernel size of 3x3.\n",
    "\n",
    "The **fourth** convolutional layer has 64 filters with a kernel size of 3x3.\n",
    "\n",
    "**we used 3 fully-connected layers:**\n",
    "\n",
    "The first fully-connected layer, has an input size of 4096 and an output size of 512, that means it has ${(4,096+1)}\\cdot 512=2,098,688$.\n",
    "\n",
    "The second fully-connected layer, has an input size of 512 and an output size of 256, that means it has ${(512+1)}\\cdot 256=131,328$.\n",
    "\n",
    "The third fully-connected layer, has an input size of 256 and an output size of 10, that means it has ${(256+1)}\\cdot 10=2,570$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The input dimension is (1, 3, 32, 32)\n",
      "\n",
      " The output dimension is (1, 64, 8, 8)\n",
      "\n",
      " The number of trainable weights in the model is 2297514\n",
      "\n",
      " The model size is 73520448 [bit] which is 8.76 [MBytes] or 9.19 [MBytes] if we calculate MB as shown in the tutorial  \n"
     ]
    }
   ],
   "source": [
    "class SVHN(nn.Module):\n",
    "\n",
    "    def __init__(self,do_p=0.2):\n",
    "        super(SVHN, self).__init__()\n",
    "\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Dropout(p=do_p),\n",
    "            nn.Linear(4096, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=do_p),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 10),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # conv layers\n",
    "        x = self.conv_layer(x)\n",
    "\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # fc layer\n",
    "        x = self.fc_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# how can we calculate the output of the convolution automatically?\n",
    "dummy_input = torch.zeros([1, 3, 32, 32])\n",
    "dummy_model = SVHN()\n",
    "dummy_output = dummy_model.conv_layer(dummy_input)\n",
    "\n",
    "print(f' The input dimension is (1, 3, 32, 32)\\n')\n",
    "print(f' The output dimension is {tuple(dummy_output.shape)}\\n')\n",
    "\n",
    "# how many weights (trainable parameters) we have in our model?\n",
    "num_trainable_params = sum([p.numel() for p in dummy_model.parameters() if p.requires_grad])\n",
    "print(f' The number of trainable weights in the model is {num_trainable_params}\\n')\n",
    "\n",
    "# calculate the model size on disk\n",
    "size_model = 0\n",
    "for param in dummy_model.parameters():\n",
    "    if param.data.is_floating_point():\n",
    "        size_model += param.numel() * torch.finfo(param.data.dtype).bits\n",
    "    else:\n",
    "        size_model += param.numel() * torch.iinfo(param.data.dtype).bits\n",
    "print(f' The model size is {size_model} [bit] which is {size_model / (8*2**20):.2f} [MBytes] or {size_model / 8e6:.2f} [MBytes] if we calculate MB as shown in the tutorial  ')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T12:38:39.977747Z",
     "start_time": "2023-05-24T12:38:39.877722Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Section 3 Answer**\n",
    "--\n",
    "**our hyper parameters are:**\n",
    "\n",
    "learning_rates\n",
    "\n",
    "batch size\n",
    "\n",
    "probability for dropout\n",
    "\n",
    "We chose them using validation set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [06:43<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.8116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [06:42<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Learning Rate = 0.0001 , Batch Size = 64 and the Dropout Probability = 0.3 the validation accuracy is = 93.64019820631475\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 573/573 [06:42<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.9604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 573/573 [06:36<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Learning Rate = 0.0001 , Batch Size = 128 and the Dropout Probability = 0.3 the validation accuracy is = 93.04912841093685\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [06:41<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.7208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [06:41<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Learning Rate = 0.0002 , Batch Size = 64 and the Dropout Probability = 0.3 the validation accuracy is = 94.74862470480636\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 573/573 [06:33<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.7738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 573/573 [06:31<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Learning Rate = 0.0002 , Batch Size = 128 and the Dropout Probability = 0.3 the validation accuracy is = 94.09476227527745\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [06:44<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.6758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [06:43<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Learning Rate = 0.0003 , Batch Size = 64 and the Dropout Probability = 0.3 the validation accuracy is = 94.72541873131578\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 573/573 [06:35<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.7462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 573/573 [06:35<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Learning Rate = 0.0003 , Batch Size = 128 and the Dropout Probability = 0.3 the validation accuracy is = 94.59983346301377\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [06:44<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.9242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [06:43<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Learning Rate = 0.0001 , Batch Size = 64 and the Dropout Probability = 0.4 the validation accuracy is = 93.31258446291822\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 573/573 [06:36<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.0870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 573/573 [06:33<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Learning Rate = 0.0001 , Batch Size = 128 and the Dropout Probability = 0.4 the validation accuracy is = 92.43075747027588\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [06:42<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.7762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [06:43<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Learning Rate = 0.0002 , Batch Size = 64 and the Dropout Probability = 0.4 the validation accuracy is = 94.4701530229193\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 573/573 [06:37<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.8762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 573/573 [06:36<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Learning Rate = 0.0002 , Batch Size = 128 and the Dropout Probability = 0.4 the validation accuracy is = 93.75759313103185\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [06:44<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.7265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [06:43<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Learning Rate = 0.0003 , Batch Size = 64 and the Dropout Probability = 0.4 the validation accuracy is = 95.01754098584435\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 573/573 [06:35<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.8150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 573/573 [06:31<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Learning Rate = 0.0003 , Batch Size = 128 and the Dropout Probability = 0.4 the validation accuracy is = 94.2503788033908\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [06:44<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.9995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [06:43<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Learning Rate = 0.0001 , Batch Size = 64 and the Dropout Probability = 0.5 the validation accuracy is = 92.96995508961601\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 573/573 [06:35<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.1976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 573/573 [06:35<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Learning Rate = 0.0001 , Batch Size = 128 and the Dropout Probability = 0.5 the validation accuracy is = 92.13454004395484\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [06:43<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.9011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [06:43<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Learning Rate = 0.0002 , Batch Size = 64 and the Dropout Probability = 0.5 the validation accuracy is = 93.8476869104659\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 573/573 [06:35<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.9477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 573/573 [06:34<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Learning Rate = 0.0002 , Batch Size = 128 and the Dropout Probability = 0.5 the validation accuracy is = 93.68934026782424\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [06:43<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.8132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [06:43<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Learning Rate = 0.0003 , Batch Size = 64 and the Dropout Probability = 0.5 the validation accuracy is = 94.36367855631543\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 573/573 [06:34<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.8372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 573/573 [06:34<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For Learning Rate = 0.0003 , Batch Size = 128 and the Dropout Probability = 0.5 the validation accuracy is = 94.44421693490041\n",
      "\n",
      "Selected Learning Rate = 0.0003, Selected Batch Size = 64 and Selected Dropout Probability = 0.4 \n",
      "\n",
      " Let's print accuracy and the loss curve as a function of epoch:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [06:43<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.7506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [06:43<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 90.13137676705593\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 576x360 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train_model(training_set, batch_size=128, learning_rate=0.1, num_epochs=500, plot=False, aug=False, do_p=0.2):\n",
    "    # dataloaders - creating batches and shuffling the data\n",
    "    train_loader = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    device_1 = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # loss criterion for classification problem\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # build our model and send it to the device\n",
    "    model = SVHN(do_p).to(device_1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    total_loss = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for t, data in enumerate(tqdm(train_loader), 0):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device_1)\n",
    "            labels = labels.to(device_1)\n",
    "            if aug:\n",
    "                augmented_inputs = aug_list(inputs)  # Apply augmentations\n",
    "                inputs = torch.cat((inputs, augmented_inputs), dim=0)  # Concatenate inputs\n",
    "                labels = torch.cat((labels, labels), dim=0)  # Extend labels\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)  # forward pass\n",
    "            loss = criterion(outputs, labels)  # calculate the loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        epoch_loss /= len(train_loader)\n",
    "        if epoch % 2 == 0:\n",
    "            print(f'Epoch: {epoch}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "        total_loss.append(epoch_loss)\n",
    "\n",
    "    if plot:\n",
    "        plot_loss_function(total_loss)\n",
    "\n",
    "    return model\n",
    "\n",
    "# function to calculate accuracy of the model\n",
    "def calculate_accuracy(model, dataloader, device2):\n",
    "    model.eval() # put in evaluation mode,  turn of DropOut, BatchNorm uses learned statistics\n",
    "    total_correct = 0\n",
    "    total_images = 0\n",
    "    test_loader = torch.utils.data.DataLoader(dataloader, batch_size=10, shuffle=True)\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images = images.to(device2)\n",
    "            labels = labels.to(device2)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_images += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    model_accuracy = total_correct / total_images * 100\n",
    "    return model_accuracy\n",
    "\n",
    "def plot_loss_function(loss_list):\n",
    "    mean_loss = []\n",
    "    for loss in loss_list:\n",
    "        mean_loss.append(np.mean(loss))\n",
    "    mean_loss = np.array(mean_loss)\n",
    "    fig2 = plt.figure(figsize=(8, 5)) # create a figure, just like in matlab\n",
    "    ax = fig2.add_subplot(1, 1 ,1) # create a subplot of certain size\n",
    "    ax.plot(mean_loss, label=\"Loss Per Epoch\")\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel(\"Mean Epoch Loss\")\n",
    "    ax.set_title(\"Epoch\")\n",
    "    ax.grid()\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "#Now we will tune our model using a validation set.\n",
    "# hyperparameters\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "possible_learning_rates = [1e-4,2e-4,3e-4]\n",
    "possible_batch_sizes = [64,128]\n",
    "possible_do_p = [0.3,0.4,0.5]\n",
    "epochs=2\n",
    "results = []\n",
    "\n",
    "for p in possible_do_p:\n",
    "    for lr in possible_learning_rates:\n",
    "        for b_size in possible_batch_sizes:\n",
    "            my_model = train_model(train_set,\n",
    "                                   batch_size=b_size,\n",
    "                                    learning_rate=lr,\n",
    "                                    num_epochs=epochs,\n",
    "                                   do_p=p)\n",
    "            accuracy = calculate_accuracy(my_model, new_val_set,device)\n",
    "            results.append((lr, b_size, p, accuracy))\n",
    "            print(f'\\nFor Learning Rate = {lr} , Batch Size = {b_size} and the Dropout Probability = {p} the validation accuracy is = {accuracy}\\n')\n",
    "\n",
    "max_acc = 0\n",
    "max_acc_idx = 0\n",
    "idx = 0\n",
    "for result in results:\n",
    "    if result[3] > max_acc:\n",
    "        max_acc = result[3]\n",
    "        max_acc_idx = idx\n",
    "    idx += 1\n",
    "print(f'Selected Learning Rate = {results[max_acc_idx][0]}, Selected Batch Size = {results[max_acc_idx][1]} and Selected Dropout Probability = {results[max_acc_idx][2]} ')\n",
    "print(f'\\n Let\\'s print accuracy and the loss curve as a function of epoch:')\n",
    "my_model = train_model(train_set,\n",
    "                       batch_size=results[max_acc_idx][1],\n",
    "                       learning_rate=results[max_acc_idx][0],\n",
    "                       num_epochs=epochs,\n",
    "                       plot=True,\n",
    "                       do_p=results[max_acc_idx][2])\n",
    "accuracy = calculate_accuracy(my_model, test_set,device)\n",
    "print(f'Test Accuracy: {accuracy}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T18:14:36.378160Z",
     "start_time": "2023-05-24T13:23:32.967412Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Section 4 Answer**\n",
    "--\n",
    "Let's add noise to see how robust our model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with noise level 0.05: 89.98156115550093\n",
      "Accuracy with noise level 0.01: 90.14674247080517\n",
      "Accuracy with noise level 0.005: 90.13521819299324\n"
     ]
    }
   ],
   "source": [
    "a = [0.05, 0.01, 0.005]\n",
    "\n",
    "noisy_test_set = []\n",
    "\n",
    "for noise_level in a:\n",
    "    noisy_test_set = []\n",
    "    for i in range(len(test_set)):\n",
    "        image = test_set[i][0]\n",
    "        label = test_set[i][1]\n",
    "        noisy_image = image + noise_level * torch.randn_like(image)\n",
    "        noisy_test_set.append((noisy_image, label))\n",
    "\n",
    "    accuracy = calculate_accuracy(my_model, noisy_test_set, device)\n",
    "    print(f\"Accuracy with noise level {noise_level}: {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T18:36:12.008056Z",
     "start_time": "2023-05-24T18:33:43.759007Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Section 5 Answer**\n",
    "--\n",
    "\n",
    "**The augmentation we've chosen is:**\n",
    "\n",
    "RandomBoxBlur - The function returns the batch of images with box blurring applied, resulting in blurred versions of the original images.\n",
    "\n",
    "RandomPerspective - The function returns images that simulate different viewing angles or perspectives.\n",
    "\n",
    "the test accuracy improved a little."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [13:13<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.7696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1145/1145 [13:15<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy and statistics: 90.96880762138906\n",
      "Accuracy with noise level 0.05: 90.86124769514444\n",
      "Accuracy with noise level 0.01: 90.96112476951444\n",
      "Accuracy with noise level 0.005: 90.97264904732637\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 576x360 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfQAAAFJCAYAAABtgt8hAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA0f0lEQVR4nO3dd3yUZbr/8c+kEQiE3lsIgVtEOigISt11V9cVXcuiKGDH7lpA93dczzlbbIi9YMFe0D277q6uBRBEEKWIAsoFKUDoEAglgdT5/TGDGxHCQDL9+369eDkzzzwzV8bAd577uZ/78ni9XkRERCS6JYS7ABEREak5BbqIiEgMUKCLiIjEAAW6iIhIDFCgi4iIxAAFuoiISAxICncBIhJ6zjkvsAKoOGTTaDNbG4T3am5mO2rzdUXkxxToIvFruEJWJHYo0EXkR5xzw4D7gXXACcB+YLyZfe+cawg8CfQGvMC/gbvNrNw5dwrwGJAGlAK3m9ls/8v+t3NuINAUeNDMngzhjyQSF3QOXSR+feqcW1blz9+qbOsPPG5mPYHpwKv+xx8DCoAe/uf0Am53ziUDfwf+x8xOAq4CHnXOHfw3JtfM+gHnAlP8zxeRWqQjdJH4Vd2Q+zdmNs9/+0XgSedcU+CXwGAz8wIlzrlngFuAj4EKM3sfwMyW4At9nHMAb/hfaxlQB0jH98VARGqJjtBF5HDKq9z2+P9bge/fjKoNIBKAZP/zf9QYwjl3knPu4EFDGYD/i0DV1xSRWqJAF5HD6e2c6+m/fTWwwMwKgY+AG5xzHudcHf+2TwADvM65nwE45/oCs9G/MSIhoyF3kfj1qXPu0MvW7gaKgS3An5xzGcA24FL/9puAx4HlQArwIfAnMyt1zp0HPOKcexDfpLjz/I8H/ycRETxqnyoiVflnuT/hn9wmIlFCw2EiIiIxQEfoIiIiMUBH6CIiIjFAgS4iIhIDFOgiIiIxIKovW6usrPRWVGgOgIiIxIfk5MQdQPPDbYvqQK+o8FJYWBzuMkREREKiefMG6460TUPuIiIiMUCBLiIiEgMU6CIiIjEgqs+hi4jIsauoKGfXru2Ul5eGuxQ5gqSkFBo3bk5iYuAxrUAXEYkzu3ZtJzW1HmlprfB41Mk20ni9XoqK9rBr13aaNWsd8H4achcRiTPl5aWkpaUrzCOUx+MhLS39mEdQFOgiInFIYR7Zjuf/j4bcRUQkpJYuXcx77/2V//7vvwTl9Tdv3sS4cWPo2tXh8XgoLS2lb9/+XHPN9cf8WueffzYtW/741MQNN9zKCSd0q1GNwfgMFOgiIhJzMjI68cQT0wCorKxk4sQryM5eQ1ZWl2N+rYcffoI6derUdom1ToHut21vCVv3ltCjTXq4SxERiUuLFi1k2rSnqVOnDunpDbnrrnsoLy/nD3+4i8rKSioqyrn99rtp164999wzmaKiIkpKDjBx4k307dv/iK9bUlJCWVkpqampbN26hQce+DOlpSWkpNThzjvvprKykkmTbiU9vSGDBg3mkkvGHbXWDz74J/PmzaW4uIjCwkImTLiSYcNGHvZnSEtL45FHHuT771dSVlbOFVdcTVpaffLz87nttpvYtWsngwefxhVXXFOjz0+B7vevlVt5ev5ahmU15bbhnWmVnhrukkREgu79lVv5x4ottfqavz6pFWd1b3lM+3i9Xh544M889dTzNG/eghkz3uTll1+gb9/+pKXV5957/0heXh5FRfvYuHEDO3cW8MgjT7Fr1y7y83+6GuratXnccMPVeDweEhISueCCMf4vAndx/vkXMWjQYBYv/opnnnmCq6++jp07C3jhhddITk7+yWv97nc3/DDknpiYyKOPPg3A/v3FTJ36JIWFu7jqqnEMGTL0sD9Djx692L27kOeee4WCgh389a8z6N//ZEpLS/nLXx6isrKS3/zmLAV6bbl0QDsSEzw8/8U6Lpi+mCsHdeTifm1JTtS8QRGRYCssLKRevTSaN28BQO/efXj22ae47rqb2LBhPZMn30ZSUhLjxl1BZmZnzjvvQu699/eUl5dz/vm//cnrVR1yryo3N5tXX53O66+/DEBSki8GW7duc9gwhyMPuffu3ZeEhASaNGlKgwbpFBTsOOzP0LBhI7p37wlA06bNuPrq61i6dDGZmZ1JSUkBOKbrzY9Ege6XnJjAuJPb8/MTmvPwpzk8MS+P91duZdKoLPq1bxTu8kREguKs7i2P+Wg6GBo1akRxcRE7duygWbNmLFu2lPbtO/D110to2rQZU6c+yYoV3/Lss09yyy13UFxcxIMPPsqOHTuYOPFyBg8+LaD36dAhgzFjxtKjRy/WrVvL118vAcDjOfaDN7NVAOzcWUBRURHNmjU/7M+QkZHBp5/OAmDfvn3cc89kxo4dT21faKBAP0Tr9FQePKc783IKeGh2NtfO+JZfdmvBTUMzaZaWEu7yRERiwldffckVV1z6w/0//OGP3Hnn7/n97+8gIcFDgwbp3H33vXg8cM89dzNjxpskJCQwYcJVtGvXnunTp/Hhh++TlJR8TEPV119/M1Om3EdpaSklJQe4+ebbj7pP1SF3gAsuGAP4gvzmmyeyb98+brttEomJiYf9GRo2bMjixV8xceIVVFRUMGHCVcfwSQXO4/VGbz/xsrIKbzDbpx4oq2D6V/m8uiifOkkJTBycwW96tSExQddvikj02rJlHa1adQx3GVHtgw/+ybp1a5k48cagvcfh/j81b95gCXDYGYA6QVyN1OREJg7O4I3L+nFiywY8ODuH8a9/zcrNe8JdmoiIyI/oCD1AXq+XT2w7U+fkUlBUyrk9W3PdkAwa1j38JAoRkUilI/TooCP0IPF4PPz8hBa8M6E/Y/q15b3lmzl/+mL+uWILlVH8pUhERGKDAv0Y1a+TxK3DOvPK2L50aFyX//loNde8/Q3Z24vCXZqISMCieXQ2HhzP/x8F+nHq2qI+z/22F//1867kFRQz9tUlTJ2TQ1FpebhLExGpVlJSCkVFexTqEepg+9SkpGO7skrn0GtB4f4ynpyXx9+Xb6F5/RR+N6wzI7s2UzcjEYlIFRXl7Nq1/Zjbc0roJCWl0Lhx858sOFPdOfSgBLpzLgF4CugFlABXmlm2f1sr4K0qT+8NTDazZ5xzXwO7/Y/nmdmE6t4nUgL9oOWb9nDfzDWs3l7EwI6NuWNkFh0a1w13WSIiEiPCEejnAb82s/HOuYHAXWZ2zmGeNwj4E/AzIBn4wsz6BPo+kRboAOWVXv66bBNPz19LaUUl4wa0Z9zJ7UlNTgx3aSIiEuXCMct9CPAhgJktPNybO+c8wOPARDOrwHc0X88597Fzbrb/i0DUSUrwcFHftrw7oT8jujTj+YXr+e3LS5ifuzPcpYmISAwLVqCn85+hc4AK59yhy8yeDaw0M/PfLwYeAs4ArgVeP8w+UaNZ/Tr88axuPHVBD5ISPNzytxXc8d5Ktuw5EO7SREQkBgUr0PcADaq+j5kdOv17LFC1Fc5q4DUz85rZaqAAaB2k+kJmQIfGvDmuH9cNyeCLtbu4YPpiXvkqn7KKynCXJiIiMSRYgT4fOBPAP3S+/DDP6QcsqHL/cmCKf582+I7yNwepvpBKTkxgwikdmDG+P6d0bMzj8/K45NWlLMkvDHdpIiISI4I9y70n4AEmAH2B+mY2zTnXHPjEzHpX2ScFeAnoAHiBSWa2gGpE4qS4QHyWU8CU2dls2lPCmSe24KbTM2mqTm4iInIUIZ/lHirRGujg7+T25XpeWbSB1OQErhvSifN6tlYnNxEROSIFegRbu7OYB2Zls2h9Id1a1mfSqC50b9Xg6DuKiEjcUaBHuEM7uZ3Xy9fJLT1VndxEROQ/FOhRYl9JOdMWrOPtrzfSMDWZm4Z24qwTW2oJWRERARToUce27eP+mdks37yHPm3TuXNUF7KapYW7LBERCTMFehSq9Hr554otPP5ZHvtKyhnTrx1XDepIvRQtISsiEq8U6FGssLiMJz7P473lW2hRP4XfDe/MiC7q5CYiEo8U6DHgW38ntzXbixiY0Zg7R2TRXp3cRETiigI9RpRXenl32Saemb+WsopKxp3cnssGqJObiEi8UKDHmB37Snhkbi4frdpOu0ap3D4ii8GdmoS7LBERCTIFeoz6at0uHpiVzbpd+xnRpRm3DsukVXpquMsSEZEgUaDHsNLySl5fsoEXFq4nwQNXDerImL5tSUoMVt8dEREJFwV6HNi0+wBTPs3hs5wCMpvWY9KoLPq2axTuskREpBYp0OPI3OwCpnyazeY9JZx1YgtuVCc3EZGYoUCPMwfKKnjxy/W8umgDdZMTuW5IBueqk5uISNRToMeptQXF3D87m8Xq5CYiEhMU6HHM6/Xy8artTJ2by051chMRiWoKdGFfSTnPLljHDH8nt5uHZnLmiS20hKyISBRRoMsPfJ3c1rB88151chMRiTIKdPmRSq+XfyzfwhPz8thXWsGYvm3VyU1EJAoo0OWwCovLeGJeHu+t8HVyu214Z4ark5uISMRSoEu1vtm4m/tnZbNmexGDMhpzhzq5iYhEJAW6HFV5pZd3lm3iWX8nt/End+Cyk9tTJ0lLyIqIRAoFugRs+74SHpmTy8fm6+R2x4gsTlUnNxGRiKBAl2P2pb+T23p1chMRiRgKdDku6uQmIhJZFOhSIxt37+eh2Tl8nruTzKb1mDyqC33aNQx3WSIicUeBLrVibnYBD83OZsveEs7q3pKbTu9Ek3rq5CYiEioKdKk1+8sqeHHhel5brE5uIiKhFvJAd84lAE8BvYAS4Eozy/ZvawW8VeXpvYHJwLQj7XMkCvTwySso5oFZa1icv5tuLeszeVQXTlQnNxGRoKou0IM1u2k0kGpmg/CF9ZSDG8xsi5kNM7NhwF3AUuC56vaRyNOpaT2euqAn/3vmCWzbV8r417/m/plr2HugPNyliYjEpWAF+hDgQwAzW8hhvk045zzA48BEM6sIZB+JLB6Ph190a8G7E/pzYZ82/N+3mzl/+iI++G4r0XwqR0QkGgUr0NOB3VXuVzjnkg55ztnASjOzY9hHIlD9OkncPiKLVy7pS5uGqfzh38Y1M74lZ0dRuEsTEYkbwQr0PUDVE6oJZnboWOxYfOfNj2UfiWCuZX1eGNObu3/WhZwdRVzy6lIe/yyX4tKKcJcmIhLzghXo84EzAZxzA4Hlh3lOP2DBMe4jES7B4+Hcnq15d0J/zjqxBa8s2sAF0xcxe80ODcOLiARRsGe59wQ8wASgL1DfzKY555oDn5hZ7+r2MbNV1b2PZrlHvqqd3E7t5Ovk1q6ROrmJiBwPXYcuYVVe6WXG1xt5dv46yisrGX9KBy4boE5uIiLHSoEuEWHb3hIemZvLJ7ad9o1SuWNkFoMy1MlNRCRQCnSJKF+u3cUDs32d3EZ2bcatwzrTskGdcJclIhLxFOgScUrLK3l1cT7Tv8wnwQNXn5rBb/u0USc3EZFqKNAlYlXt5Na5WT0mj+xCb3VyExE5LAW6RDSv18tnOQU8NDuHLXtL+JW/k1tjdXITEfkRBbpEhf1lFbzg7+SWlpLI9UMyOKeHOrmJiBykQJeokltQxAOzslmSv5vurRowaVQW3Vqqk5uIiAJdoo7X6+WjVduZOieHwv1l/KZXGyYOzqBBqpb3F5H4pUCXqLX3QDnPLljLO8s20ahuMjcPzeSX3Vrg8WgYXkTijwJdot6qrXu5b2Y2K7fspW+7hkwalUVm07RwlyUiElIKdIkJlV4vf1++hSfn5VFUWsEl/dpx5aAO1E1ODHdpIiIhoUCXmLKruJTHP8vjnyu30rJBHW4b3plhWU01DC8iMU+BLjHpm427uW9mNtk7ihjcqQm3j+isTm4iEtMU6BKzyisqmbFsE8/OX0eF18v4k9tz2YD2pKiTm4jEIAW6xLxte0uYOieXmau306FxXe4Y0ZmB6uQmIjFGgS5xY+HanTw4O4f1u/Yzqmtzbh2WSQt1chORGKFAl7hStZNbosfD1ad25CJ1chORGKBAl7i0odDXyW1+3k6ymqUxeVQWvdqqk5uIRC8FusQtr9fL3OwCHvo0h617Szi7e0tuVCc3EYlSCnSJe/vLKnj+i/W8vsTfye20Tozu0YoEXbsuIlFEgS7il1tQxP0zs1m6wdfJbfKoLE5QJzcRiRIKdJEqvF4v//5+G4/OzaVwfxnn92rDterkJiJRQIEuchh7D5TzzPy1vPuNr5PbLcMy+cUJ6uQmIpFLgS5Sje/9ndy+27KXfu0bMmlkFzo1rRfuskREfkKBLnIUFZVe3lu+mSc/X0tRaQVj+7fjioHq5CYikUWBLhKgnf5Obv9auZVW/k5uQ9XJTUQihAJd5Bgt27Cb+2atIWdHMUMym3DbcHVyE5HwU6CLHIfyikre/noT0xb4OrlNOKU9l/ZXJzcRCR8FukgNbN1bwiNzcpi5egcdGtflzhFZnJLRONxliUgcCnmgO+cSgKeAXkAJcKWZZVfZPgB4GPAAW4CxZnbAOfc1sNv/tDwzm1Dd+yjQJZS+WLuTB2dlk194gJ+55twyVJ3cRCS0qgv0o66k4ZzrDqQDlcCfgT+b2ayj7DYaSDWzQc65gcAU4Bz/63mA54DzzSzbOXcl0NE5tw7AzIYF8kOJhNqgjCa8Oa4/ryzK56Uv1zM/dyfXDO7IhX3akpSgSXMiEl6BnAx8Bt9R9v8Dfg/8IYB9hgAfApjZQn78baIrUADc4pybCzQxM8N3NF/POfexc262/4uASESpk5TAVYM68vb4/vRul87UOblc9tpSvtm4++g7i4gEUSCBXgasBFL84RzI+pjp/GfoHKDCOXdwv2bAqfiG5EcBI51zI4Fi4CHgDOBa4PUq+4hElHaN6vLIuSfxwK9PZPf+Mq586xv+9yOjsLgs3KWJSJwKJNC9wBvAB865C4GiAPbZA1TteJFgZuX+2wVAtpl9Z2Zl+I7k+wGrgdfMzGtmq/3Pax3gzyESch6Ph+FdmvHOhAFcNqAd73+3jfOnL+Jv326mMoonm4pIdAok0C8CXgAeA7b57x/NfOBMAP/Q+fIq23KB+s65LP/90/CNAFyO71w7zrk2+I7yNwfwXiJhVS8lkRtPz+T1S/uS2SyNP3+yhiveXIZt3Rfu0kQkjhx1lrtzrjXQGCgHJgGPm9myo+xzcJZ7T3wz2ScAfYH6ZjbNOTcCuM+/bYGZ3eycSwFeAjrgGxWYZGYLqnsfzXKXSHNoJ7cLevs6udWvo7NHIlJzNbpszTn3Cb7Z7dcD7wLXmNnw2i7yeCjQJVLtPVDO0/PX8u6yTTRJS+GWoZmccUJzLSErIjVSXaAHMuSeBHwGNDKztwB1qxA5igapSdw5MouXLulDywZ1+K8PVnHdO9+SV6AvoCISHIEEegq+RWA+c84NJ7BZ7iICnNiqAS+O6c3kUVnYtiIufmUJT87LY39ZRbhLE5EYE0igjwcM3znv5sDYYBYkEmsSEzz8plcb3r28P2d0a8FLX+Vz4fTFzM3eEe7SRCSGBBLoufgmr03FdxnZhqBWJBKjmtRL4d5fOJ69qCf1UhK5/b3vuPVvK9i4e3+4SxORGBBIoE8DMoFPgAzg+WAWJBLr+rZrxOuX9uXmoZksyS/kopeW8OLC9ZSWV4a7NBGJYoGcD+9iZqf7b//dOVftpWQicnRJiQmM7d+On7nmTJ2Tw9Pz1/L+d1u5c2QWp3RUJzcROXaBHKGnOufqATjn6qJZ7iK1pmWDOtx39ok8et5JVHq93PDucu7+1/ds31cS7tJEJMoEEuiPAt845/4GLAMeCWZBIvHo1E5NeGtcf64e1JG52Tu4YPpi3ly6kfJKLSErIoEJqB+6c64JvvPoecB+M4uIi2m1sIzEog2F+3lgVjZfrN1Fl+ZpTBqZRa+2DcNdlohEgBqtFHco59xXZnZybRRWUwp0iVVer5dPswuYMjubbftKOeekVtxwWica1UsOd2kiEkbVBfrxLBKjtStFgszj8TCiSzMGdmzM81+s442lG5mTvYMbTuvEr3u0IkFLyIrIIQI5h34ondQTCZF6KYncNDST1y7tS2bTevzpkzVc+eYybJs6uYnIjx1xyN059xd+Gt4e4DIzaxvswgKhIXeJJwc7uT0yJ5fdB9TJTSQeHe+Q+6ojPH53jSsSkWPm8Xg488SWDMlswtOfr2XG15uYuXoHtw7N5Ofq5CYS9455Ulwk0RG6xLOVW/Zy/8w1fL91H/07NGLSiCwymtYLd1kiEkS1Oss9kijQJd5VVHr527ebefLzPA6UVXLpgHZcfkoHUpO1/pNILFKgi8S4gqJSHv8sl/e/20br9DrcPiKL0zs3DXdZIlLLahTozrkM4Hzgh7E8M/ufWqzvuCnQRX5s6YZC7p+ZTW5BMadlNuH2EVm0aZga7rJEpJZUF+iBXLb2JpAGbK3yR0Qi0MFObjed3onF+YVc+NJipn+pTm4i8SCQI/RZZjYyRPUcEx2hixzZlj0HmDonl9lrdtCxcV3uHJnFyerkJhLVjmvI3TnX1X/zXuBfwBL816Wb2epar/I4KNBFjm5+3k4emp3NhsIDnHFCc24Zmkmz+nXCXZaIHIfjDfRPj/B6XjMbUUu11YgCXSQwJeWVvPzVel7+Kp/kxASuGZzBBb3bkJSga9dFoklNJ8WlAt3M7Gvn3GjgfTMrq/Uqj4MCXeTYrN+1nwdnZ7PQ38lt8qgu9GyTHu6yRCRANZ0U9xpwiv92V+DlWqpLREKsQ+O6PHbeSdx3djd27y/jijeX8cePV1O4PyK+o4tIDQQS6G3N7BkAM3sAaB3ckkQkmDweDyO7NmfGhP6M7d+Of63YwvkvLuK95ZupjOJ1KUTiXUDd1g5OkHPOdQa0BJVIDEhLSeLmoZm8dmk/OjWtxx8/XsOVb36jTm4iUSqQc+inAM8CLYBNwLVmtjgEtR2VzqGL1A6v18v7323lsbl57D5QxoV92nLNqR3VyU0kwtR46VfnXFOgM5BrZjtqt7zjp0AXqV17DpTx1Odr+b9vNtM0LYVbh2XyM6dObiKRoqaz3C8E/hf4DugB3Gtmrx1lnwTgKaAXUAJcaWbZVbYPAB7G1199CzAWKK1un8NRoIsER9VObgM6NOLOkVlkNFEnN5Fwq+ks91uBfmZ2LtAHuDmAfUYDqWY2CJgMTDm4wTnnAZ4DJpjZEOBDoGN1+4hIaHVv1YDpF/fhzpFZfL91L2NeXsLTn+dxoKwi3KWJyBEEEuiVZrYPwMz2AgcC2OdgUGNmC/nxt4muQAFwi3NuLtDEzOwo+4hIiCUmeLigdxvenTCAn5/QnBe/zOeilxbzWU5BuEsTkcMIJNBznHNTnHPnOOemADkB7JMO7K5yv8I5d3B2TTPgVHzD66OAkc65kUfZR0TCpGlaCv/9yxN45sKe1ElO5La/r+S2v69k0+5AvtuLSKgEEuiXA7n4wjcHuCqAffYADaq+j5mV+28XANlm9p1/xbkPgX5H2UdEwqxf+0a84e/k9tW6XT90ciurUCc3kUgQSKAnAyn+/wZ03TowHzgTwDk3EFheZVsuUN85l+W/fxqw8ij7iEgESEpM4NIB7XlnQn9O7dSEpz5fy8WvLGHR+l3hLk0k7gUyy/3vwCrgC2Aw0MbMxh5ln4Oz3Hvim8k+AegL1Dezac65EcB9/m0LzOzmw+1jZquqex/NchcJr/m5O3lwdjYbd6uTm0go1PSytXlmdtqR7oeTAl0k/A6UVfDyV/m8vCiflMQErh2cwfnq5CYSFDW9bG2lc24wgHOuB7DOOZfsnEupxRpFJEqlJidyzeAM3hrXnx6t05nyaQ7jXlvK8k17wl2aSFwJ5Ah9JVAP38IvVUPca2aZQaztqHSELhJZvF4vs9fs4OFPc9i2r5TRPVpx/WmdaFQ3OdylicSEGi/9GqkU6CKRqai0nOcWrOetpRuoXyeJm07P5FcntSRBS8iK1MhxDbk75x6tcvuSKrf/VqvViUjMSUtJ4pZhvk5uGU3q8b8fr+aqt75htTq5iQRNdefQe1S5fUWV242CU4qIxJqs5mlM+20v7jmjK+t37eey15by8Kc57CvREhMita26QPcc4Xb0jtGLSMgleDycfVIr3p3Qn9E9W/PW0o1cMH0xH6/aRjSf8hOJNNUFuvcIt0VEjlnDuslMHtWF6Rf3pllaCr9/fxU3vLucdTs1D0akNhxxUpxzbh3wOr6j84ur3B5jZhmhKrA6mhQnEp0qKr389ZvNPD0/j5LySi4d0J4JJ7cnNTkx3KWJRLTjmuXunBt3pBc0s5drp7SaUaCLRLcdRaU8NjeXf3+/jTbpdbh9RBandW4a7rJEIpYuWxORiLYkv5D7Z2aTt7OYoZ2bctuIzrROTw13WSIRR4EuIhGvrKKSN5ds5Lkv1uEFrhzYgUv6tyM5MdCeUCKxT4EuIlFjy54DTPk0hznZBWQ0qcukkV3o36FRuMsSiQg1bc7SAPgl8MP4l5m9UpsFHi8Fukjs+jy3gAdn57Bp9wF+0a0FNw/NpFmaWkhIfKsu0JMC2P89YBOQ778fvYf0IhI1hmQ2pX/7Rrz0VT6vLMpnXk4B1w3J4De92pCoTm4iPxHIEfocMxsWmnKOjY7QReLD+l37eWDWGr5cV4hrUZ/Jo7I4qXV6uMsSCbmaDrk/hu8a9GX4j87NrLR2Szw+CnSR+OH1epm1egcPz8lhx75SRvdsxfVDOtFQndwkjtR0yH0ocHaV+14grG1TRST+eDweRrnmDOrUmGkL1vH20o18uqaAG0/vxK+6q5ObiGa5i0hUWrN9H/fNzObbTXvo1SadSaOy6NK8frjLEgmqmg65/xq4HkjGt/RrUzPrWdtFHg8Fukh8q/R6+dfKrTz+WR57D5RxUd+2XH1qR9JSAhl8FIk+x9UPvYp7gHvxzXJ/GVhea5WJiNRAgsfDr/2d3H7doxVvLvF1cvvEtquTm8SdQAK9wMy+ADCzl4B2Qa1IROQYNaybzN0/68qLF/emSb0U7v7X99z4V3Vyk/gSSKCXOOdOB5Kdc2cArYNck4jIcTmpdTovX9KHO0Z0ZsXmvYx5ZQnPzF/LgbKKcJcmEnSBBPpEfOfP/whcjW8IXkQkIiUmeLiwT1vevXwAI7s254WF67no5SXMz90Z7tJEgiqgWe7OuZH4LlX7ElhtZgeCXVggNClORI5m8fpC7p+1hrU79zMsqym3De9MK3VykyhV01nuf8Z33rwb8ATwCzMbU9tFHg8FuogEoqyikjeWbOT5L9YBcOWgjlzcr606uUnUqeks9yFmdhmwz8xeBjrVZnEiIsGWnJjAuJPbM2NCfwZmNOaJeXlc8spSluQXhrs0kVoTSKAnOedSAa9zLhHQ7BIRiUqt01N58JzuPDy6OyXlFVw741vu+WAVO4oiYjVrkRoJZPWFqcASoDm+c+hTg1qRiEiQnda5KQM6NGL6V/m8uiifebkFTBysTm4S3QKdFNcYyALyzGxHAM9PAJ4CegElwJVmll1l+++AK4Dt/oeuMTNzzn0N7PY/lmdmE6p7H51DF5GaWruzmAdnZfPV+kJO8Hdy665ObhKhjqs5i3PuxSM8jpldfpT3HA2kmtkg59xAYApwTpXtfYHLzGxJlddNBYjUVq0iEpsymtTjifN78IltZ+qcXCa8sYxze7bmuiEZ6uQmUaW6Iff+QD3gNWABvnXcAzUE+BDAzBY65w79NtEPuMs51wp438z+gu9ovp5z7mN/XXeb2cJjeE8RkePi8Xj4+QktOLVTE577wtfJbfaaHdx0eifOUic3iRJHnBTnb8AyGkgFJgODgBwz+yiA103nP0PnABXOuapfHt4CrgVGAEOcc78CioGHgDP8214/ZB8RkaCqXyeJW4d15pWxfenQuC7/89Fqrnn7G7K3F4W7NJGjqnaWu5mtMLPJZjYCmA38xTkXyFHzHqBB1fcxs3IA55wHeMTMdphZKfA+0AdYDbxmZl4zWw0UoGVmRSQMuraoz3O/7cV//bwreQXFjH11CVPn5FBUWh7u0kSO6KhHwM65dOBcYAyQhm8I/mjmA2cDM/zn0Kt2aEsHVjjnugFF+I7SXwQuB3oA1znn2viftznwH0VEpPYkeDz8ukcrTs9qypPz8nhjyUY+se38blhnRnZthkfD8BJhjjjL3Tl3Ab4Q7wD8H/CGma0N5EWrzHLvie/c+wR8E+Hqm9k059ylwE34ZsDPMrM/OOdSgJf87+cFJpnZgureR7PcRSRUlm/aw30z17B6exEDOzbmjpFZdGhcN9xlSZw5rqVfnXOVwCrgG/9DPzzRzC6u5RqPiwJdREKpvNLLX5dt4un5aymtqGTcgPaMO7k9qcmJ4S5N4sRxXbYGDA9OOSIi0SkpwcNFfdsysmszHpmby/ML1/Pv77dxx4gsBmc2CXd5EucCWlgmUukIXUTCadH6Xdw/M5t1u9TJTUKjRt3WIpkCXUTCrayiktcWb+CFhevxAFcN6sgYdXKTIFGgi4gE2abdB3j40xzm5hTQqWk9Jo3Mol/7RuEuS2KMAl1EJEQ+yylgyuxsNu0p4cwTW3DT6Zk0TUsJd1kSIxToIiIhdKCsgulfrueVRRtITU7guiGdOK9na3VykxpToIuIhMHancU8MCubResL6dayPpNGdaF7qwZH31HkCBToIiJh4vV6f+jkVlBUynm9fJ3c0lPVyU2OnQJdRCTM9pWUM23BOt7+eiMNU5O5aWgnzjqxpZaQlWOiQBcRiRC2bR/3z8xm+eY99Gmbzp2jupDVLC3cZUmUUKCLiESQSq+Xf67YwuOf5bGvpJwx/dpx1aCO1EvRErJSPQW6iEgEKiwu44nP83hv+RZa1E/hd8M7M6KLOrnJkSnQRUQi2Lf+Tm5rthcxMKMxd47Ior06uclhKNBFRCJceaWXd5dt4pn5aymrqGTcye25bIA6ucmPKdBFRKLEjn0lPDI3l49Wbaddo1RuH5HF4E7q5CY+CnQRkSjz1bpdPDDL18ltRJdm3DosU53cRIEuIhKNSssreX2Jr5Nbgsffya1vW5LUyS1uKdBFRKLYpt0HmPJpDp/lFJDZtB6TRmXRt12jcJclYaBAFxGJAXOzC5jyaTab95Rw1oktuFGd3OKOAl1EJEYcKKvgxS/X8+qiDdRNTuS6IRmcq05ucUOBLiISY9YWFHP/7GwWq5NbXFGgi4jEIK/Xy8ertjN1bi471cktLijQRURi2L6Scp5dsI4Z/k5uNw/N5MwTW2gJ2RikQBcRiQO+Tm5rWL55rzq5xSgFuohInKj0evnH8i08MS+PfaUVjOnbVp3cYogCXUQkzhQWl/HEvDzeW+Hr5Hbb8M4MVye3qKdAFxGJU99s3M39s7JZs72IQRmNuUOd3KKaAl1EJI6VV3p5Z9kmnvV3cht/cgcuO7k9dZK0hGy0CXmgO+cSgKeAXkAJcKWZZVfZ/jvgCmC7/6FrgDXV7XM4CnQRkcBt31fCI3Ny+dh8ndzuGJHFqerkFlWqC/RgfT0bDaSa2SBgMjDlkO19gcvMbJj/jwWwj4iI1EDz+nX406+68cT5PUjweLj5/1Yw6R/fsWXPgXCXJrUgWIE+BPgQwMwW8tNvE/2Au5xznzvn7gpwHxERqQWndGzMm5f147ohGczP28mFLy3m1UX5lFdUhrs0qYFgBXo6sLvK/QrnXFKV+28B1wIjgCHOuV8FsI+IiNSSlKQEJpzSgbfH96Nf+0Y89lkel7y6lK837D76zhKRghXoe4CqiwonmFk5gHPOAzxiZjvMrBR4H+hT3T4iIhIcbRvWZeq5J/HQOd0pLq3g6re/4d4PjZ3FpeEuTY5RsAJ9PnAmgHNuILC8yrZ0YIVzrr4/3EcAS46yj4iIBNHQrKbMmNCf8Se356Pvt3H+i4t5d9kmKiqj90qoeBPsWe49AQ8wAd9EuPpmNs05dylwE77Z7LPM7A+H28fMVlX3PprlLiJS+/IKinlg1hoW5++mW8v6TB7VhRPVyS0i6Dp0ERE5Jl6vl49WbecRfye33/RqzXVDOtEgVVObwkmBLiIix2VfSTnPzF/LO8s20aiur5PbL7upk1u4KNBFRKRGbOs+7pu1hhWb99KnXUMmjcyiszq5hZwCXUREaqzS6+U9fye3otIKLunXlisGqpNbKCnQRUSk1uwqLuWJeXn8Y8VWXye3EVkMz2qqYfgQUKCLiEitq9rJ7dROvk5u7Rqpk1swKdBFRCQoyiu9zPh6I8/OX0d5ZSXjT+nAZQPUyS1YFOgiIhJU2/aW8MjcXD6x7bRvlModI7MYlKFObrVNgS4iIiHx5dpdPDA7m/W79jOyazNuHdaZlg3qhLusmKFAFxGRkCktr+TVxflM/zKfBA9cfWoGv+3ThqREDcPXlAJdRERCbuPu/Tw0O4fPc3fSuVk9Jo/sQu92DcNdVlRToIuISFh4vV4+yyngodk5bNlbwq+6t+Sm0zvRuF5KuEuLSgp0EREJq/1lFbywcD2vLd5AWkoi1w/J4JwerUlM0LXrx0KBLiIiESG3oIgHZmWzJH833Vs1YNKoLLq1VCe3QCnQRUQkYhzs5DZ1Tg6F+8v4Ta82TBycoU5uAVCgi4hIxNl7oJxnF6iT27FQoIuISMRatXUv983MZuWWvfRt15BJo7LIbKpOboejQBcRkYhW6fXy9+VbePKHTm7tuHJQB+omq5NbVQp0ERGJCruKS3n8szz+uXIrLRvU4bbhnRmmTm4/UKCLiEhU+Wbjbu6bmU32jiIGd2rC7SM6q5MbCnQREYlC5RWVzFi2iWfnr6PC62X8ye25bEB7UuK4k5sCXUREota2vSVMnZPLzNXb6dC4LneM6MzAOO3kpkAXEZGot3DtTh6cncP6XfsZ1bU5tw7LpEWcdXJToIuISEyo2skt0ePh6lM7clEcdXJToIuISEzZUOjr5DY/bydZzdKYPCqLXm1jv5ObAl1ERGKO1+tlbnYBD32aw9a9JZzdvSU3xngnNwW6iIjErP1lFTz/xXpeX+Lv5HZaJ0b3aEVCDF67rkAXEZGYl1tQxP0zs1m6wdfJbfKoLE6IsU5uCnQREYkLXq+Xf3+/jUfn5lK4v4zze7Xh2hjq5BbyQHfOJQBPAb2AEuBKM8s+zPOmATvNbLL//tfAbv/mPDObUN37KNBFRORw9h4o55n5a3n3G18nt1uGZfKLE6K/k1t1gR6sef6jgVQzGwRMBqYc+gTn3DVAjyr3UwHMbJj/T7VhLiIiciQNUpO4Y2QWL13Sh1bpqdzzgTHxnW/JK4jdg8BgBfoQ4EMAM1vIId8mnHODgIHAs1Ue7gXUc8597Jyb7ZwbGKTaREQkTnRr2YAXx/TmrlFZrNlexJhXlvDEvDz2l1WEu7RaF6xAT+c/Q+cAFc65JADnXGvgXuD6Q/YpBh4CzgCuBV4/uI+IiMjxSkzwcF6vNrwzoT+/7NaCl7/K58Lpi5mzZgfRPI/sUMEKzD1A1amFCWZW7r99AdAM+ABohe+ofBXwJpBtZl5gtXOuAGgN5AepRhERiSNN6qXwh184zjmpFffNWsMd//iOIZlNuG14bHRyC9YR+nzgTAD/0PnygxvM7DEz62dmw4D7gDfM7CXgcvzn2p1zbfAd5W8OUn0iIhKnerdryGtj+3LL0EyW5u/mty8v4YWF6ygtrwx3aTUSrED/G3DAObcAmArc6py72Dl3dTX7vAA0cs59DrwNXF7lqF5ERKTWJCUmcEn/dsyY0J/TMpvwzPx1jHllCV+u3RXu0o6brkMXEZG498XanTw4K5v8wgP8zDXnlqGR2clNC8uIiIgcRUl5Ja8syuelL9eTlJDANYM7cmGftiQlRM616wp0ERGRAG0o3M+Ds7NZkLeLLs3TmDQycjq5KdBFRESOgdfrZU52AQ/NzmbbvlJ+fVJLbjwtk0b1ksNalwJdRETkOBSXVvDCwnW8vmQj9f2d3M4JYyc3BbqIiEgN5Owo4v5Z2Xy9YTcntW7A5JFdcC3rh7wOBbqIiEgNHdrJ7YLevk5u9euEblFTBbqIiEgt2XugnKfnr+XdZZtokpbCLUMzOeOE5iHp5KZAFxERqWXfbdnL/bOy+W7LXvq3b8idI7vQqWm9oL6nAl1ERCQIKiq9/H35Zp6ct5b9ZRWM7d+Oywd2oG5yYlDeT4EuIiISRDuLS3nsszzeX7mVVg3qcPuIzgzNalbr76NAFxERCYGlGwq5f2Y2uQXFDMlswuRRXWhZi0vIVhfowWrOIiIiEnf6tmvE65f25eahmSzJL+TtpRtD9t46QhcREQmCotJyEj0eUmvxfHp1R+ihu3hOREQkjqSlhDZiNeQuIiISAxToIiIiMUCBLiIiEgMU6CIiIjFAgS4iIhIDFOgiIiIxQIEuIiISAxToIiIiMUCBLiIiEgMU6CIiIjEgqtdyB7YD68JdhIiISIh0BJofbkO0B7qIiIigIXcREZGYoEAXERGJAQp0ERGRGKBAFxERiQEKdBERkRiQFO4CQs05lwA8BfQCSoArzSy7yvazgXuAcuBFM3suLIVGuAA+xzHALUAF8C1wnZlVhqHUiHW0z7DK86YBO81scohLjHgB/B4OAB4GPMAWYKyZHQhHrZEqgM/wEuA2fH+XXzSzp8NSaBRwzp0C3G9mww55PCS5Eo9H6KOBVDMbBEwGphzc4JxLBqYCPweGAlc751qFo8goMJojf451gT8Cw83sVKAh8KtwFBnhRnOEz/Ag59w1QI8Q1xVNRnPk30MP8BwwwcyGAB/iu4ZXfmw01f8ePgSMAgYDtznnGoe2vOjgnLsTeB5IPeTxkOVKPAb6wb/YmNlCoH+Vbd2AbDPbZWalwOfAaaEvMSpU9zmWAKeaWbH/fhKgo6Kfqu4zxDk3CBgIPBv60qJGdZ9hV6AAuMU5NxdoYmYW+hIjXrW/h/hG2BriCyoPoMVLDi8HOO8wj4csV+Ix0NOB3VXuVzjnko6wbS++X2T5qSN+jmZWaWZbAZxzNwL1gU9CX2LEO+Jn6JxrDdwLXB+GuqJJdX+fmwGn4htOHgWMdM6NDHF90aC6zxBgBbAEWAn8y8wKQ1hb1DCzvwJlh9kUslyJx0DfAzSocj/BzMqPsK0BUBiiuqJNdZ8jzrkE59xDwM+A35iZvtX/VHWf4QX4AukDfMOgFzvnxoe2vKhQ3WdYgO/I6DszK8N3FNov1AVGgSN+hs65nsBZQCcgA2jhnLsg5BVGt5DlSjwG+nzgTADn3EBgeZVt3wNdnHNNnHMpwOnAF6EvMSpU9zmCb5g4FRhdZehdfuyIn6GZPWZm/fyTa+4D3jCzl8JRZISr7vcwF6jvnMvy3z8N31Gm/Fh1n+FuYD+w38wqgG2AzqEfm5DlStyt5V5lRmdPfOeDJgB9gfpmNq3KbMQEfLMRnwxbsRGsus8RWOz/M4//nG971Mz+FoZSI9bRfherPG88cIJmuf9UAH+fR+D7QuQBFpjZzWErNkIF8BleC1wOlOI7T3yV/1ywHMI5lwG8ZWYDnXMXE+JcibtAFxERiUXxOOQuIiIScxToIiIiMUCBLiIiEgMU6CIiIjFAgS4iIhID4q45i4j8mHNuGDAD+K7Kw9vNrEYLiDjnXsJ3Cc+HNXkdEQmMAl1EAGab2W/DXYSIHD8FuogclnNuDrAKOAHfgiMXmdkW59wUfA09wLeC3aPOuS74Ok2lAMXAwS8H1/i7UDUEJprZV6H8GUTiiQJdRABG+AP8oPf9/11gZtc6564D7nbOfYxvXe+B+P79+Nw5Nxtfu9y/mNmHzrkLgT7+/ZeY2R/9q92NBxToIkGiQBcROMyQu3PuLGC2/+4C4BwgH5jnb7ZT5pxbCJwIOPzrU5vZDP/+F+Pr0gWwBagX7B9CJJ5plruIVOdgd7LB+BqbfI9/uN05l4yvPeka/+MD/I9f4m+bC+qdLRIyOkIXEfjpkDtAXWC8c+53QBFwqZkVOOeGOee+wHe+fIaZLXXO3QE865z7f/jOoY9FrUpFQkrNWUTksPwBf62ZrQp3LSJydBpyFxERiQE6QhcREYkBOkIXERGJAQp0ERGRGKBAFxERiQEKdBERkRigQBcREYkBCnQREZEY8P8B8FQqi3U2aOgAAAAASUVORK5CYII=\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a sequence of augmentations\n",
    "aug_list = AugmentationSequential(\n",
    "    k.RandomBoxBlur((5, 5), p=1.),\n",
    "    k.RandomPerspective(0.1, p=0.25),\n",
    "    same_on_batch=False,\n",
    ")\n",
    "\n",
    "my_model = train_model(train_set,\n",
    "                       batch_size=results[max_acc_idx][1],\n",
    "                       learning_rate=results[max_acc_idx][0],\n",
    "                       num_epochs=epochs,\n",
    "                       plot=True,\n",
    "                       aug=True,\n",
    "                       do_p=results[max_acc_idx][2])\n",
    "accuracy = calculate_accuracy(my_model, test_set,device)\n",
    "print(f'Test Accuracy and statistics: {accuracy}')\n",
    "\n",
    "a = [0.05, 0.01, 0.005]\n",
    "\n",
    "noisy_test_set = []\n",
    "\n",
    "for noise_level in a:\n",
    "    noisy_test_set = []\n",
    "    for i in range(len(test_set)):\n",
    "        image = test_set[i][0]\n",
    "        label = test_set[i][1]\n",
    "        noisy_image = image + noise_level * torch.randn_like(image)\n",
    "        noisy_test_set.append((noisy_image, label))\n",
    "\n",
    "    accuracy = calculate_accuracy(my_model, noisy_test_set, device)\n",
    "    print(f\"Accuracy with noise level {noise_level}: {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-24T19:06:15.143341Z",
     "start_time": "2023-05-24T18:36:36.341008Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "---\n",
    "* Icons made by <a href=\"https://www.flaticon.com/authors/becris\" title=\"Becris\">Becris</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\">www.flaticon.com</a>\n",
    "* Icons from <a href=\"https://icons8.com/\">Icons8.com</a> - https://icons8.com\n",
    "* Datasets from <a href=\"https://www.kaggle.com/\">Kaggle</a> - https://www.kaggle.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
