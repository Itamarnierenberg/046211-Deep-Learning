{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzV9wsJ5pGhf"
   },
   "source": [
    "# <img src=\"https://img.icons8.com/bubbles/50/000000/mind-map.png\" style=\"height:50px;display:inline\"> ECE 046211 - Technion - Deep Learning\n",
    "---\n",
    "\n",
    "## HW3 - Sequential Tasks and Training Methods\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq2c8X93pGhh"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/clouds/96/000000/keyboard.png\" style=\"height:50px;display:inline\"> Keyboard Shortcuts\n",
    "---\n",
    "* Run current cell: **Ctrl + Enter**\n",
    "* Run current cell and move to the next: **Shift + Enter**\n",
    "* Show lines in a code cell: **Esc + L**\n",
    "* View function documentation: **Shift + Tab** inside the parenthesis or `help(name_of_module)`\n",
    "* New cell below: **Esc + B**\n",
    "* Delete cell: **Esc + D, D** (two D's)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZZybn3NpGhh"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/information.png\" style=\"height:50px;display:inline\"> Students Information\n",
    "---\n",
    "* Fill in\n",
    "\n",
    "|Name     |Campus Email| ID  |\n",
    "|---------|--------------------------------|----------|\n",
    "|Student 1| student_1@campus.technion.ac.il| 123456789|\n",
    "|Student 2| student_2@campus.technion.ac.il| 987654321|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDK5zqhdpGhi"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/upload-to-cloud.png\" style=\"height:50px;display:inline\"> Submission Guidelines\n",
    "---\n",
    "* Maximal garde: 100.\n",
    "* Submission only in **pairs**. \n",
    "    * Please make sure you have registered your group in Moodle (there is a group creation component on the Moodle where you need to create your group and assign members).\n",
    "* **No handwritten submissions.** You can choose whether to answer in a Markdown cell in this notebook or attach a PDF with your answers.\n",
    "* <a style='color:red'> SAVE THE NOTEBOOKS WITH THE OUTPUT, CODE CELLS THAT WERE NOT RUN WILL NOT GET ANY POINTS! </a>\n",
    "* What you have to submit:\n",
    "    * If you have answered the questions in the notebook, you should submit this file only, with the name: `ece046211_hw3_id1_id2.ipynb`.\n",
    "    * If you answered the questionss in a different file you should submit a `.zip` file with the name `ece046211_hw3_id1_id2.zip` with content:\n",
    "        * `ece046211_hw3_id1_id2.ipynb` - the code tasks\n",
    "        * `ece046211_hw3_id1_id2.pdf` - answers to questions.\n",
    "    * No other file-types (`.py`, `.docx`...) will be accepted.\n",
    "* Submission on the course website (Moodle).\n",
    "* **Latex in Colab** - in some cases, Latex equations may no be rendered. To avoid this, make sure to not use *bullets* in your answers (\"* some text here with Latex equations\" -> \"some text here with Latex equations\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmSj_UufpGhi"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/online.png\" style=\"height:50px;display:inline\"> Working Online and Locally\n",
    "---\n",
    "* You can choose your working environment:\n",
    "    1. `Jupyter Notebook`, **locally** with <a href=\"https://www.anaconda.com/distribution/\">Anaconda</a> or **online** on <a href=\"https://colab.research.google.com/\">Google Colab</a>\n",
    "        * Colab also supports running code on GPU, so if you don't have one, Colab is the way to go. To enable GPU on Colab, in the menu: `Runtime`$\\rightarrow$ `Change Runtime Type` $\\rightarrow$`GPU`.\n",
    "    2. Python IDE such as <a href=\"https://www.jetbrains.com/pycharm/\">PyCharm</a> or <a href=\"https://code.visualstudio.com/\">Visual Studio Code</a>.\n",
    "        * Both allow editing and running Jupyter Notebooks.\n",
    "\n",
    "* Please refer to `Setting Up the Working Environment.pdf` on the Moodle or our GitHub (https://github.com/taldatech/ee046211-deep-learning) to help you get everything installed.\n",
    "* If you need any technical assistance, please go to our Piazza forum (`hw3` folder) and describe your problem (preferably with images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlp1Fp4ppGhj"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "---\n",
    "\n",
    "* [Part 1 - Theory](#-Part-1---Theory)\n",
    "    * [Q1 - Dropout](#-Question-1--Dropout)\n",
    "    * [Q2 - Preventing Variance Explosion](#-Question-2--Preventing-Variance-Explosion)\n",
    "    * [Q3 - Batch Normalization](#-Solution-4--Batch-Normalization)\n",
    "* [Part 2 - Code Assignments - Sequence-to-Sequence with Transformers](#-Part-2---Code-Assignments)\n",
    "    * [Task 1 - Task 1 - Loading and Observing the Data](#-Task-1----Loading-and-Observing-the-Data)\n",
    "    * [Task 2 - Preparing the Data - Separating to Inputs and Targets](#-Task-2----Preparing-the--Data---Separating-to-Inputs-and-Targets)\n",
    "    * [Task 3 - Define Hyperparameters and Initialize the Model](#-Task-3----Define-Hyperparameters-and-Initialize-the-Model)\n",
    "    * [Task 4 - Train and Evaluate the Language Model](#-Task-4----Train-and-Evaluate-the-Language-Model)\n",
    "    * [Task 5 - Generate Sentences](#-Task-5----Generate-Sentences)\n",
    "* [Credits](#-Credits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKtSiQX_pGhj"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/cute-clipart/64/000000/ball-point-pen.png\" style=\"height:50px;display:inline\"> Part 1 - Theory\n",
    "---\n",
    "* You can choose whether to answser these straight in the notebook (Markdown + Latex) or use another editor (Word, LyX, Latex, Overleaf...) and submit an additional PDF file, **but no handwritten submissions**.\n",
    "* You can attach additional figures (drawings, graphs,...) in a separate PDF file, just make sure to refer to them in your answers.\n",
    "\n",
    "* $\\large\\LaTeX$ <a href=\"https://kapeli.com/cheat_sheets/LaTeX_Math_Symbols.docset/Contents/Resources/Documents/index\">Cheat-Sheet</a> (to write equations)\n",
    "    * <a href=\"http://tug.ctan.org/info/latex-refsheet/LaTeX_RefSheet.pdf\">Another Cheat-Sheet</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsqSFZG1pGhj"
   },
   "source": [
    "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 1 -Dropout\n",
    "---\n",
    "In this question, we are going to analyze the following idea:\n",
    "\n",
    "**Idea: use Droput regularization as a feature selection mechanism for the input.**\n",
    "\n",
    "To implement the idea, we wish to create a Dropout mask with probability $p_i$ to drop (=zero out) the $i^{th}$ component of the input feature vector, and optimize $p_i$ such that it encourages a deterministic selction of features (i.e., $p_i \\to 0 \\text{ or } 1$).\n",
    "\n",
    "We will analyze the method on the simple case of Linear Regression: $$ \\mathcal{L}(w)=\\frac{1}{2}\\sum_{n=1}^N\\left(y^{(n)} -w^TD^{(n)}x^{(n)} \\right)^2, $$ where $w \\in \\mathbb{R}^d$ is the parameters vector, $x^{(n)}\\in \\mathbb{R}^d$ are the trainin set samples, $y^{(n)} \\in \\mathbb{R}$ are the corresponding labels and $D^{(n)}\\in \\{0, 1 \\}^{d \\times d}$ is the *diagonal* random Dropout mask, where each element is sampled independently according to: \n",
    "$$ D_{ii}^{(n)}=\\frac{1}{1-p_i}\\begin{cases} 1 \\text{ w.p. } 1-p_i \\\\ 0 \\text{ w.p. } p_i \\end{cases} ,$$\n",
    "where $p_i \\in [0,1]$ is the probability to drop (=zero out) the $i^{th}$ component in the input vector, and we denote $p=[p_1, ..., p_d]^T$.\n",
    "\n",
    "1. Find $\\mathbb{E}[D_{ii}^{(n)}]$ and show that $\\mathbb{E}[D_{ii}^{(n)}D_{jj}^{(n)}]=1 + \\delta_{ij}\\frac{p_i}{1-p_i}$.\n",
    "\n",
    "2. Show that the mean cost function $\\bar{\\mathcal{L}}(w,p)$ (the mean is over the *masks*) is $$ \\bar{\\mathcal{L}}(w,p) = \\mathbb{E}[\\mathcal{L}] =\\frac{1}{2}\\sum_{n=1}^N \\left(y^{(n)} - w^Tx^{(n)} \\right)^2 + \\frac{1}{2}\\sum_{i=1}^d \\frac{p_i}{1-p_i}c_iw_i^2, $$ where $c_i = \\sum_{n=1}^N \\left(x_i^{(n)} \\right)^2$. From this section onwards, you can always assume $\\forall i : c_i >0$.\n",
    "\n",
    "3. Briefly explain what is the difference between $\\bar{\\mathcal{L}}(w,p)$ and the standard Linear Regression loss function without Dropout.\n",
    "\n",
    "4. Note that $\\bar{\\mathcal{L}}(w,p)$ is dependent on $p$, but we know that $p_i \\in [0,1]$. Suggest a function $p=f(u)$ such that we can use Gradient Descent without cinstraints on $\\bar{\\mathcal{L}}(w,f(u))$.\n",
    "\n",
    "5. Recall that we want $p_i \\to 1$ for some features and for the rest $p_i \\to 0$. Assume that there exists a *sparse* solution $w_0$ (that includes zeros), and a *dense* (non-sparse) solution $w_*$ such that $$ \\forall n : y^{(n)} = w_0^Tx^{(n)}=w_*^Tx^{(n)}.$$ Does that necessarily mean that we get $w_0$ by reaching the minimum of $\\bar{\\mathcal{L}}(w,p)$ at $w,p$?\n",
    "\n",
    "6. After some experiments, we got an improvement by adding noise to the input and regularization on $p$, and got $$ \\bar{\\mathcal{L}}(w,p) = \\mathbb{E}[\\mathcal{L}] =\\frac{1}{2}\\sum_{n=1}^N \\left(y^{(n)} - w^Tx^{(n)} \\right)^2 + \\frac{1}{2}\\sum_{i=1}^d \\frac{1}{1-p_i}w_i^2\\sum_{n=1}^N\\left(x_i^{(n)}\\right)^2 + \\mu\\sum_{i=1}^d(1-p_i). $$ Show by calculating $\\bar{\\mathcal{L}}(w)=\\min_{p\\in \\mathbb{R}^d}\\bar{\\mathcal{L}}(w,p)$ that we can omit the Dropout and instead add a regularization $R(w)$ directly to the Linear Regression. Calculate the regularization $R(w)$ and explain how it helps in feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsqSFZG1pGhj"
   },
   "source": [
    "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 2 - Preventing Variance Explosion\n",
    "---\n",
    "This question relates to lectures 8-9 (from slide 7):\n",
    "\n",
    "Find an initializtion scheme such that $$ \\forall l, i,: \\text{(1) } \\mathbb{E}\\left[F_l(u_l)|u_l\\right]=0, \\text{ (2) } Var(u_l[i]) = 1, $$ assuming skip connections: $u_{l+1} = u_l + F_l(u_l)$ with a single skip $F_l(u_l)=W_l\\phi(u_l)+b_l$ and the activation is ReLU: $\\phi(x) = \\text{ReLU}(x) = \\max(0,x)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsqSFZG1pGhj"
   },
   "source": [
    "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 3 -Batch Normalization\n",
    "---\n",
    "This question relates to lectures 8-9 (from slide 9):\n",
    "\n",
    "Prove that **without** regularization, BatchNorm **scale invariance** for parameters $\\mathbf{w}$ implies:\n",
    "1. $\\nabla \\mathcal{L}(\\mathbf{w})^T\\mathbf{w} = 0$\n",
    "2. And under gradient flow dynamics ($\\dot{\\mathbf{w}} = -\\eta \\nabla \\mathcal{L}(\\mathbf{w})$) this implies (L2) norm conservation: $\\forall t: ||\\mathbf{w}(t)||^2 = C$\n",
    "\n",
    "Hint: see results from the multilayer networks lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7D-14iM7pGhm"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/officel/80/000000/code.png\" style=\"height:50px;display:inline\"> Part 2 - Code Assignments\n",
    "---\n",
    "* You must write your code in this notebook and save it with the output of all of the code cells.\n",
    "* Additional text can be added in Markdown cells.\n",
    "* You can use any other IDE you like (PyCharm, VSCode...) to write/debug your code, but for the submission you must copy it to this notebook, run the code and save the notebook with the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T13:19:55.060787Z",
     "start_time": "2023-06-03T13:19:55.033201Z"
    }
   },
   "outputs": [],
   "source": [
    "# this part uses the Wikitext-2 dataset. To access torchtext datasets, please install `torchdata`:\n",
    "# `pip install torchdata` ir `conda install -c pytorch torchdata` in activated environment \n",
    "# or `!pip install torchdata` on colab.\n",
    "!pip install torchdata\n",
    "# notes:\n",
    "# torch=2.0.0 <-> torchtext 0.15.1\n",
    "# torch=1.13.0 <-> torchtext 0.14.0\n",
    "# torch=1.12.1 <-> torchtext 0.13.1\n",
    "# downgrading torchtext example: !pip install torchtext==0.13.1 --no-deps\n",
    "# torchtext requires the `portalocker` package to download datasets:\n",
    "!pip install portalocker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "ExecuteTime": {
     "end_time": "2023-06-04T07:24:22.724957Z",
     "start_time": "2023-06-04T07:24:22.690136Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports for the practice (you can add more if you need)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "# torchtext\n",
    "import torchtext\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "seed = 211\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T07:24:22.727908Z",
     "start_time": "2023-06-04T07:24:22.693788Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7f78e0be5d38>"
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'pytorch: {torch.__version__}, torchtext: {torchtext.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "ExecuteTime": {
     "end_time": "2023-06-04T07:24:22.755889Z",
     "start_time": "2023-06-04T07:24:22.704226Z"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/bubbles/50/000000/workflow.png\" style=\"height:50px;display:inline\">  Sequence-to-Sequence with Transformers\n",
    "---\n",
    "* In this exercise, you are going to build a language model using PyTroch's Transformer module.\n",
    "* We will work with the **Wikitext-2** dataset: the WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia.\n",
    "* After training, you will be able to generate senetences!"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch: 1.10.2, torchtext: 0.11.2\n"
     ]
    }
   ],
   "execution_count": 136
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 1  - Loading and Observing the Data\n",
    "---\n",
    "1. Run the following cells that define the functions `batchify` and `data_process` and initialize the tokenizer, vocabulary and the WikiText2 train dataset.\n",
    "2. Create the train, valid and test data using the provided `batchify` function.\n",
    "5. Print the shape of `train_data`, write in a comment the meaning of each dimension (e.g. `# [meaning of dim1, meaning of dim2]`).\n",
    "6. Print the first 20 words of one training sample from `train_data`. Use the vocabulary you built to transfer between tokens to words: `itos = vocab.vocab.get_itos()` will give a \"int to string\" list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T13:19:55.072202Z",
     "start_time": "2023-06-03T13:19:55.045902Z"
    }
   },
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T07:24:22.757378Z",
     "start_time": "2023-06-04T07:24:22.708112Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T07:24:22.758147Z",
     "start_time": "2023-06-04T07:24:22.711522Z"
    }
   },
   "outputs": [],
   "source": [
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T07:24:23.906217Z",
     "start_time": "2023-06-04T07:24:22.715214Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_iter was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T07:24:25.390358Z",
     "start_time": "2023-06-04T07:24:23.894950Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "eval_batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T07:24:25.390926Z",
     "start_time": "2023-06-04T07:24:25.372665Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your Code Here\n",
    "\"\"\"\n",
    "train_data = # complete\n",
    "val_data = # complete\n",
    "test_data = # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T07:24:25.409869Z",
     "start_time": "2023-06-04T07:24:25.378149Z"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 2  - Preparing the  Data - Separating to Inputs and Targets\n",
    "---\n",
    "* For a language modeling task, the model needs the following words as `Target`.\n",
    "    * For example, for the senetence \"I have a nice dog\", the model will be given \"I have a nice\" as input, and \"have a nice dog\" as the target.\n",
    "* Implement (complete) the function `get_batch(source, i, bptt)`: it generates the input and target sequence for the transformer model. It subdivides the source data into chunks of length `bptt`.\n",
    "    * For example, for `bptt=2` and at `i=0`, the output of `data, target = get_batch(train_data, i=0, bptt=2)`: `data` will be of shape (2, 20), where the batch size is 20 and `target` will be of length 40 (the target for each element is two words, but we flatten `target`).\n",
    "    * Example: for `bptt=2`, and the ABCDEFG... characters as input, our batches will be in the form of: `data=[a, b], target=[b, c]`. For `bptt=3`: `data=[a, b, c], target=[b, c, d]` and so on. This one example is a batch.\n",
    "    * Print a sample from `data` and `target`."
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset shape is = torch.Size([102499, 20])\n",
      "First 20 words = = all party major modern polygamist , present arrangement trying <unk> military offering the and or while escaping receive the\n"
     ]
    }
   ],
   "execution_count": 142
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T13:19:57.732763Z",
     "start_time": "2023-06-03T13:19:57.730493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Data = =\n",
      "Sample Data = valkyria\n",
      "\n",
      "\n",
      "Sample Target = valkyria\n",
      "Sample Target = chronicles\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Your Code Here\n",
    "\"\"\"\n",
    "def get_batch(source, i, bptt):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "        bptt: int\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
    "        target has shape [seq_len * batch_size]\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i + seq_len]\n",
    "    target = # compelte\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T07:24:25.417386Z",
     "start_time": "2023-06-04T07:24:25.395942Z"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 3  - Define Hyperparameters and Initialize the Model\n",
    "---\n",
    "* Define the following hyperparameters (`[a, b]` means in the range between `a` and `b`):\n",
    "    * Embedding size: choose from `[200, 250]`\n",
    "    * Number of hidden units: choose from `[200, 250]`\n",
    "    * Number of layers: choose from `[2, 4]`\n",
    "    * Number of attention heads: choose from `[2, 4]`\n",
    "    * Dropout: choose from `[0.0, 0.3]`\n",
    "    * Loss criterion: `nn.CrossEntropyLoss()`\n",
    "    * Optimizer: choose from `[SGD, Adam, RAdam]`\n",
    "    * Learning rate: choose from `[5e-3, 5.0]`\n",
    "    * Learning Scheduler: `torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)` or any scheduler of your choosing.\n",
    "    * Transformer LayerNormalization: `post` (`norm_first=False`) or `pre` (`norm_first=True`).\n",
    "* Intialize an instance of `TransformerModel` (given) and send it to `device`. Note that you need to give it the number of tokens to define the output of the decoder. You should use the number of tokens in the vocabulary. Print the number of tokens,  print **all** the chosen hyper-parameters and print the model (`print(model`)."
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Sample: valkyria chronicles iii = senjō no valkyria 3 <unk> chronicles\n",
      "Target Sample: chronicles iii = senjō no valkyria 3 <unk> chronicles (\n"
     ]
    }
   ],
   "execution_count": 143
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T13:19:57.758763Z",
     "start_time": "2023-06-03T13:19:57.732959Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5, norm_first=False):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout, norm_first=norm_first)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T07:24:25.417552Z",
     "start_time": "2023-06-04T07:24:25.410112Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your Code Here\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T08:12:47.752945Z",
     "start_time": "2023-06-04T08:12:47.567967Z"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 4  - Train and Evaluate the Language Model\n",
    "---\n",
    "* Fill in the missing line in the training code and train the model.\n",
    "* Use `bptt=35`.\n",
    "* Use the provided function to evaluate it on the validatation set (after each epoch) and on test test (after training is done). **Print and plot** the results (loss and perplexity).\n",
    "* If you see that the performance does not improve, go back to Task 3 and re-think you hyper-parameters."
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens = 28782\n",
      "Embedding Size = 240\n",
      "Num of hidden units = 220\n",
      "Num of layers = 3\n",
      "Num of attention heads = 2\n",
      "Dropout = 0.2\n",
      "Learning Rate = 0.1\n",
      "Optimizer = Adam\n",
      "Scheduler = StepLR\n",
      "Pre transformer layer normalization = False\n",
      "\n",
      "\n",
      "Model =\n",
      "TransformerModel(\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=240, out_features=240, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=240, out_features=220, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=220, out_features=240, bias=True)\n",
      "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=240, out_features=240, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=240, out_features=220, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=220, out_features=240, bias=True)\n",
      "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=240, out_features=240, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=240, out_features=220, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=220, out_features=240, bias=True)\n",
      "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (encoder): Embedding(28782, 240)\n",
      "  (decoder): Linear(in_features=240, out_features=28782, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 149
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-03T13:19:57.904026Z",
     "start_time": "2023-06-03T13:19:57.902096Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, eval_data):\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i, bptt)\n",
    "            seq_len = data.size(0)\n",
    "            if seq_len != bptt:\n",
    "                src_mask = src_mask[:seq_len, :seq_len]\n",
    "            output = model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T08:12:50.169777Z",
     "start_time": "2023-06-04T08:12:50.135696Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your Code Here\n",
    "\"\"\"\n",
    "def train(model, bptt):\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i, bptt)\n",
    "        seq_len = data.size(0)\n",
    "        if seq_len != bptt:  # only on last batch\n",
    "            src_mask = src_mask[:seq_len, :seq_len]\n",
    "        output = # complete\n",
    "        loss = # complete\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T08:12:51.796476Z",
     "start_time": "2023-06-04T08:12:51.775658Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your Code Here\n",
    "\"\"\"\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = # complete the number of epochs to run\n",
    "best_model = None\n",
    "bptt = 35\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    # complete: call train() here with appropriate paramteters\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T15:47:40.688649Z",
     "start_time": "2023-06-04T08:12:54.199557Z"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 5  - Generate Sentences\n",
    "---\n",
    "Use the following function to generate 3 sentences of length 20, and print them. Do they make sense? (you can compare generated sentences over epochs, to see if some logic is gained during training)."
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2928 batches | lr 0.10 | ms/batch 903.54 | loss  8.39 | ppl  4420.12\n",
      "| epoch   1 |   400/ 2928 batches | lr 0.10 | ms/batch 899.61 | loss  7.08 | ppl  1193.58\n",
      "| epoch   1 |   600/ 2928 batches | lr 0.10 | ms/batch 895.82 | loss  7.09 | ppl  1201.68\n",
      "| epoch   1 |   800/ 2928 batches | lr 0.10 | ms/batch 894.28 | loss  7.12 | ppl  1240.46\n",
      "| epoch   1 |  1000/ 2928 batches | lr 0.10 | ms/batch 893.66 | loss  7.13 | ppl  1253.01\n",
      "| epoch   1 |  1200/ 2928 batches | lr 0.10 | ms/batch 893.63 | loss  7.17 | ppl  1297.20\n",
      "| epoch   1 |  1400/ 2928 batches | lr 0.10 | ms/batch 895.24 | loss  7.21 | ppl  1349.10\n",
      "| epoch   1 |  1600/ 2928 batches | lr 0.10 | ms/batch 894.14 | loss  7.18 | ppl  1312.22\n",
      "| epoch   1 |  1800/ 2928 batches | lr 0.10 | ms/batch 894.54 | loss  7.17 | ppl  1298.67\n",
      "| epoch   1 |  2000/ 2928 batches | lr 0.10 | ms/batch 894.55 | loss  7.21 | ppl  1350.10\n",
      "| epoch   1 |  2200/ 2928 batches | lr 0.10 | ms/batch 893.20 | loss  7.16 | ppl  1292.71\n",
      "| epoch   1 |  2400/ 2928 batches | lr 0.10 | ms/batch 893.66 | loss  7.21 | ppl  1357.37\n",
      "| epoch   1 |  2600/ 2928 batches | lr 0.10 | ms/batch 895.03 | loss  7.22 | ppl  1366.37\n",
      "| epoch   1 |  2800/ 2928 batches | lr 0.10 | ms/batch 896.53 | loss  7.18 | ppl  1315.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [45:14<6:47:13, 2714.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 2714.80s | valid loss  7.23 | valid ppl  1375.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2928 batches | lr 0.10 | ms/batch 900.29 | loss  7.06 | ppl  1165.04\n",
      "| epoch   2 |   400/ 2928 batches | lr 0.10 | ms/batch 896.37 | loss  7.15 | ppl  1271.28\n",
      "| epoch   2 |   600/ 2928 batches | lr 0.10 | ms/batch 898.81 | loss  7.14 | ppl  1260.72\n",
      "| epoch   2 |   800/ 2928 batches | lr 0.10 | ms/batch 897.32 | loss  7.24 | ppl  1390.73\n",
      "| epoch   2 |  1000/ 2928 batches | lr 0.10 | ms/batch 896.53 | loss  7.30 | ppl  1485.71\n",
      "| epoch   2 |  1200/ 2928 batches | lr 0.10 | ms/batch 895.76 | loss  7.42 | ppl  1668.27\n",
      "| epoch   2 |  1400/ 2928 batches | lr 0.10 | ms/batch 895.35 | loss  7.44 | ppl  1697.47\n",
      "| epoch   2 |  1600/ 2928 batches | lr 0.10 | ms/batch 895.32 | loss  7.47 | ppl  1749.55\n",
      "| epoch   2 |  1800/ 2928 batches | lr 0.10 | ms/batch 894.29 | loss  7.57 | ppl  1930.74\n",
      "| epoch   2 |  2000/ 2928 batches | lr 0.10 | ms/batch 896.03 | loss  7.55 | ppl  1908.00\n",
      "| epoch   2 |  2200/ 2928 batches | lr 0.10 | ms/batch 893.69 | loss  7.56 | ppl  1917.21\n",
      "| epoch   2 |  2400/ 2928 batches | lr 0.10 | ms/batch 894.40 | loss  7.68 | ppl  2174.47\n",
      "| epoch   2 |  2600/ 2928 batches | lr 0.10 | ms/batch 896.67 | loss  7.67 | ppl  2150.32\n",
      "| epoch   2 |  2800/ 2928 batches | lr 0.10 | ms/batch 890.59 | loss  7.75 | ppl  2324.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [1:30:27<6:01:48, 2713.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 2712.80s | valid loss  8.39 | valid ppl  4422.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2928 batches | lr 0.05 | ms/batch 888.09 | loss  7.23 | ppl  1381.29\n",
      "| epoch   3 |   400/ 2928 batches | lr 0.05 | ms/batch 888.80 | loss  7.19 | ppl  1332.59\n",
      "| epoch   3 |   600/ 2928 batches | lr 0.05 | ms/batch 890.76 | loss  7.13 | ppl  1243.92\n",
      "| epoch   3 |   800/ 2928 batches | lr 0.05 | ms/batch 893.22 | loss  7.13 | ppl  1249.46\n",
      "| epoch   3 |  1000/ 2928 batches | lr 0.05 | ms/batch 893.24 | loss  7.16 | ppl  1289.90\n",
      "| epoch   3 |  1200/ 2928 batches | lr 0.05 | ms/batch 892.19 | loss  7.17 | ppl  1300.05\n",
      "| epoch   3 |  1400/ 2928 batches | lr 0.05 | ms/batch 894.28 | loss  7.20 | ppl  1343.14\n",
      "| epoch   3 |  1600/ 2928 batches | lr 0.05 | ms/batch 893.40 | loss  7.21 | ppl  1352.75\n",
      "| epoch   3 |  1800/ 2928 batches | lr 0.05 | ms/batch 895.60 | loss  7.21 | ppl  1349.66\n",
      "| epoch   3 |  2000/ 2928 batches | lr 0.05 | ms/batch 895.24 | loss  7.23 | ppl  1384.87\n",
      "| epoch   3 |  2200/ 2928 batches | lr 0.05 | ms/batch 894.18 | loss  7.22 | ppl  1363.42\n",
      "| epoch   3 |  2400/ 2928 batches | lr 0.05 | ms/batch 894.43 | loss  7.27 | ppl  1442.76\n",
      "| epoch   3 |  2600/ 2928 batches | lr 0.05 | ms/batch 893.41 | loss  7.25 | ppl  1408.92\n",
      "| epoch   3 |  2800/ 2928 batches | lr 0.05 | ms/batch 895.70 | loss  7.23 | ppl  1379.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [2:15:35<5:16:15, 2710.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 2707.41s | valid loss  7.61 | valid ppl  2011.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 2928 batches | lr 0.05 | ms/batch 898.22 | loss  6.99 | ppl  1089.26\n",
      "| epoch   4 |   400/ 2928 batches | lr 0.05 | ms/batch 895.56 | loss  7.01 | ppl  1108.44\n",
      "| epoch   4 |   600/ 2928 batches | lr 0.05 | ms/batch 893.98 | loss  6.93 | ppl  1027.40\n",
      "| epoch   4 |   800/ 2928 batches | lr 0.05 | ms/batch 896.39 | loss  6.99 | ppl  1083.87\n",
      "| epoch   4 |  1000/ 2928 batches | lr 0.05 | ms/batch 893.82 | loss  6.99 | ppl  1089.65\n",
      "| epoch   4 |  1200/ 2928 batches | lr 0.05 | ms/batch 895.36 | loss  7.04 | ppl  1147.07\n",
      "| epoch   4 |  1400/ 2928 batches | lr 0.05 | ms/batch 895.23 | loss  7.05 | ppl  1149.56\n",
      "| epoch   4 |  1600/ 2928 batches | lr 0.05 | ms/batch 894.61 | loss  7.03 | ppl  1133.90\n",
      "| epoch   4 |  1800/ 2928 batches | lr 0.05 | ms/batch 894.72 | loss  7.07 | ppl  1181.92\n",
      "| epoch   4 |  2000/ 2928 batches | lr 0.05 | ms/batch 886.91 | loss  7.08 | ppl  1183.95\n",
      "| epoch   4 |  2200/ 2928 batches | lr 0.05 | ms/batch 882.97 | loss  7.02 | ppl  1122.46\n",
      "| epoch   4 |  2400/ 2928 batches | lr 0.05 | ms/batch 882.36 | loss  7.03 | ppl  1134.80\n",
      "| epoch   4 |  2600/ 2928 batches | lr 0.05 | ms/batch 886.50 | loss  7.04 | ppl  1146.17\n",
      "| epoch   4 |  2800/ 2928 batches | lr 0.05 | ms/batch 891.60 | loss  7.05 | ppl  1150.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [3:00:38<4:30:48, 2708.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 2703.91s | valid loss  7.52 | valid ppl  1847.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 2928 batches | lr 0.03 | ms/batch 898.84 | loss  6.98 | ppl  1070.59\n",
      "| epoch   5 |   400/ 2928 batches | lr 0.03 | ms/batch 894.97 | loss  7.06 | ppl  1158.79\n",
      "| epoch   5 |   600/ 2928 batches | lr 0.03 | ms/batch 896.85 | loss  6.98 | ppl  1074.36\n",
      "| epoch   5 |   800/ 2928 batches | lr 0.03 | ms/batch 884.75 | loss  7.03 | ppl  1129.16\n",
      "| epoch   5 |  1000/ 2928 batches | lr 0.03 | ms/batch 883.28 | loss  7.00 | ppl  1095.45\n",
      "| epoch   5 |  1200/ 2928 batches | lr 0.03 | ms/batch 884.58 | loss  7.07 | ppl  1173.55\n",
      "| epoch   5 |  1400/ 2928 batches | lr 0.03 | ms/batch 886.50 | loss  7.05 | ppl  1158.18\n",
      "| epoch   5 |  1600/ 2928 batches | lr 0.03 | ms/batch 889.40 | loss  7.04 | ppl  1146.18\n",
      "| epoch   5 |  1800/ 2928 batches | lr 0.03 | ms/batch 890.17 | loss  7.05 | ppl  1149.65\n",
      "| epoch   5 |  2000/ 2928 batches | lr 0.03 | ms/batch 895.58 | loss  7.04 | ppl  1141.06\n",
      "| epoch   5 |  2200/ 2928 batches | lr 0.03 | ms/batch 895.78 | loss  6.98 | ppl  1072.43\n",
      "| epoch   5 |  2400/ 2928 batches | lr 0.03 | ms/batch 894.19 | loss  7.00 | ppl  1102.05\n",
      "| epoch   5 |  2600/ 2928 batches | lr 0.03 | ms/batch 894.84 | loss  7.05 | ppl  1149.09\n",
      "| epoch   5 |  2800/ 2928 batches | lr 0.03 | ms/batch 895.08 | loss  6.96 | ppl  1058.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [3:45:42<3:45:32, 2706.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 2703.48s | valid loss  7.46 | valid ppl  1729.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/ 2928 batches | lr 0.03 | ms/batch 899.68 | loss  6.85 | ppl   945.28\n",
      "| epoch   6 |   400/ 2928 batches | lr 0.03 | ms/batch 896.36 | loss  6.92 | ppl  1011.68\n",
      "| epoch   6 |   600/ 2928 batches | lr 0.03 | ms/batch 895.58 | loss  6.84 | ppl   933.61\n",
      "| epoch   6 |   800/ 2928 batches | lr 0.03 | ms/batch 895.81 | loss  6.92 | ppl  1008.12\n",
      "| epoch   6 |  1000/ 2928 batches | lr 0.03 | ms/batch 895.16 | loss  6.91 | ppl   997.91\n",
      "| epoch   6 |  1200/ 2928 batches | lr 0.03 | ms/batch 895.81 | loss  6.97 | ppl  1065.22\n",
      "| epoch   6 |  1400/ 2928 batches | lr 0.03 | ms/batch 895.85 | loss  6.96 | ppl  1050.29\n",
      "| epoch   6 |  1600/ 2928 batches | lr 0.03 | ms/batch 897.57 | loss  6.96 | ppl  1054.44\n",
      "| epoch   6 |  1800/ 2928 batches | lr 0.03 | ms/batch 897.32 | loss  6.97 | ppl  1064.98\n",
      "| epoch   6 |  2000/ 2928 batches | lr 0.03 | ms/batch 898.10 | loss  6.95 | ppl  1044.73\n",
      "| epoch   6 |  2200/ 2928 batches | lr 0.03 | ms/batch 897.52 | loss  6.90 | ppl   992.96\n",
      "| epoch   6 |  2400/ 2928 batches | lr 0.03 | ms/batch 897.40 | loss  6.95 | ppl  1043.66\n",
      "| epoch   6 |  2600/ 2928 batches | lr 0.03 | ms/batch 897.84 | loss  6.96 | ppl  1057.10\n",
      "| epoch   6 |  2800/ 2928 batches | lr 0.03 | ms/batch 897.73 | loss  6.93 | ppl  1019.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [4:31:01<3:00:42, 2710.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 2719.03s | valid loss  7.54 | valid ppl  1875.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/ 2928 batches | lr 0.01 | ms/batch 899.03 | loss  6.94 | ppl  1036.19\n",
      "| epoch   7 |   400/ 2928 batches | lr 0.01 | ms/batch 890.96 | loss  6.95 | ppl  1048.18\n",
      "| epoch   7 |   600/ 2928 batches | lr 0.01 | ms/batch 883.55 | loss  6.90 | ppl   993.83\n",
      "| epoch   7 |   800/ 2928 batches | lr 0.01 | ms/batch 883.43 | loss  6.93 | ppl  1018.45\n",
      "| epoch   7 |  1000/ 2928 batches | lr 0.01 | ms/batch 891.30 | loss  6.90 | ppl   989.76\n",
      "| epoch   7 |  1200/ 2928 batches | lr 0.01 | ms/batch 887.52 | loss  6.98 | ppl  1071.53\n",
      "| epoch   7 |  1400/ 2928 batches | lr 0.01 | ms/batch 896.49 | loss  6.97 | ppl  1064.54\n",
      "| epoch   7 |  1600/ 2928 batches | lr 0.01 | ms/batch 885.48 | loss  6.97 | ppl  1069.19\n",
      "| epoch   7 |  1800/ 2928 batches | lr 0.01 | ms/batch 897.17 | loss  6.94 | ppl  1035.44\n",
      "| epoch   7 |  2000/ 2928 batches | lr 0.01 | ms/batch 898.67 | loss  6.94 | ppl  1029.96\n",
      "| epoch   7 |  2200/ 2928 batches | lr 0.01 | ms/batch 900.75 | loss  6.90 | ppl   989.59\n",
      "| epoch   7 |  2400/ 2928 batches | lr 0.01 | ms/batch 895.34 | loss  6.93 | ppl  1020.27\n",
      "| epoch   7 |  2600/ 2928 batches | lr 0.01 | ms/batch 896.84 | loss  6.93 | ppl  1022.79\n",
      "| epoch   7 |  2800/ 2928 batches | lr 0.01 | ms/batch 899.17 | loss  6.88 | ppl   975.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [5:16:09<2:15:29, 2709.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 2707.89s | valid loss  7.34 | valid ppl  1547.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/ 2928 batches | lr 0.01 | ms/batch 889.34 | loss  6.81 | ppl   909.42\n",
      "| epoch   8 |   400/ 2928 batches | lr 0.01 | ms/batch 884.31 | loss  6.82 | ppl   919.75\n",
      "| epoch   8 |   600/ 2928 batches | lr 0.01 | ms/batch 889.25 | loss  6.77 | ppl   871.12\n",
      "| epoch   8 |   800/ 2928 batches | lr 0.01 | ms/batch 895.31 | loss  6.84 | ppl   937.58\n",
      "| epoch   8 |  1000/ 2928 batches | lr 0.01 | ms/batch 899.33 | loss  6.81 | ppl   905.92\n",
      "| epoch   8 |  1200/ 2928 batches | lr 0.01 | ms/batch 904.84 | loss  6.88 | ppl   975.60\n",
      "| epoch   8 |  1400/ 2928 batches | lr 0.01 | ms/batch 900.89 | loss  6.89 | ppl   984.74\n",
      "| epoch   8 |  1600/ 2928 batches | lr 0.01 | ms/batch 910.79 | loss  6.90 | ppl   995.09\n",
      "| epoch   8 |  1800/ 2928 batches | lr 0.01 | ms/batch 904.91 | loss  6.86 | ppl   949.99\n",
      "| epoch   8 |  2000/ 2928 batches | lr 0.01 | ms/batch 908.93 | loss  6.87 | ppl   967.52\n",
      "| epoch   8 |  2200/ 2928 batches | lr 0.01 | ms/batch 910.39 | loss  6.83 | ppl   923.82\n",
      "| epoch   8 |  2400/ 2928 batches | lr 0.01 | ms/batch 906.97 | loss  6.88 | ppl   970.06\n",
      "| epoch   8 |  2600/ 2928 batches | lr 0.01 | ms/batch 908.58 | loss  6.87 | ppl   959.31\n",
      "| epoch   8 |  2800/ 2928 batches | lr 0.01 | ms/batch 906.01 | loss  6.83 | ppl   928.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [6:01:43<1:30:35, 2717.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 2734.29s | valid loss  7.41 | valid ppl  1655.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   200/ 2928 batches | lr 0.01 | ms/batch 909.73 | loss  6.88 | ppl   972.16\n",
      "| epoch   9 |   400/ 2928 batches | lr 0.01 | ms/batch 903.80 | loss  6.90 | ppl   990.92\n",
      "| epoch   9 |   600/ 2928 batches | lr 0.01 | ms/batch 908.98 | loss  6.82 | ppl   919.30\n",
      "| epoch   9 |   800/ 2928 batches | lr 0.01 | ms/batch 908.31 | loss  6.87 | ppl   963.61\n",
      "| epoch   9 |  1000/ 2928 batches | lr 0.01 | ms/batch 904.96 | loss  6.82 | ppl   918.46\n",
      "| epoch   9 |  1200/ 2928 batches | lr 0.01 | ms/batch 910.56 | loss  6.91 | ppl  1004.26\n",
      "| epoch   9 |  1400/ 2928 batches | lr 0.01 | ms/batch 902.90 | loss  6.90 | ppl   991.61\n",
      "| epoch   9 |  1600/ 2928 batches | lr 0.01 | ms/batch 887.90 | loss  6.89 | ppl   981.42\n",
      "| epoch   9 |  1800/ 2928 batches | lr 0.01 | ms/batch 900.19 | loss  6.85 | ppl   945.07\n",
      "| epoch   9 |  2000/ 2928 batches | lr 0.01 | ms/batch 900.93 | loss  6.89 | ppl   978.08\n",
      "| epoch   9 |  2200/ 2928 batches | lr 0.01 | ms/batch 900.34 | loss  6.82 | ppl   919.48\n",
      "| epoch   9 |  2400/ 2928 batches | lr 0.01 | ms/batch 907.84 | loss  6.86 | ppl   954.67\n",
      "| epoch   9 |  2600/ 2928 batches | lr 0.01 | ms/batch 905.00 | loss  6.84 | ppl   933.18\n",
      "| epoch   9 |  2800/ 2928 batches | lr 0.01 | ms/batch 902.38 | loss  6.84 | ppl   930.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [6:47:23<45:24, 2724.40s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 2739.37s | valid loss  7.17 | valid ppl  1302.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/ 2928 batches | lr 0.01 | ms/batch 907.28 | loss  6.78 | ppl   877.61\n",
      "| epoch  10 |   400/ 2928 batches | lr 0.01 | ms/batch 904.65 | loss  6.80 | ppl   896.71\n",
      "| epoch  10 |   600/ 2928 batches | lr 0.01 | ms/batch 903.61 | loss  6.74 | ppl   846.69\n",
      "| epoch  10 |   800/ 2928 batches | lr 0.01 | ms/batch 903.36 | loss  6.79 | ppl   892.58\n",
      "| epoch  10 |  1000/ 2928 batches | lr 0.01 | ms/batch 902.60 | loss  6.76 | ppl   860.38\n",
      "| epoch  10 |  1200/ 2928 batches | lr 0.01 | ms/batch 902.35 | loss  6.85 | ppl   942.93\n",
      "| epoch  10 |  1400/ 2928 batches | lr 0.01 | ms/batch 901.25 | loss  6.83 | ppl   925.85\n",
      "| epoch  10 |  1600/ 2928 batches | lr 0.01 | ms/batch 898.58 | loss  6.83 | ppl   926.50\n",
      "| epoch  10 |  1800/ 2928 batches | lr 0.01 | ms/batch 898.33 | loss  6.79 | ppl   892.83\n",
      "| epoch  10 |  2000/ 2928 batches | lr 0.01 | ms/batch 898.87 | loss  6.83 | ppl   920.85\n",
      "| epoch  10 |  2200/ 2928 batches | lr 0.01 | ms/batch 906.01 | loss  6.77 | ppl   871.56\n",
      "| epoch  10 |  2400/ 2928 batches | lr 0.01 | ms/batch 910.67 | loss  6.80 | ppl   899.84\n",
      "| epoch  10 |  2600/ 2928 batches | lr 0.01 | ms/batch 908.96 | loss  6.78 | ppl   883.68\n",
      "| epoch  10 |  2800/ 2928 batches | lr 0.01 | ms/batch 900.84 | loss  6.79 | ppl   890.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [7:33:00<00:00, 2718.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 2737.72s | valid loss  7.21 | valid ppl  1350.35\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 7.148878623746916\n",
      "Test PPL = 1272.6780037964272\n"
     ]
    }
   ],
   "execution_count": 152
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, vocab, nwords=100, temp=1.0):\n",
    "    model.eval()\n",
    "    ntokens = len(vocab)\n",
    "    itos = vocab.vocab.get_itos()\n",
    "    model_input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
    "    words = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(nwords):\n",
    "            output = model(model_input, None)\n",
    "            word_weights = output[-1].squeeze().div(temp).exp().cpu()\n",
    "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "            word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
    "            model_input = torch.cat([model_input, word_tensor], 0)\n",
    "            word = itos[word_idx]\n",
    "            words.append(word)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-04T15:49:24.077535Z",
     "start_time": "2023-06-04T15:49:24.070745Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Yout code Here\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-04T08:12:29.317451Z"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "---\n",
    "* Icons made by <a href=\"https://www.flaticon.com/authors/becris\" title=\"Becris\">Becris</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\">www.flaticon.com</a>\n",
    "* Icons from <a href=\"https://icons8.com/\">Icons8.com</a> - https://icons8.com\n",
    "* Datasets from <a href=\"https://www.kaggle.com/\">Kaggle</a> - https://www.kaggle.com/"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
