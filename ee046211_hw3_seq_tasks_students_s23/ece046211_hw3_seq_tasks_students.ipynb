{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzV9wsJ5pGhf"
   },
   "source": [
    "# <img src=\"https://img.icons8.com/bubbles/50/000000/mind-map.png\" style=\"height:50px;display:inline\"> ECE 046211 - Technion - Deep Learning\n",
    "---\n",
    "\n",
    "## HW3 - Sequential Tasks and Training Methods\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq2c8X93pGhh"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/clouds/96/000000/keyboard.png\" style=\"height:50px;display:inline\"> Keyboard Shortcuts\n",
    "---\n",
    "* Run current cell: **Ctrl + Enter**\n",
    "* Run current cell and move to the next: **Shift + Enter**\n",
    "* Show lines in a code cell: **Esc + L**\n",
    "* View function documentation: **Shift + Tab** inside the parenthesis or `help(name_of_module)`\n",
    "* New cell below: **Esc + B**\n",
    "* Delete cell: **Esc + D, D** (two D's)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZZybn3NpGhh"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/information.png\" style=\"height:50px;display:inline\"> Students Information\n",
    "---\n",
    "* Fill in\n",
    "\n",
    "|Name     |Campus Email| ID  |\n",
    "|---------|--------------------------------|----------|\n",
    "|Student 1| student_1@campus.technion.ac.il| 123456789|\n",
    "|Student 2| student_2@campus.technion.ac.il| 987654321|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDK5zqhdpGhi"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/upload-to-cloud.png\" style=\"height:50px;display:inline\"> Submission Guidelines\n",
    "---\n",
    "* Maximal garde: 100.\n",
    "* Submission only in **pairs**. \n",
    "    * Please make sure you have registered your group in Moodle (there is a group creation component on the Moodle where you need to create your group and assign members).\n",
    "* **No handwritten submissions.** You can choose whether to answer in a Markdown cell in this notebook or attach a PDF with your answers.\n",
    "* <a style='color:red'> SAVE THE NOTEBOOKS WITH THE OUTPUT, CODE CELLS THAT WERE NOT RUN WILL NOT GET ANY POINTS! </a>\n",
    "* What you have to submit:\n",
    "    * If you have answered the questions in the notebook, you should submit this file only, with the name: `ece046211_hw3_id1_id2.ipynb`.\n",
    "    * If you answered the questionss in a different file you should submit a `.zip` file with the name `ece046211_hw3_id1_id2.zip` with content:\n",
    "        * `ece046211_hw3_id1_id2.ipynb` - the code tasks\n",
    "        * `ece046211_hw3_id1_id2.pdf` - answers to questions.\n",
    "    * No other file-types (`.py`, `.docx`...) will be accepted.\n",
    "* Submission on the course website (Moodle).\n",
    "* **Latex in Colab** - in some cases, Latex equations may no be rendered. To avoid this, make sure to not use *bullets* in your answers (\"* some text here with Latex equations\" -> \"some text here with Latex equations\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmSj_UufpGhi"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/online.png\" style=\"height:50px;display:inline\"> Working Online and Locally\n",
    "---\n",
    "* You can choose your working environment:\n",
    "    1. `Jupyter Notebook`, **locally** with <a href=\"https://www.anaconda.com/distribution/\">Anaconda</a> or **online** on <a href=\"https://colab.research.google.com/\">Google Colab</a>\n",
    "        * Colab also supports running code on GPU, so if you don't have one, Colab is the way to go. To enable GPU on Colab, in the menu: `Runtime`$\\rightarrow$ `Change Runtime Type` $\\rightarrow$`GPU`.\n",
    "    2. Python IDE such as <a href=\"https://www.jetbrains.com/pycharm/\">PyCharm</a> or <a href=\"https://code.visualstudio.com/\">Visual Studio Code</a>.\n",
    "        * Both allow editing and running Jupyter Notebooks.\n",
    "\n",
    "* Please refer to `Setting Up the Working Environment.pdf` on the Moodle or our GitHub (https://github.com/taldatech/ee046211-deep-learning) to help you get everything installed.\n",
    "* If you need any technical assistance, please go to our Piazza forum (`hw3` folder) and describe your problem (preferably with images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlp1Fp4ppGhj"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "---\n",
    "\n",
    "* [Part 1 - Theory](#-Part-1---Theory)\n",
    "    * [Q1 - Dropout](#-Question-1--Dropout)\n",
    "    * [Q2 - Preventing Variance Explosion](#-Question-2--Preventing-Variance-Explosion)\n",
    "    * [Q3 - Batch Normalization](#-Solution-4--Batch-Normalization)\n",
    "* [Part 2 - Code Assignments - Sequence-to-Sequence with Transformers](#-Part-2---Code-Assignments)\n",
    "    * [Task 1 - Task 1 - Loading and Observing the Data](#-Task-1----Loading-and-Observing-the-Data)\n",
    "    * [Task 2 - Preparing the Data - Separating to Inputs and Targets](#-Task-2----Preparing-the--Data---Separating-to-Inputs-and-Targets)\n",
    "    * [Task 3 - Define Hyperparameters and Initialize the Model](#-Task-3----Define-Hyperparameters-and-Initialize-the-Model)\n",
    "    * [Task 4 - Train and Evaluate the Language Model](#-Task-4----Train-and-Evaluate-the-Language-Model)\n",
    "    * [Task 5 - Generate Sentences](#-Task-5----Generate-Sentences)\n",
    "* [Credits](#-Credits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKtSiQX_pGhj"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/cute-clipart/64/000000/ball-point-pen.png\" style=\"height:50px;display:inline\"> Part 1 - Theory\n",
    "---\n",
    "* You can choose whether to answser these straight in the notebook (Markdown + Latex) or use another editor (Word, LyX, Latex, Overleaf...) and submit an additional PDF file, **but no handwritten submissions**.\n",
    "* You can attach additional figures (drawings, graphs,...) in a separate PDF file, just make sure to refer to them in your answers.\n",
    "\n",
    "* $\\large\\LaTeX$ <a href=\"https://kapeli.com/cheat_sheets/LaTeX_Math_Symbols.docset/Contents/Resources/Documents/index\">Cheat-Sheet</a> (to write equations)\n",
    "    * <a href=\"http://tug.ctan.org/info/latex-refsheet/LaTeX_RefSheet.pdf\">Another Cheat-Sheet</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsqSFZG1pGhj"
   },
   "source": [
    "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 1 -Dropout\n",
    "---\n",
    "In this question, we are going to analyze the following idea:\n",
    "\n",
    "**Idea: use Droput regularization as a feature selection mechanism for the input.**\n",
    "\n",
    "To implement the idea, we wish to create a Dropout mask with probability $p_i$ to drop (=zero out) the $i^{th}$ component of the input feature vector, and optimize $p_i$ such that it encourages a deterministic selction of features (i.e., $p_i \\to 0 \\text{ or } 1$).\n",
    "\n",
    "We will analyze the method on the simple case of Linear Regression: $$ \\mathcal{L}(w)=\\frac{1}{2}\\sum_{n=1}^N\\left(y^{(n)} -w^TD^{(n)}x^{(n)} \\right)^2, $$ where $w \\in \\mathbb{R}^d$ is the parameters vector, $x^{(n)}\\in \\mathbb{R}^d$ are the trainin set samples, $y^{(n)} \\in \\mathbb{R}$ are the corresponding labels and $D^{(n)}\\in \\{0, 1 \\}^{d \\times d}$ is the *diagonal* random Dropout mask, where each element is sampled independently according to: \n",
    "$$ D_{ii}^{(n)}=\\frac{1}{1-p_i}\\begin{cases} 1 \\text{ w.p. } 1-p_i \\\\ 0 \\text{ w.p. } p_i \\end{cases} ,$$\n",
    "where $p_i \\in [0,1]$ is the probability to drop (=zero out) the $i^{th}$ component in the input vector, and we denote $p=[p_1, ..., p_d]^T$.\n",
    "\n",
    "1. Find $\\mathbb{E}[D_{ii}^{(n)}]$ and show that $\\mathbb{E}[D_{ii}^{(n)}D_{jj}^{(n)}]=1 + \\delta_{ij}\\frac{p_i}{1-p_i}$.\n",
    "\n",
    "2. Show that the mean cost function $\\bar{\\mathcal{L}}(w,p)$ (the mean is over the *masks*) is $$ \\bar{\\mathcal{L}}(w,p) = \\mathbb{E}[\\mathcal{L}] =\\frac{1}{2}\\sum_{n=1}^N \\left(y^{(n)} - w^Tx^{(n)} \\right)^2 + \\frac{1}{2}\\sum_{i=1}^d \\frac{p_i}{1-p_i}c_iw_i^2, $$ where $c_i = \\sum_{n=1}^N \\left(x_i^{(n)} \\right)^2$. From this section onwards, you can always assume $\\forall i : c_i >0$.\n",
    "\n",
    "3. Briefly explain what is the difference between $\\bar{\\mathcal{L}}(w,p)$ and the standard Linear Regression loss function without Dropout.\n",
    "\n",
    "4. Note that $\\bar{\\mathcal{L}}(w,p)$ is dependent on $p$, but we know that $p_i \\in [0,1]$. Suggest a function $p=f(u)$ such that we can use Gradient Descent without cinstraints on $\\bar{\\mathcal{L}}(w,f(u))$.\n",
    "\n",
    "5. Recall that we want $p_i \\to 1$ for some features and for the rest $p_i \\to 0$. Assume that there exists a *sparse* solution $w_0$ (that includes zeros), and a *dense* (non-sparse) solution $w_*$ such that $$ \\forall n : y^{(n)} = w_0^Tx^{(n)}=w_*^Tx^{(n)}.$$ Does that necessarily mean that we get $w_0$ by reaching the minimum of $\\bar{\\mathcal{L}}(w,p)$ at $w,p$?\n",
    "\n",
    "6. After some experiments, we got an improvement by adding noise to the input and regularization on $p$, and got $$ \\bar{\\mathcal{L}}(w,p) = \\mathbb{E}[\\mathcal{L}] =\\frac{1}{2}\\sum_{n=1}^N \\left(y^{(n)} - w^Tx^{(n)} \\right)^2 + \\frac{1}{2}\\sum_{i=1}^d \\frac{1}{1-p_i}w_i^2\\sum_{n=1}^N\\left(x_i^{(n)}\\right)^2 + \\mu\\sum_{i=1}^d(1-p_i). $$ Show by calculating $\\bar{\\mathcal{L}}(w)=\\min_{p\\in \\mathbb{R}^d}\\bar{\\mathcal{L}}(w,p)$ that we can omit the Dropout and instead add a regularization $R(w)$ directly to the Linear Regression. Calculate the regularization $R(w)$ and explain how it helps in feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Question 2 - Answer\n",
    "--\n",
    "### Sub Section 1:\n",
    "We'll use the definiton of $ D_{ii}^{(n)} $ to calculate the expectation:\n",
    "$$ \\mathbb{E}[D_{ii}^{(n)}] = (1-p_i)\\cdot\\frac{1}{1-p_i} + 0\\cdot\\frac{1}{1-p_i} = 1ֿ$$\n",
    "\n",
    "For $ \\mathbb{E}[D_{ii}^{(n)}D_{jj}^{(n)}] $ Let's consider the two different possibilities:\n",
    "* $ i \\neq j  \\Rightarrow D_{ii}$ is independent of  $ D_{jj} $:\n",
    "$$ \\mathbb{E}[D_{ii}^{(n)}D_{jj}^{(n)}] = \\mathbb{E}[D_{ii}^{(n)}]\\cdot\\mathbb{E}[D_{jj}^{(n)}] = 1 $$\n",
    "* $ i = j $:\n",
    "$$ \\mathbb{E}[(D_{ii}^{(n)})^2] = (1-p_i)\\cdot\\frac{1}{(1-p_i)^2} + 0\\cdot \\frac{1}{(1-p_i)^2} = \\frac{1}{1-p_i}$$\n",
    "\n",
    "And when considering both cases the following applies:\n",
    "$$ \\mathbb{E}[D_{ii}^{(n)}D_{jj}^{(n)}] = 1 + \\delta_{ij}\\frac{p_i}{1-p_i} $$\n",
    "\n",
    "### Sub Section 2:\n",
    "\n",
    "$$ \\bar{\\mathcal{L}}(w,p) = \\mathbb{E}[\\mathcal{L}] = \\mathbb{E}[\\frac{1}{2} \\sum_{n=1}^N \\left(y^{(n)} - w^TD^{(n)}x^{(n)} \\right)^2] = \\frac{1}{2}\\sum_{n=1}^N \\mathbb{E}[((y^{(n)})^2 - 2w^TD^{(n)}x^{(n)}y^{(n)} + w^TD^{(n)}x^{(n)}w^TD^{(n)}x^{(n)})]$$\n",
    "\n",
    "At this point we will use the fact that the mean is over the masks and the results of sub-section 1:\n",
    "\n",
    "$$ \\frac{1}{2}\\sum_{n=1}^N \\mathbb{E}[((y^{(n)})^2 - 2w^TD^{(n)}x^{(n)}y^{(n)} + w^TD^{(n)}x^{(n)}w^TD^{(n)}x^{(n)})] = \\frac{1}{2}\\sum_{n=1}^N ((y^{(n)})^2 - 2w^Tx^{(n)}y^{(n)} + \\mathbb{E}[w^TD^{(n)}x^{(n)}w^TD^{(n)}x^{(n)}])$$\n",
    "\n",
    "Since $ w^TD^{(n)}x^{(n)} $ is a scalar then $ (w^TD^{(n)}x^{(n)})^T = w^TD^{(n)}x^{(n)} $:\n",
    "$$ \\frac{1}{2}\\sum_{n=1}^N ((y^{(n)})^2 - 2w^Tx^{(n)}y^{(n)} + \\mathbb{E}[w^TD^{(n)}x^{(n)}w^TD^{(n)}x^{(n)}]) = \\frac{1}{2}\\sum_{n=1}^N ((y^{(n)})^2 - 2w^Tx^{(n)}y^{(n)} + \\mathbb{E}[w^TD^{(n)}x^{(n)}(x^{(n)})^TD^{(n)}w]) = \\frac{1}{2}\\sum_{n=1}^N ((y^{(n)})^2 - 2w^Tx^{(n)}y^{(n)} + w^T\\mathbb{E}[D^{(n)}x^{(n)}(x^{(n)})^TD^{(n)}]w)$$\n",
    "\n",
    "Now we will examine closely the expression $ w^T\\mathbb{E}[D^{(n)}x^{(n)}(x^{(n)})^TD^{(n)}]w $:\n",
    "\n",
    "$$ x^{(n)}(x^{(n)})^T = \\begin{pmatrix} (x_1^{(n)})^2 & x_1^{(n)}x_2^{(n)} & x_1^{(n)}x_3^{(n)} & ... & x_1^{(n)}x_d^{(n)} \\\\ x_2^{(n)}x_1^{(n)} & (x_2^{(n)})^2 & x_2^{(n)}x_3^{(n)} & ... & x_2^{(n)}x_d^{(n)} \\\\ . & & & & . \\\\ . & & & & . \\\\ . & & & & . \\\\ x_d^{(n)}x_1^{(n)} & x_d^{(n)}x_2^{(n)} & x_d^{(n)}x_3^{(n)} & ... & (x_d^{(n)})^2 \\end{pmatrix} $$\n",
    "\n",
    "$$ D^{(n)}x^{(n)}(x^{(n)})^T = \\begin{pmatrix} D_{11}^{(n)}(x_1^{(n)})^2 & D_{11}^{(n)}x_1^{(n)}x_2^{(n)} & D_{11}^{(n)}x_1^{(n)}x_3^{(n)} & ... & D_{11}^{(n)}x_1^{(n)}x_d^{(n)} \\\\ D_{22}^{(n)}x_2^{(n)}x_1^{(n)} & D_{22}^{(n)}(x_2^{(n)})^2 & D_{22}^{(n)}x_2^{(n)}x_3^{(n)} & ... & D_{22}^{(n)}x_2^{(n)}x_d^{(n)} \\\\ . & & & & . \\\\ . & & & & . \\\\ . & & & & . \\\\ D_{dd}^{(n)}x_d^{(n)}x_1^{(n)} & D_{dd}^{(n)}x_d^{(n)}x_2^{(n)} & D_{dd}^{(n)}x_d^{(n)}x_3^{(n)} & ... & D_{dd}^{(n)}(x_d^{(n)})^2 \\end{pmatrix} $$\n",
    "\n",
    "$$ D^{(n)}x^{(n)}(x^{(n)})^TD^{(n)} = \\begin{pmatrix} (D_{11}^{(n)})^2(x_1^{(n)})^2 & D_{22}^{(n)}D_{11}^{(n)}x_1^{(n)}x_2^{(n)} & D_{33}^{(n)}D_{11}^{(n)}x_1^{(n)}x_3^{(n)} & ... & D_{dd}^{(n)}D_{11}^{(n)}x_1^{(n)}x_d^{(n)} \\\\ D_{11}^{(n)}D_{22}^{(n)}x_2^{(n)}x_1^{(n)} & (D_{22}^{(n)})^2(x_2^{(n)})^2 & D_{33}^{(n)}D_{22}^{(n)}x_2^{(n)}x_3^{(n)} & ... & D_{dd}^{(n)}D_{22}^{(n)}x_2^{(n)}x_d^{(n)} \\\\ . & & & & . \\\\ . & & & & . \\\\ . & & & & . \\\\ D_{11}^{(n)}D_{dd}^{(n)}x_d^{(n)}x_1^{(n)} & D_{22}^{(n)}D_{dd}^{(n)}x_d^{(n)}x_2^{(n)} & D_{33}^{(n)}D_{dd}^{(n)}x_d^{(n)}x_3^{(n)} & ... & (D_{dd}^{(n)})^2(x_d^{(n)})^2 \\end{pmatrix} $$\n",
    "\n",
    "So in conclusion we get $ [D^{(n)}x^{(n)}(x^{(n)})^TD^{(n)}]_{ij} = D_{ii}^{(n)}D_{jj}^{(n)}x_i^{(n)}x_j^{(n)} $ Now we can calculate based on sub section 1 results:\n",
    "\n",
    "$$ \\mathbb{E}[[D^{(n)}x^{(n)}(x^{(n)})^TD^{(n)}]_{ij}] = \\mathbb{E}[D_{ii}^{(n)}D_{jj}^{(n)}]x_i^{(n)}x_j^{(n)} = x_i^{(n)}x_j^{(n)} + \\delta_{ij}\\frac{p_i}{1-p_i}x_i^{(n)}x_j^{(n)} $$\n",
    "\n",
    "This means that:\n",
    "$$ \\mathbb{E}[D^{(n)}x^{(n)}(x^{(n)})^TD^{(n)}] = x^{(n)}(x^{(n)})^T + \\begin{pmatrix} \\frac{p_1}{1-p_1}(x_1^{(n)})^2 & 0 & 0 & ... & 0 \\\\ 0 & \\frac{p_2}{1-p_2}(x_2^{(n)})^2 & 0 & ... & 0 \\\\ 0 & 0 & \\frac{p_3}{1-p_3}(x_3^{(n)})^2 & ... & 0 \\\\ . & & & & . \\\\ . & & & & . \\\\ . & & & & . \\\\ 0 & 0 & 0 &  ... & \\frac{p_d}{1-p_d}(x_d^{(n)})^2 \\end{pmatrix} $$\n",
    "\n",
    "Let's notate:\n",
    "$$ B^{(n)} = \\begin{pmatrix} \\frac{p_1}{1-p_1}(x_1^{(n)})^2 & 0 & 0 & ... & 0 \\\\ 0 & \\frac{p_2}{1-p_2}(x_2^{(n)})^2 & 0 & ... & 0 \\\\ 0 & 0 & \\frac{p_3}{1-p_3}(x_3^{(n)})^2 & ... & 0 \\\\ . & & & & . \\\\ . & & & & . \\\\ . & & & & . \\\\ 0 & 0 & 0 &  ... & \\frac{p_d}{1-p_d}(x_d^{(n)})^2 \\end{pmatrix} $$\n",
    "\n",
    "Using this notation we will caclulate the loss:\n",
    "\n",
    "$$ \\frac{1}{2}\\sum_{n=1}^N ((y^{(n)})^2 - 2w^Tx^{(n)}y^{(n)} + w^T\\mathbb{E}[D^{(n)}x^{(n)}(x^{(n)})^TD^{(n)}]w) = \\frac{1}{2}\\sum_{n=1}^N ((y^{(n)})^2 - 2w^Tx^{(n)}y^{(n)} + w^T(x^{(n)}(x^{(n)})^T + B^{(n)})w) = $$ $$ = \\frac{1}{2}\\sum_{n=1}^N ((y^{(n)})^2 - 2w^Tx^{(n)}y^{(n)} + w^Tx^{(n)}(x^{(n)})^Tw + w^TB^{(n)}w) = \\frac{1}{2}\\sum_{n=1}^N ((y^{(n)} - w^Tx^{(n)})^2 + w^TB^{(n)}w) = $$ $$ = \\frac{1}{2}\\sum_{n=1}^N (y^{(n)} - w^Tx^{(n)})^2 + \\frac{1}{2}\\sum_{n=1}^N w^TB^{(n)}w $$\n",
    "\n",
    "Now let's examine $ \\frac{1}{2}\\sum_{n=1}^N w^TB^{(n)}w $:\n",
    "$$ \\frac{1}{2}\\sum_{n=1}^N w^TB^{(n)}w = \\frac{1}{2}\\sum_{n=1}^N \\sum_{i=1}^d \\frac{p_i}{1-p_i}(x_i^{(n)})^2w_i^2 = $$ $$ = \\frac{1}{2}\\sum_{i=1}^d \\frac{p_i}{1-p_i}\\sum_{n=1}^N((x_i^{(n)})^2)w_i^2 = \\frac{1}{2}\\sum_{i=1}^d \\frac{p_i}{1-p_i}c_iw_i^2$$\n",
    "\n",
    "So lastly we achieve:\n",
    "\n",
    "$$ \\bar{\\mathcal{L}}(w,p) = \\frac{1}{2}\\sum_{n=1}^N (y^{(n)} - w^Tx^{(n)})^2 + \\frac{1}{2}\\sum_{n=1}^N w^TB^{(n)}w = \\frac{1}{2}\\sum_{n=1}^N (y^{(n)} - w^Tx^{(n)})^2 + \\frac{1}{2}\\sum_{i=1}^d \\frac{p_i}{1-p_i}c_iw_i^2 $$\n",
    "$$ \\blacksquare $$\n",
    "\n",
    "### Sub Section 3\n",
    "\n",
    "When using $ \\bar{\\mathcal{L}}(w,p) $, when we will perform the optimization we will try to minimize $ \\frac{1}{2}\\sum_{i=1}^d \\frac{p_i}{1-p_i}c_iw_i^2 $ added to the regular linear regression risk, since the minimization is over $ w $ and $ p $ and if $ p_i \\rightarrow 1 $ then the term explodes to infinity so this will encourage the minimization to set  $ w_i = 0 $ in order to achieve the minimum and if $ p_i \\rightarrow 0 $ the term will have no effect. So in conclusion the features which effect the least over the regular linear regression term will achieve high $ p_i $ and in parallel a very low $ w_i $ effectively ignoring them since the weights will be zeroed out. So this term will encourage ignoring features that are least correlated to the prediction, achieving the dropout effect.\n",
    "\n",
    "### Sub Section 4\n",
    "\n",
    "We can select the Logistic Regression function notated $ f(u_i) $ since it's consistent with the constraint $ f(u_i) \\in [0,1] $ so we will set $ \\forall \\;\\; i \\in \\{1,..,d\\} \\;\\;, p_i = f(u_i) = \\frac{1}{1+e^{-u_i}} $ And accordingly we will get $ p = f(u) $\n",
    "\n",
    "### Sub Section 5\n",
    "No it doesn't necessarily mean that we will converge to the $ w_0 $ solution since it depends on the initilization of $ p_i $ and $ w_i $.\n",
    "Let's assume we initialize $ \\forall i \\in[1,...,d] \\;\\;\\; p_i=0 $ With this initialization $ \\bar{\\mathcal{L}}(w,p) $ is reduced to:\n",
    "\n",
    "$$ \\bar{\\mathcal{L}}(w,p) = \\frac{1}{2}\\sum_{n=1}^N (y^{(n)} - w^Tx^{(n)})^2 $$\n",
    "\n",
    "And for this expression both $ w_0 $ and $ w_* $ achieve minimum loss so both are likely to be converged to while minimizing the loss.\n",
    "\n",
    "So even in the very particular case of initializing $ \\forall i \\in[1,...,d] \\;\\;\\; p_i=0 $ and $ w = w_* $ we will get $ \\bar{\\mathcal{L}}(w,p) = 0 $ so we already converged to the minimum loss and we will get no improvement over time.\n",
    "This example shows that we won't necessarily converge to $ w_0 $ and this depends on the initialization of the parameters.\n",
    "\n",
    "### Sub Section 6\n",
    "\n",
    "Let's derive the risk by $ p_i $:\n",
    "\n",
    "$$ \\frac{\\partial\\bar{\\mathcal{L}}(w,p)}{\\partial p_i} = \\frac{\\partial \\frac{1}{2}\\sum_{i=1}^d \\frac{1}{1-p_i}w_i^2\\sum_{n=1}^N\\left(x_i^{(n)}\\right)^2 + \\mu\\sum_{i=1}^d(1-p_i)}{\\partial p_i} = \\frac{1}{(1-p_i)^2}w_i^2\\sum_{n=1}^N\\left(x_i^{(n)}\\right)^2 - \\mu = 0 \\Rightarrow$$\n",
    "$$ \\mu(1-p_i)^2 = w_i^2\\sum_{n=1}^N\\left(x_i^{(n)}\\right)^2 \\Rightarrow 1-p_i = \\frac{|w_i|\\sqrt{\\sum_{n=1}^N\\left(x_i^{(n)}\\right)^2}}{\\sqrt{\\mu}} $$\n",
    "\n",
    "Now let's use this result to express $ \\bar{\\mathcal{L}}(w,p) $ as $ \\bar{\\mathcal{L}}(w) $:\n",
    "\n",
    "$$ \\mathbb{E}[\\mathcal{L}] =\\frac{1}{2}\\sum_{n=1}^N \\left(y^{(n)} - w^Tx^{(n)} \\right)^2 + \\frac{1}{2}\\sum_{i=1}^d \\frac{1}{1-p_i}w_i^2\\sum_{n=1}^N\\left(x_i^{(n)}\\right)^2 + \\mu\\sum_{i=1}^d(1-p_i) = $$\n",
    "$$ = \\frac{1}{2}\\sum_{n=1}^N \\left(y^{(n)} - w^Tx^{(n)} \\right)^2 + \\frac{1}{2}\\sum_{n=1}^N \\left(y^{(n)} - w^Tx^{(n)} \\right)^2 + \\frac{1}{2}\\sum_{i=1}^d \\frac{\\sqrt{\\mu}}{|w_i|\\sqrt{\\sum_{n=1}^N\\left(x_i^{(n)}\\right)^2}}w_i^2\\sum_{n=1}^N\\left(x_i^{(n)}\\right)^2 + \\mu\\sum_{i=1}^d \\frac{|w_i|\\sqrt{\\sum_{n=1}^N\\left(x_i^{(n)}\\right)^2}}{\\sqrt{\\mu}} = $$\n",
    "$$ =\\frac{1}{2}\\sum_{n=1}^N \\left(y^{(n)} - w^Tx^{(n)} \\right)^2 + \\frac{\\sqrt{\\mu}}{2} \\sum_{i=1}^d |w_i| \\sqrt{\\sum_{n=1}^N\\left(x_i^{(n)}\\right)^2} + \\sqrt{\\mu} \\sum_{i=1}^d |w_i| \\sqrt{\\sum_{n=1}^N\\left(x_i^{(n)}\\right)^2} = $$ $$ = \\frac{1}{2}\\sum_{n=1}^N \\left(y^{(n)} - w^Tx^{(n)} \\right)^2 +  \\frac{3\\sqrt{\\mu}}{2} \\sum_{i=1}^d |w_i| \\sqrt{\\sum_{n=1}^N\\left(x_i^{(n)}\\right)^2} = \\frac{1}{2}\\sum_{n=1}^N \\left(y^{(n)} - w^Tx^{(n)} \\right)^2 + \\lambda \\sum_{i=1}^d |w_i| \\sqrt{\\sum_{n=1}^N\\left(x_i^{(n)}\\right)^2}$$\n",
    "\n",
    "So finally we get:\n",
    "\n",
    "$$ \\bar{\\mathcal{L}}(w) = \\frac{1}{2}\\sum_{n=1}^N \\left(y^{(n)} - w^Tx^{(n)} \\right)^2 + \\lambda \\sum_{i=1}^d |w_i| \\sqrt{\\sum_{n=1}^N\\left(x_i^{(n)}\\right)^2} $$\n",
    "\n",
    "And:\n",
    "\n",
    "$$ R(w) = \\lambda \\sum_{i=1}^d |w_i| \\sqrt{\\sum_{n=1}^N\\left(x_i^{(n)}\\right)^2} $$\n",
    "\n",
    "This regularization helps with feature selection as it inspires to minimize $ R(w) $ and this can be achieved only by reducing $ |w_i| $ intuitively this effect will only occur for the features that participate the least in minimizing the linear regression expression, so if there is a redundant feature that we can predict without we cant anticipate that under optimal conditions we will set it's $ w_i = 0 $ to achieve $ R(w_i) = 0 $ and this is encouraging the minimization to set $ w_i \\neq 0 $ to the lowest number of features possible $ \\Rightarrow $ feature selection."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsqSFZG1pGhj"
   },
   "source": [
    "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 2 - Preventing Variance Explosion\n",
    "---\n",
    "This question relates to lectures 8-9 (from slide 7):\n",
    "\n",
    "Find an initializtion scheme such that $$ \\forall l, i,: \\text{(1) } \\mathbb{E}\\left[F_l(u_l)|u_l\\right]=0, \\text{ (2) } Var(u_l[i]) = 1, $$ assuming skip connections: $u_{l+1} = u_l + F_l(u_l)$ with a single skip $F_l(u_l)=W_l\\phi(u_l)+b_l$ and the activation is ReLU: $\\phi(x) = \\text{ReLU}(x) = \\max(0,x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Question 2 - answer\n",
    "--\n",
    "\n",
    "### Sub Section 1:\n",
    "\n",
    "lets evaluate the expression  $\\mathbb{E}\\left[F_l(u_l)|u_l\\right]$.\n",
    "\n",
    "$ \\mathbb{E}\\left[F_l(u_l)|u_l\\right]=\\mathbb{E}\\left[W_l\\phi(u_l)+b_l|u_l\\right]=\\mathbb{E}\\left[W_l\\cdot\\text{max}(0,u_l)|u_l\\right]+\\mathbb{E}\\left[b_l|u_l\\right]$\n",
    "\n",
    "finally we would like that $∀u_l$:\n",
    "\n",
    "$\\text{max}(0,u_l)\\mathbb{E}\\left[W_l|u_l\\right]+\\mathbb{E}\\left[b_l|u_l\\right]=0$\n",
    "\n",
    "that would happen for example if:\n",
    "\n",
    "$\\mathbb{E}\\left[W_l|u_l\\right]=\\mathbb{E}\\left[W_l\\right]=0,\\mathbb{E}\\left[b_l|u_l\\right]=\\mathbb{E}\\left[b_l\\right]=0$\n",
    "\n",
    "### Sub Section 2:\n",
    "\n",
    "let's evaluate $Var(u_l[i])$.\n",
    "\n",
    "from the law of total variance we get that:\n",
    "\n",
    "$Var(u_l[i])=\\mathbb{E}\\left[Var(u_{l}[i]|u_{l-1}[i])\\right]+Var(\\mathbb{E}\\left[u_{l}[i]|u_{l-1}[i]\\right])$\n",
    "\n",
    "since $u_{l+1} = u_l + F_l(u_l)$,\n",
    "\n",
    "we get that $u_{l} = u_{l-1} + F_{l-1}(u_{l-1})$\n",
    "\n",
    "therefor:\n",
    "\n",
    "$Var(u_l[i])=\\mathbb{E}\\left[Var(u_{l-1}[i] + F_{l-1}(u_{l-1}[i])|u_{l-1}[i])\\right]+Var(\\mathbb{E}\\left[u_{l-1}[i] + F_{l-1}(u_{l-1}[i])|u_{l-1}[i]\\right])$\n",
    "\n",
    "from (1) and assuming (2) is happening for layers with index lower than l, we get that:\n",
    "\n",
    "$Var(\\mathbb{E}\\left[u_{l-1}[i] + F_{l-1}(u_{l-1}[i])|u_{l-1}[i]\\right])=Var(u_{l-1}[i]+0)=1$\n",
    "\n",
    "to finish the proof we need to show that:\n",
    "\n",
    "$\\mathbb{E}\\left[Var(u_{l-1}[i] + F_{l-1}(u_{l-1}[i])|u_{l-1}[i])\\right]=0$\n",
    "\n",
    "Variance is invariant with respect to scalar addition:\n",
    "\n",
    "$\\mathbb{E}\\left[Var(u_{l-1}[i] + F_{l-1}(u_{l-1}[i])|u_{l-1}[i])\\right]=\\mathbb{E}\\left[Var( F_{l-1}(u_{l-1}[i])|u_{l-1}[i])\\right]=\\mathbb{E}\\left[Var( F_{l-1}(u_{l-1}[i])|u_{l-1}[i])\\right]=\\mathbb{E}\\left[Var( W_{l-1}\\phi(u_{l-1}[i])+b_{l-1}|u_{l-1}[i])\\right]=\\mathbb{E}\\left[Var( W_{l-1}\\phi(u_{l-1}[i])|u_{l-1}[i])\\right]$\n",
    "\n",
    "from $(1)$\n",
    "\n",
    "$Var( W_{l-1}\\phi(u_{l-1}[i])|u_{l-1}[i])=\\mathbb{E}\\left[ \\phi(u_{l-1}[i])^TW_{l-1}^TW_{l-1}\\phi(u_{l-1}[i])|u_{l-1}[i]\\right]$\n",
    "\n",
    "so we get:\n",
    "\n",
    "$\\mathbb{E}\\left[Var( W_{l-1}\\phi(u_{l-1}[i])|u_{l-1}[i])\\right]=\\mathbb{E}\\left[\\mathbb{E}\\left[ \\phi(u_{l-1}[i])^TW_{l-1}^TW_{l-1}\\phi(u_{l-1}[i])|u_{l-1}[i]\\right]\\right]=\n",
    "\\mathbb{E}\\left[\\phi(u_{l-1}[i])^T\\right]\\mathbb{E}\\left[ W_{l-1}^TW_{l-1}\\right]\\mathbb{E}\\left[\\phi(u_{l-1}[i])\\right]$\n",
    "\n",
    "for both (1) and (2) if we choose **W**=0 , **b**=0 as initializtion will work.\n",
    "\n",
    "since we have skip connection this initializtion won't cause a problem to train the model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsqSFZG1pGhj"
   },
   "source": [
    "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 3 -Batch Normalization\n",
    "---\n",
    "This question relates to lectures 8-9 (from slide 9):\n",
    "\n",
    "Prove that **without** regularization, BatchNorm **scale invariance** for parameters $\\mathbf{w}$ implies:\n",
    "1. $\\nabla \\mathcal{L}(\\mathbf{w})^T\\mathbf{w} = 0$\n",
    "2. And under gradient flow dynamics ($\\dot{\\mathbf{w}} = -\\eta \\nabla \\mathcal{L}(\\mathbf{w})$) this implies (L2) norm conservation: $\\forall t: ||\\mathbf{w}(t)||^2 = C$\n",
    "\n",
    "Hint: see results from the multilayer networks lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Question 3 - Answer\n",
    "--\n",
    "\n",
    "### Sub Section 1:\n",
    "\n",
    "in lecture 5 we saw that if MNN is invariant to $\\psi_{\\alpha}(w)$ then without\n",
    "regularization ($\\mu=0$) we get that:\n",
    "\n",
    "$\\nabla\\mathcal{L}(\\psi_{\\alpha}(w))^T\\frac{∂}{∂\\alpha} \\psi_{\\alpha}(w) =0 $\n",
    "\n",
    "in our case, $\\psi_{\\alpha}(w)→ BN(w)=BN(\\alpha w)$\n",
    "so we get:\n",
    "\n",
    "$\\nabla\\mathcal{L}(\\mathbf{w})^T\\mathbf{w}= 0$\n",
    "\n",
    "### Sub Section 2:\n",
    "\n",
    "$\\dot{\\mathbf{w}} = -\\eta \\nabla \\mathcal{L}(\\mathbf{w}) →\\nabla \\mathcal{L}(\\mathbf{w})=\\frac{\\dot{\\mathbf{w}}}{-\\eta}$\n",
    "\n",
    "lets apply that in $(1)$ and we will get that:\n",
    "\n",
    "$\\dot{\\mathbf{w}}^T\\mathbf{w}= 0$\n",
    "\n",
    "$\\frac{1}{2}\\cdot\\frac{∂||\\mathbf{w}(t)||^2}{∂t}=0→_*||\\mathbf{w}(t)||^2=const$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7D-14iM7pGhm"
   },
   "source": [
    "### <img src=\"https://img.icons8.com/officel/80/000000/code.png\" style=\"height:50px;display:inline\"> Part 2 - Code Assignments\n",
    "---\n",
    "* You must write your code in this notebook and save it with the output of all of the code cells.\n",
    "* Additional text can be added in Markdown cells.\n",
    "* You can use any other IDE you like (PyCharm, VSCode...) to write/debug your code, but for the submission you must copy it to this notebook, run the code and save the notebook with the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T07:32:10.340195Z",
     "start_time": "2023-06-17T07:32:07.053291Z"
    }
   },
   "outputs": [],
   "source": [
    "# this part uses the Wikitext-2 dataset. To access torchtext datasets, please install `torchdata`:\n",
    "# `pip install torchdata` ir `conda install -c pytorch torchdata` in activated environment \n",
    "# or `!pip install torchdata` on colab.\n",
    "# !pip install torchdata\n",
    "# notes:\n",
    "# torch=2.0.0 <-> torchtext 0.15.1\n",
    "# torch=1.13.0 <-> torchtext 0.14.0\n",
    "# torch=1.12.1 <-> torchtext 0.13.1\n",
    "# downgrading torchtext example: !pip install torchtext==0.13.1 --no-deps\n",
    "# torchtext requires the `portalocker` package to download datasets:\n",
    "# !pip install portalocker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x111232330>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x111232330>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports for the practice (you can add more if you need)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "# torchtext\n",
    "import torchtext\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "NUM_OF_WORDS_TO_PRINT = 20\n",
    "\n",
    "seed = 211\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T07:32:11.792526Z",
     "start_time": "2023-06-17T07:32:11.785782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch: 2.0.1, torchtext: 0.15.2\n"
     ]
    }
   ],
   "source": [
    "print(f'pytorch: {torch.__version__}, torchtext: {torchtext.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/bubbles/50/000000/workflow.png\" style=\"height:50px;display:inline\">  Sequence-to-Sequence with Transformers\n",
    "---\n",
    "* In this exercise, you are going to build a language model using PyTroch's Transformer module.\n",
    "* We will work with the **Wikitext-2** dataset: the WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia.\n",
    "* After training, you will be able to generate senetences!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 1  - Loading and Observing the Data\n",
    "---\n",
    "1. Run the following cells that define the functions `batchify` and `data_process` and initialize the tokenizer, vocabulary and the WikiText2 train dataset.\n",
    "2. Create the train, valid and test data using the provided `batchify` function.\n",
    "5. Print the shape of `train_data`, write in a comment the meaning of each dimension (e.g. `# [meaning of dim1, meaning of dim2]`).\n",
    "6. Print the first 20 words of one training sample from `train_data`. Use the vocabulary you built to transfer between tokens to words: `itos = vocab.vocab.get_itos()` will give a \"int to string\" list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T07:32:13.608540Z",
     "start_time": "2023-06-17T07:32:13.602135Z"
    }
   },
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T07:32:16.756650Z",
     "start_time": "2023-06-17T07:32:16.749253Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T07:32:18.845900Z",
     "start_time": "2023-06-17T07:32:18.033775Z"
    }
   },
   "outputs": [],
   "source": [
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T07:32:20.981543Z",
     "start_time": "2023-06-17T07:32:19.734325Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Current training device: mps\n"
     ]
    }
   ],
   "source": [
    "# train_iter was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "# FIXME - Using Apple GPU Here, Change to cuda so it will compile when submitting\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device = \"mps\" if torch.has_mps else \"cpu\"\n",
    "print(f\"[INFO] Current training device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T07:32:22.422838Z",
     "start_time": "2023-06-17T07:32:22.416743Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "eval_batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T07:32:23.625221Z",
     "start_time": "2023-06-17T07:32:23.567105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([102499, 20])\n",
      "polygamist families , but janelle was not . although christine ' s mother left the faith she still supports them \n"
     ]
    }
   ],
   "source": [
    "train_data = batchify(train_data, batch_size)\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)\n",
    "print(train_data.shape)\n",
    "# Dimension 1 is the sequence length for each batch\n",
    "# Dimension 2 is the number of batches\n",
    "itos = vocab.vocab.get_itos()\n",
    "word_sample = ''\n",
    "for i in range(NUM_OF_WORDS_TO_PRINT):\n",
    "    word_code = train_data[i,5]\n",
    "    word_sample += f'{itos[word_code]} '\n",
    "print(word_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 2  - Preparing the  Data - Separating to Inputs and Targets\n",
    "---\n",
    "* For a language modeling task, the model needs the following words as `Target`.\n",
    "    * For example, for the senetence \"I have a nice dog\", the model will be given \"I have a nice\" as input, and \"have a nice dog\" as the target.\n",
    "* Implement (complete) the function `get_batch(source, i, bptt)`: it generates the input and target sequence for the transformer model. It subdivides the source data into chunks of length `bptt`.\n",
    "    * For example, for `bptt=2` and at `i=0`, the output of `data, target = get_batch(train_data, i=0, bptt=2)`: `data` will be of shape (2, 20), where the batch size is 20 and `target` will be of length 40 (the target for each element is two words, but we flatten `target`).\n",
    "    * Example: for `bptt=2`, and the ABCDEFG... characters as input, our batches will be in the form of: `data=[a, b], target=[b, c]`. For `bptt=3`: `data=[a, b, c], target=[b, c, d]` and so on. This one example is a batch.\n",
    "    * Print a sample from `data` and `target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T07:32:25.927835Z",
     "start_time": "2023-06-17T07:32:25.895717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using bptt = 5 and batch size = 20\n",
      "Data Shape: torch.Size([5, 20])\n",
      "Target Shape: torch.Size([100])\n",
      "Data = polygamist families , but janelle \n",
      "Target = families , but janelle was \n"
     ]
    }
   ],
   "source": [
    "def get_batch(source, i, bptt):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "        bptt: int\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
    "        target has shape [seq_len * batch_size]\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i + seq_len]\n",
    "    target = source[i+1:i+seq_len+1]\n",
    "    target = target.flatten()\n",
    "    return data, target\n",
    "\n",
    "sample_bptt = 5\n",
    "sample_batch = get_batch(train_data, 0, sample_bptt)\n",
    "sample_data = ''\n",
    "sample_target = ''\n",
    "print(f'Using bptt = {sample_bptt} and batch size = {batch_size}')\n",
    "print(f'Data Shape: {sample_batch[0].shape}')\n",
    "print(f'Target Shape: {sample_batch[1].shape}')\n",
    "for i in range(sample_bptt):\n",
    "    sample_data_code = sample_batch[0][i,5]\n",
    "    sample_target_code = sample_batch[1][5 + i * batch_size]\n",
    "    sample_data += f'{itos[sample_data_code]} '\n",
    "    sample_target += f'{itos[sample_target_code]} '\n",
    "print(f'Data = {sample_data}')\n",
    "print(f'Target = {sample_target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 3  - Define Hyperparameters and Initialize the Model\n",
    "---\n",
    "* Define the following hyperparameters (`[a, b]` means in the range between `a` and `b`):\n",
    "    * Embedding size: choose from `[200, 250]`\n",
    "    * Number of hidden units: choose from `[200, 250]`\n",
    "    * Number of layers: choose from `[2, 4]`\n",
    "    * Number of attention heads: choose from `[2, 4]`\n",
    "    * Dropout: choose from `[0.0, 0.3]`\n",
    "    * Loss criterion: `nn.CrossEntropyLoss()`\n",
    "    * Optimizer: choose from `[SGD, Adam, RAdam]`\n",
    "    * Learning rate: choose from `[5e-3, 5.0]`\n",
    "    * Learning Scheduler: `torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)` or any scheduler of your choosing.\n",
    "    * Transformer LayerNormalization: `post` (`norm_first=False`) or `pre` (`norm_first=True`).\n",
    "* Intialize an instance of `TransformerModel` (given) and send it to `device`. Note that you need to give it the number of tokens to define the output of the decoder. You should use the number of tokens in the vocabulary. Print the number of tokens,  print **all** the chosen hyper-parameters and print the model (`print(model`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T07:32:29.122651Z",
     "start_time": "2023-06-17T07:32:29.118615Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5, norm_first=False):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout, norm_first=norm_first)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T07:32:31.261902Z",
     "start_time": "2023-06-17T07:32:31.098779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model:\n",
      "TransformerModel(\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=222, out_features=222, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=222, out_features=210, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=210, out_features=222, bias=True)\n",
      "        (norm1): LayerNorm((222,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((222,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (encoder): Embedding(28782, 222)\n",
      "  (decoder): Linear(in_features=222, out_features=28782, bias=True)\n",
      ")\n",
      "[INFO] Chosen Hyper Parameters:\n",
      "[INFO]\t Num Of Tokens       = 28782,\n",
      "[INFO]\t Embedding Size      = 222,\n",
      "[INFO]\t Num Of Hidden Units = 210,\n",
      "[INFO]\t Num Of Layers       = 3,\n",
      "[INFO]\t Num Of Heads        = 3,\n",
      "[INFO]\t Dropout Probability = 0.2,\n",
      "[INFO]\t Loss Function       = CrossEntropyLoss,\n",
      "[INFO]\t Optimizer           = SGD,\n",
      "[INFO]\t Learning Rate       = 0.1,\n",
      "[INFO]\t Scheduler           = StepLR,\n",
      "[INFO]\t Norm First          = False,\n"
     ]
    }
   ],
   "source": [
    "## Defining Hyper Parameters ##\n",
    "\n",
    "embed_size = 222                                                                   # [200, 250]\n",
    "assert 200 <= embed_size <= 250\n",
    "num_hid_units = 210                                                                # [200, 250]\n",
    "assert 200 <= num_hid_units <= 250\n",
    "num_layers = 3                                                                     # {2, 3, 4}\n",
    "assert num_layers in (2, 3, 4)\n",
    "num_heads = 3                                                                      # {2, 3, 4}\n",
    "assert num_heads in (2, 3, 4)\n",
    "dropout_prob = 0.2                                                                 # [0.0, 0.3]\n",
    "assert 0.0 <= dropout_prob <= 0.3\n",
    "loss_name = 'CrossEntropyLoss'\n",
    "assert loss_name == 'CrossEntropyLoss'\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-1                                                                # [5e-3, 5.0]\n",
    "assert 5e-3 <= learning_rate <= 5.0\n",
    "norm_first = False                                                                  # True / False\n",
    "num_tokens = len(itos)\n",
    "model = TransformerModel(\n",
    "    ntoken=num_tokens,\n",
    "    ninp=embed_size,\n",
    "    nhead=num_heads,\n",
    "    nhid=num_hid_units,\n",
    "    nlayers=num_layers,\n",
    "    dropout=dropout_prob,\n",
    "    norm_first=norm_first\n",
    ")\n",
    "model.to(device)\n",
    "optimizer_name = 'SGD'\n",
    "assert optimizer_name in ['SGD', 'Adam', 'RAdam']                                   # SGD, Adam, RAdam\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=learning_rate)\n",
    "sched_name = 'StepLR'\n",
    "learning_sched = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.95)\n",
    "print(f'[INFO] Model:\\n{model}')\n",
    "print(f'[INFO] Chosen Hyper Parameters:')\n",
    "print(f'[INFO]\\t Num Of Tokens       = {num_tokens},')\n",
    "print(f'[INFO]\\t Embedding Size      = {embed_size},')\n",
    "print(f'[INFO]\\t Num Of Hidden Units = {num_hid_units},')\n",
    "print(f'[INFO]\\t Num Of Layers       = {num_layers},')\n",
    "print(f'[INFO]\\t Num Of Heads        = {num_heads},')\n",
    "print(f'[INFO]\\t Dropout Probability = {dropout_prob},')\n",
    "print(f'[INFO]\\t Loss Function       = {loss_name},')\n",
    "print(f'[INFO]\\t Optimizer           = {optimizer_name},')\n",
    "print(f'[INFO]\\t Learning Rate       = {learning_rate},')\n",
    "print(f'[INFO]\\t Scheduler           = {sched_name},')\n",
    "print(f'[INFO]\\t Norm First          = {norm_first},')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 4  - Train and Evaluate the Language Model\n",
    "---\n",
    "* Fill in the missing line in the training code and train the model.\n",
    "* Use `bptt=35`.\n",
    "* Use the provided function to evaluate it on the validatation set (after each epoch) and on test test (after training is done). **Print and plot** the results (loss and perplexity).\n",
    "* If you see that the performance does not improve, go back to Task 3 and re-think you hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T07:32:41.619336Z",
     "start_time": "2023-06-17T07:32:41.608272Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, eval_data):\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i, bptt)\n",
    "            seq_len = data.size(0)\n",
    "            if seq_len != bptt:\n",
    "                src_mask = src_mask[:seq_len, :seq_len]\n",
    "            output = model(data, src_mask)\n",
    "            output_flat = output.view(-1, num_tokens)\n",
    "            total_loss += seq_len * loss_func(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T07:32:42.890260Z",
     "start_time": "2023-06-17T07:32:42.882335Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, bptt):\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    num_batches = len(train_data) // bptt\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i, bptt)\n",
    "        seq_len = data.size(0)\n",
    "        if seq_len != bptt:  # only on last batch\n",
    "            src_mask = src_mask[:seq_len, :seq_len]\n",
    "        output = model(data, src_mask)\n",
    "        loss = loss_func(output.view(-1, num_tokens), targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = learning_sched.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-06-17T07:24:10.038646Z"
    }
   },
   "outputs": [],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 200\n",
    "best_model = None\n",
    "bptt = 35\n",
    "train_loss_per_epoch = list()\n",
    "val_loss_per_epoch = list()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, bptt)\n",
    "    epoch_loss = evaluate(model, train_data)\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    train_loss_per_epoch.append(epoch_loss)\n",
    "    val_loss_per_epoch.append(val_loss)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    learning_sched.step()\n",
    "\n",
    "x_axis = list(range(1, epochs + 1))\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(x_axis, train_loss_per_epoch, label='Train Loss')\n",
    "plt.plot(x_axis, val_loss_per_epoch, label='Val Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel(\"#No of Epochs\")\n",
    "plt.title('Training Loss and Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(x_axis, np.exp(train_loss_per_epoch), label='Train PPL')\n",
    "plt.plot(x_axis, np.exp(val_loss_per_epoch), label='Val PPL')\n",
    "plt.ylabel('PPL')\n",
    "plt.xlabel(\"#No of Epochs\")\n",
    "plt.title('Training PPL and Validation PPL')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 5  - Generate Sentences\n",
    "---\n",
    "Use the following function to generate 3 sentences of length 20, and print them. Do they make sense? (you can compare generated sentences over epochs, to see if some logic is gained during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T13:26:23.057069Z",
     "start_time": "2023-06-16T13:26:23.033675Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate(model, vocab, nwords=100, temp=1.0):\n",
    "    model.eval()\n",
    "    ntokens = len(vocab)\n",
    "    itos = vocab.vocab.get_itos()\n",
    "    model_input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
    "    words = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(nwords):\n",
    "            output = model(model_input, None)\n",
    "            word_weights = output[-1].squeeze().div(temp).exp().cpu()\n",
    "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "            word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
    "            model_input = torch.cat([model_input, word_tensor], 0)\n",
    "            word = itos[word_idx]\n",
    "            words.append(word)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T13:26:27.648421Z",
     "start_time": "2023-06-16T13:26:25.658796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "during the chicks after = = chicks fledge after john moved on the nest after raising the nest nearby and\n",
      "hospital bhai attack on the hospital and <unk> hospital home with police . picard discovers nothing if he believes she\n",
      "suburbs . florida sand dunes high @-@ level forests are mainly low @-@ level forests just north at main cave\n"
     ]
    }
   ],
   "source": [
    "num_sentences = 3\n",
    "for i in range(num_sentences):\n",
    "    sentence = generate(model, vocab, nwords=20)\n",
    "    sentence_str = ' '\n",
    "    print(sentence_str.join(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "---\n",
    "* Icons made by <a href=\"https://www.flaticon.com/authors/becris\" title=\"Becris\">Becris</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\">www.flaticon.com</a>\n",
    "* Icons from <a href=\"https://icons8.com/\">Icons8.com</a> - https://icons8.com\n",
    "* Datasets from <a href=\"https://www.kaggle.com/\">Kaggle</a> - https://www.kaggle.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
